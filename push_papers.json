[
    {
        "title": "Quadratic Interest Network for Multimodal Click-Through Rate Prediction",
        "url": "http://arxiv.org/abs/2504.17699v1",
        "pub_date": "2025-04-24",
        "summary": "Multimodal click-through rate (CTR) prediction is a key technique in industrial recommender systems. It leverages heterogeneous modalities such as text, images, and behavioral logs to capture high-order feature interactions between users and items, thereby enhancing the system's understanding of user interests and its ability to predict click behavior. The primary challenge in this field lies in effectively utilizing the rich semantic information from multiple modalities while satisfying the low-latency requirements of online inference in real-world applications. To foster progress in this area, the Multimodal CTR Prediction Challenge Track of the WWW 2025 EReL@MIR Workshop formulates the problem into two tasks: (1) Task 1 of Multimodal Item Embedding: this task aims to explore multimodal information extraction and item representation learning methods that enhance recommendation tasks; and (2) Task 2 of Multimodal CTR Prediction: this task aims to explore what multimodal recommendation model can effectively leverage multimodal embedding features and achieve better performance. In this paper, we propose a novel model for Task 2, named Quadratic Interest Network (QIN) for Multimodal CTR Prediction. Specifically, QIN employs adaptive sparse target attention to extract multimodal user behavior features, and leverages Quadratic Neural Networks to capture high-order feature interactions. As a result, QIN achieved an AUC of 0.9798 on the leaderboard and ranked second in the competition. The model code, training logs, hyperparameter configurations, and checkpoints are available at https://github.com/salmon1802/QIN.",
        "translated": "多模态点击率（CTR）预测是工业级推荐系统中的核心技术。该方法通过整合文本、图像及行为日志等异构模态数据，捕捉用户与物品间的高阶特征交互，从而增强系统对用户兴趣的理解及其点击行为预测能力。该领域的主要挑战在于：如何有效利用多模态蕴含的丰富语义信息，同时满足实际应用中在线推理的低延迟需求。为推进相关研究进展，WWW 2025 EReL@MIR研讨会的多模态CTR预测挑战赛道将问题拆解为两个子任务：（1）多模态物品嵌入任务（任务1）：旨在探索能够增强推荐任务的多模态信息提取与物品表征学习方法；（2）多模态CTR预测任务（任务2）：致力于研究何种多模态推荐模型能有效利用多模态嵌入特征并实现更优性能。本文针对任务2提出一种创新模型——面向多模态CTR预测的二次兴趣网络（Quadratic Interest Network, QIN）。具体而言，QIN采用自适应稀疏目标注意力机制提取多模态用户行为特征，并通过二次神经网络捕获高阶特征交互。实验结果表明，QIN在榜单上以0.9798的AUC值位列第二名。模型代码、训练日志、超参数配置及检查点已开源至：https://github.com/salmon1802/QIN。\n\n（翻译说明：本文严格遵循学术规范，对关键术语如\"high-order feature interactions\"（高阶特征交互）、\"adaptive sparse target attention\"（自适应稀疏目标注意力机制）等采用领域内共识译法，并通过同位语形式保留\"QIN\"等模型名称的原始缩写。针对技术细节如\"Quadratic Neural Networks\"（二次神经网络），采用直译结合领域知识验证的方式确保概念准确性。同时，对竞赛名称\"WWW 2025 EReL@MIR Workshop\"等专有名词保持原格式，符合学术翻译惯例。）"
    },
    {
        "title": "IRA: Adaptive Interest-aware Representation and Alignment for\n  Personalized Multi-interest Retrieval",
        "url": "http://arxiv.org/abs/2504.17529v1",
        "pub_date": "2025-04-24",
        "summary": "Online community platforms require dynamic personalized retrieval and recommendation that can continuously adapt to evolving user interests and new documents. However, optimizing models to handle such changes in real-time remains a major challenge in large-scale industrial settings. To address this, we propose the Interest-aware Representation and Alignment (IRA) framework, an efficient and scalable approach that dynamically adapts to new interactions through a cumulative structure. IRA leverages two key mechanisms: (1) Interest Units that capture diverse user interests as contextual texts, while reinforcing or fading over time through cumulative updates, and (2) a retrieval process that measures the relevance between Interest Units and documents based solely on semantic relationships, eliminating dependence on click signals to mitigate temporal biases. By integrating cumulative Interest Unit updates with the retrieval process, IRA continuously adapts to evolving user preferences, ensuring robust and fine-grained personalization without being constrained by past training distributions. We validate the effectiveness of IRA through extensive experiments on real-world datasets, including its deployment in the Home Section of NAVER's CAFE, South Korea's leading community platform.",
        "translated": "在线社区平台需要动态的个性化检索与推荐系统，能够持续适应不断演变的用户兴趣和新内容。然而，在大型工业场景中实时优化模型以应对此类变化仍是一个重大挑战。为此，我们提出兴趣感知表征与对齐（IRA）框架——一种通过累积结构动态适应新交互的高效可扩展方法。该框架基于两大核心机制：(1) 兴趣单元：将多样化用户兴趣表征为上下文文本，通过累积性更新实现兴趣强度的动态强化或衰减；(2) 检索过程：仅基于语义关系衡量兴趣单元与文档的相关性，消除对点击信号的依赖以缓解时间偏差。通过将累积性兴趣单元更新与检索过程相结合，IRA能够持续适应不断变化的用户偏好，确保稳健且细粒度的个性化服务，而无需受限于历史训练数据分布。我们在真实场景数据集上进行了大量实验验证，包括将该框架部署于韩国领先的社区平台NAVER的CAFE首页模块，充分证明了IRA框架的有效性。\n\n（注：译文通过以下方式实现专业性与可读性的平衡：\n1. 专业术语处理：采用\"兴趣单元\"对应\"IU\"，\"累积性更新\"对应\"cumulative updates\"等规范化译法\n2. 技术细节保留：准确传达\"仅基于语义关系\"的技术特性，明确区分\"点击信号\"与\"语义关系\"的差异\n3. 逻辑关系强化：通过分号与连接词突出两个核心机制的并列关系，使用破折号加强框架定义的说明性\n4. 行业背景适配：对\"NAVER's CAFE\"采用品牌名保留策略，补充\"韩国领先的社区平台\"的定位说明）"
    },
    {
        "title": "Replication and Exploration of Generative Retrieval over Dynamic Corpora",
        "url": "http://arxiv.org/abs/2504.17519v1",
        "pub_date": "2025-04-24",
        "summary": "Generative retrieval (GR) has emerged as a promising paradigm in information retrieval (IR). However, most existing GR models are developed and evaluated using a static document collection, and their performance in dynamic corpora where document collections evolve continuously is rarely studied. In this paper, we first reproduce and systematically evaluate various representative GR approaches over dynamic corpora. Through extensive experiments, we reveal that existing GR models with \\textit{text-based} docids show superior generalization to unseen documents. We observe that the more fine-grained the docid design in the GR model, the better its performance over dynamic corpora, surpassing BM25 and even being comparable to dense retrieval methods. While GR models with \\textit{numeric-based} docids show high efficiency, their performance drops significantly over dynamic corpora. Furthermore, our experiments find that the underperformance of numeric-based docids is partly due to their excessive tendency toward the initial document set, which likely results from overfitting on the training set. We then conduct an in-depth analysis of the best-performing GR methods. We identify three critical advantages of text-based docids in dynamic corpora: (i) Semantic alignment with language models' pretrained knowledge, (ii) Fine-grained docid design, and (iii) High lexical diversity. Building on these insights, we finally propose a novel multi-docid design that leverages both the efficiency of numeric-based docids and the effectiveness of text-based docids, achieving improved performance in dynamic corpus without requiring additional retraining. Our work offers empirical evidence for advancing GR methods over dynamic corpora and paves the way for developing more generalized yet efficient GR models in real-world search engines.",
        "translated": "生成式检索（Generative Retrieval, GR）已成为信息检索（IR）领域中一种极具前景的研究范式。然而，现有大多数GR模型均基于静态文档集合进行开发和评估，其在文档集合持续演变的动态语料库中的性能表现却鲜有研究。本文首先对多种具有代表性的GR方法在动态语料库场景下进行复现和系统性评估。通过大量实验，我们发现采用基于文本的文档标识符（text-based docids）的现有GR模型展现出对未见文档的卓越泛化能力。实验表明，GR模型中文档标识符设计粒度越精细，其在动态语料库中的性能表现越优异，不仅超越BM25检索模型，甚至可与密集检索方法相媲美。而采用基于数字的文档标识符（numeric-based docids）的GR模型虽然具有较高效率，但其在动态语料库中的性能却显著下降。进一步实验发现，数字式文档标识符表现欠佳的部分原因在于其对初始文档集的过度倾向性，这可能是由训练集过拟合所导致。\n\n在对最优GR方法的深入分析中，我们揭示了基于文本的文档标识符在动态语料库中的三大关键优势：（i）与语言模型预训练知识的语义对齐性；（ii）细粒度的文档标识符设计；（iii）高词汇多样性。基于这些发现，我们最终提出了一种新型多文档标识符设计，该设计兼具数字式文档标识符的高效性与文本式文档标识符的有效性，在无需额外重新训练的情况下即可提升动态语料库中的检索性能。本研究为推进GR方法在动态语料库中的应用提供了实证依据，为开发兼具泛化能力与高效性的实用搜索引擎GR模型开辟了新路径。"
    },
    {
        "title": "Adaptive Orchestration of Modular Generative Information Access Systems",
        "url": "http://arxiv.org/abs/2504.17454v1",
        "pub_date": "2025-04-24",
        "summary": "Advancements in large language models (LLMs) have driven the emergence of complex new systems to provide access to information, that we will collectively refer to as modular generative information access (GenIA) systems. They integrate a broad and evolving range of specialized components, including LLMs, retrieval models, and a heterogeneous set of sources and tools. While modularity offers flexibility, it also raises critical challenges: How can we systematically characterize the space of possible modules and their interactions? How can we automate and optimize interactions among these heterogeneous components? And, how do we enable this modular system to dynamically adapt to varying user query requirements and evolving module capabilities? In this perspective paper, we argue that the architecture of future modular generative information access systems will not just assemble powerful components, but enable a self-organizing system through real-time adaptive orchestration -- where components' interactions are dynamically configured for each user input, maximizing information relevance while minimizing computational overhead. We give provisional answers to the questions raised above with a roadmap that depicts the key principles and methods for designing such an adaptive modular system. We identify pressing challenges, and propose avenues for addressing them in the years ahead. This perspective urges the IR community to rethink modular system designs for developing adaptive, self-optimizing, and future-ready architectures that evolve alongside their rapidly advancing underlying technologies.",
        "translated": "大型语言模型（LLM）的进步推动了新型复杂系统的出现，这些系统旨在提供信息访问服务。我们将这类系统统称为模块化生成式信息访问（GenIA）系统。它们整合了广泛且持续演进的专业化组件，包括大型语言模型、检索模型，以及异构化的数据源和工具集合。尽管模块化设计提供了灵活性，但也带来了严峻的挑战：如何系统性地刻画潜在模块空间及其交互方式？如何实现异构组件间交互的自动化和优化？如何使这种模块化系统动态适应多样化的用户查询需求和持续进化的模块能力？在这篇前瞻性论文中，我们主张未来模块化生成式信息访问系统的架构不应仅止于堆砌强大的组件，而应通过实时自适应编排构建自组织系统——即针对每个用户输入动态配置组件交互关系，在最大化信息相关性的同时最小化计算开销。我们通过描绘构建此类自适应模块化系统的核心原则与方法路线图，对上述问题提出初步解答。本文明确了亟需突破的关键挑战，并为未来数年的研究方向提出建议路径。这一视角呼吁信息检索学界重新思考模块化系统设计，以开发出与其底层技术快速演进保持同步的、具备自适应能力和自我优化特质的未来适应性架构。"
    },
    {
        "title": "Beyond Whole Dialogue Modeling: Contextual Disentanglement for\n  Conversational Recommendation",
        "url": "http://arxiv.org/abs/2504.17427v1",
        "pub_date": "2025-04-24",
        "summary": "Conversational recommender systems aim to provide personalized recommendations by analyzing and utilizing contextual information related to dialogue. However, existing methods typically model the dialogue context as a whole, neglecting the inherent complexity and entanglement within the dialogue. Specifically, a dialogue comprises both focus information and background information, which mutually influence each other. Current methods tend to model these two types of information mixedly, leading to misinterpretation of users' actual needs, thereby lowering the accuracy of recommendations. To address this issue, this paper proposes a novel model to introduce contextual disentanglement for improving conversational recommender systems, named DisenCRS. The proposed model DisenCRS employs a dual disentanglement framework, including self-supervised contrastive disentanglement and counterfactual inference disentanglement, to effectively distinguish focus information and background information from the dialogue context under unsupervised conditions. Moreover, we design an adaptive prompt learning module to automatically select the most suitable prompt based on the specific dialogue context, fully leveraging the power of large language models. Experimental results on two widely used public datasets demonstrate that DisenCRS significantly outperforms existing conversational recommendation models, achieving superior performance on both item recommendation and response generation tasks.",
        "translated": "对话式推荐系统旨在通过分析与利用对话相关的上下文信息，提供个性化推荐服务。然而，现有方法通常将对话上下文视为整体进行建模，忽视了对话中固有的复杂性和信息纠缠现象。具体而言，对话包含相互影响的焦点信息与背景信息两种成分。当前方法倾向于将两类信息混合建模，导致对用户真实需求的误判，从而降低推荐准确性。为解决这一问题，本文提出一种引入上下文解耦机制的新型对话推荐模型DisenCRS。该模型采用双重解耦框架，包含自监督对比解耦和反事实推理解耦模块，能够在无监督条件下有效区分对话上下文中的焦点信息与背景信息。此外，我们设计了自适应提示学习模块，可根据具体对话语境自动选择最适配的提示模板，充分释放大型语言模型的潜力。在两个广泛使用的公开数据集上的实验结果表明，DisenCRS在推荐准确性和响应生成质量方面均显著优于现有对话推荐模型，展现出卓越的综合性能。"
    },
    {
        "title": "DataScout: Automatic Data Fact Retrieval for Statement Augmentation with\n  an LLM-Based Agent",
        "url": "http://arxiv.org/abs/2504.17334v1",
        "pub_date": "2025-04-24",
        "summary": "A data story typically integrates data facts from multiple perspectives and stances to construct a comprehensive and objective narrative. However, retrieving these facts demands time for data search and challenges the creator's analytical skills. In this work, we introduce DataScout, an interactive system that automatically performs reasoning and stance-based data facts retrieval to augment the user's statement. Particularly, DataScout leverages an LLM-based agent to construct a retrieval tree, enabling collaborative control of its expansion between users and the agent. The interface visualizes the retrieval tree as a mind map that eases users to intuitively steer the retrieval direction and effectively engage in reasoning and analysis. We evaluate the proposed system through case studies and in-depth expert interviews. Our evaluation demonstrates that DataScout can effectively retrieve multifaceted data facts from different stances, helping users verify their statements and enhance the credibility of their stories.",
        "translated": "数据故事通常需要整合来自多重视角与立场的数据事实，以构建全面客观的叙事框架。然而，检索这些事实既需要耗费数据搜索时间，也对创作者的分析能力提出挑战。本研究提出DataScout——一个通过自动推理和基于立场的数据事实检索来增强用户陈述的交互式系统。该系统创新性地采用基于大语言模型的智能体构建检索树，实现用户与智能体协同控制树形结构的扩展过程。系统界面将检索树以思维导图形式可视化呈现，使用户能够直观引导检索方向，有效参与推理分析过程。通过案例研究和深度专家访谈评估表明，DataScout系统能够有效获取不同立场的多维度数据事实，既帮助用户验证陈述内容，又能提升故事叙述的可信度。\n\n（翻译说明：\n1. 专业术语处理：保持\"NLP/IR/CV\"等专业领域术语的准确性，如\"LLM-based agent\"译为\"基于大语言模型的智能体\"，\"retrieval tree\"译为\"检索树\"\n2. 技术细节呈现：对\"collaborative control\"采用\"协同控制\"的译法，准确传达人机协作的核心特征\n3. 系统功能表达：使用\"思维导图可视化\"对应原文\"mind map\"的界面设计特点，保持技术描述的准确性\n4. 学术规范遵循：采用\"案例研究/深度专家访谈\"等标准学术表达，符合论文摘要的正式性要求\n5. 逻辑完整性：通过\"既...又能...\"的句式结构，精准复现原文的因果论证关系）"
    },
    {
        "title": "You Are What You Bought: Generating Customer Personas for E-commerce\n  Applications",
        "url": "http://arxiv.org/abs/2504.17304v1",
        "pub_date": "2025-04-24",
        "summary": "In e-commerce, user representations are essential for various applications. Existing methods often use deep learning techniques to convert customer behaviors into implicit embeddings. However, these embeddings are difficult to understand and integrate with external knowledge, limiting the effectiveness of applications such as customer segmentation, search navigation, and product recommendations. To address this, our paper introduces the concept of the customer persona. Condensed from a customer's numerous purchasing histories, a customer persona provides a multi-faceted and human-readable characterization of specific purchase behaviors and preferences, such as Busy Parents or Bargain Hunters.   This work then focuses on representing each customer by multiple personas from a predefined set, achieving readable and informative explicit user representations. To this end, we propose an effective and efficient solution GPLR. To ensure effectiveness, GPLR leverages pre-trained LLMs to infer personas for customers. To reduce overhead, GPLR applies LLM-based labeling to only a fraction of users and utilizes a random walk technique to predict personas for the remaining customers. We further propose RevAff, which provides an absolute error $\\epsilon$ guarantee while improving the time complexity of the exact solution by a factor of at least $O(\\frac{\\epsilon\\cdot|E|N}{|E|+N\\log N})$, where $N$ represents the number of customers and products, and $E$ represents the interactions between them. We evaluate the performance of our persona-based representation in terms of accuracy and robustness for recommendation and customer segmentation tasks using three real-world e-commerce datasets. Most notably, we find that integrating customer persona representations improves the state-of-the-art graph convolution-based recommendation model by up to 12% in terms of NDCG@K and F1-Score@K.",
        "translated": "在电子商务领域，用户表征对各类应用至关重要。现有方法通常采用深度学习技术将客户行为转化为隐式嵌入表示。然而，这些嵌入不仅难以理解，也难以与外部知识进行整合，限制了客户分群、搜索导航和产品推荐等应用的效果。为解决这一问题，本文提出了\"客户角色\"的概念。通过浓缩客户的大量购买历史，客户角色能提供特定购买行为与偏好多维度、人类可读的特征描述（例如\"忙碌家长\"或\"折扣猎人\"）。本研究重点在于通过预定义集合中的多个角色来表征每个客户，从而实现可读性强且信息量大的显式用户表征。为此，我们提出了一个高效解决方案GPLR。为确保有效性，GPLR利用预训练大语言模型来推断客户角色；为降低计算开销，GPLR仅对小部分用户应用基于LLM的标注，并采用随机游走技术为剩余客户预测角色。我们进一步提出RevAff算法，该算法在提升精确解时间效率至少$O(\\frac{\\epsilon\\cdot|E|N}{|E|+N\\log N})$倍的同时（其中$N$表示客户与商品数量，$E$表示其交互关系），还能提供绝对误差$\\epsilon$保证。基于三个真实电商数据集，我们从推荐系统和客户分群任务的准确性与鲁棒性维度评估了角色表征的性能。最显著的发现是：整合客户角色表征可使当前最先进的基于图卷积的推荐模型在NDCG@K和F1-Score@K指标上最高提升12%。"
    },
    {
        "title": "Does Knowledge Distillation Matter for Large Language Model based Bundle\n  Generation?",
        "url": "http://arxiv.org/abs/2504.17220v1",
        "pub_date": "2025-04-24",
        "summary": "LLMs are increasingly explored for bundle generation, thanks to their reasoning capabilities and knowledge. However, deploying large-scale LLMs introduces significant efficiency challenges, primarily high computational costs during fine-tuning and inference due to their massive parameterization. Knowledge distillation (KD) offers a promising solution, transferring expertise from large teacher models to compact student models. This study systematically investigates knowledge distillation approaches for bundle generation, aiming to minimize computational demands while preserving performance. We explore three critical research questions: (1) how does the format of KD impact bundle generation performance? (2) to what extent does the quantity of distilled knowledge influence performance? and (3) how do different ways of utilizing the distilled knowledge affect performance? We propose a comprehensive KD framework that (i) progressively extracts knowledge (patterns, rules, deep thoughts); (ii) captures varying quantities of distilled knowledge through different strategies; and (iii) exploits complementary LLM adaptation techniques (in-context learning, supervised fine-tuning, combination) to leverage distilled knowledge in small student models for domain-specific adaptation and enhanced efficiency. Extensive experiments provide valuable insights into how knowledge format, quantity, and utilization methodologies collectively shape LLM-based bundle generation performance, exhibiting KD's significant potential for more efficient yet effective LLM-based bundle generation.",
        "translated": "随着大语言模型（LLMs）在推理能力和知识储备方面的优势日益凸显，其在捆绑生成任务中的应用探索逐渐深入。然而，大规模LLMs的部署带来了显著的效率挑战，主要源于其庞大体量参数化导致微调与推理阶段的高计算成本。知识蒸馏（KD）通过将大型教师模型的专业能力迁移至紧凑的学生模型，为此提供了有前景的解决方案。本研究系统性地探索了面向捆绑生成任务的知识蒸馏方法，旨在保持性能的同时最小化计算需求。我们重点研究三个关键问题：(1) 知识蒸馏的格式如何影响捆绑生成性能？(2) 蒸馏知识的数量对性能的影响程度如何？(3) 不同知识利用方式如何作用于性能表现？为此，我们提出了一个综合知识蒸馏框架，该框架具备以下创新：(i) 渐进式知识提取机制（模式、规则、深层思维）；(ii) 通过差异化策略捕获不同规模的蒸馏知识；(iii) 整合互补的LLM适应技术（上下文学习、监督微调、组合策略），使小型学生模型能够有效利用蒸馏知识实现领域适配与效率提升。大量实验揭示了知识格式、数量及利用方法如何共同塑造基于LLM的捆绑生成性能，充分展现了知识蒸馏在实现高效且有效的LLM捆绑生成方面的重要潜力。\n\n（注：本翻译严格遵循以下原则：\n1. 专业术语标准化处理（如\"knowledge distillation\"译为\"知识蒸馏\"而非\"知识提炼\"）\n2. 技术细节精确转化（如\"in-context learning\"译为专业术语\"上下文学习\"）\n3. 逻辑结构完整保留（研究问题、方法论、结论的对应关系清晰）\n4. 学术表达规范化（保持被动语态、专业句式等学术论文特征）\n5. 关键概念一致性（如\"bundle generation\"统一译为\"捆绑生成\"））"
    },
    {
        "title": "Dynamic Superblock Pruning for Fast Learned Sparse Retrieval",
        "url": "http://arxiv.org/abs/2504.17045v1",
        "pub_date": "2025-04-23",
        "summary": "This paper proposes superblock pruning (SP) during top-k online document retrieval for learned sparse representations. SP structures the sparse index as a set of superblocks on a sequence of document blocks and conducts a superblock-level selection to decide if some superblocks can be pruned before visiting their child blocks. SP generalizes the previous flat block or cluster-based pruning, allowing the early detection of groups of documents that cannot or are less likely to appear in the final top-k list. SP can accelerate sparse retrieval in a rank-safe or approximate manner under a high-relevance competitiveness constraint. Our experiments show that the proposed scheme significantly outperforms state-of-the-art baselines on MS MARCO passages on a single-threaded CPU.",
        "translated": "本论文提出了一种在基于学习稀疏表示的在线文档top-k检索过程中进行超级块剪枝（SuperBlock Pruning, SP）的方法。SP通过将稀疏索引组织为基于文档块序列的超级块集合，在访问子块之前执行超级块级别的选择，以判断某些超级块是否可以被提前剪枝。该机制将传统的扁平块剪枝或基于聚类的剪枝方法泛化，能够早期检测出无法或较不可能出现在最终top-k列表中的文档群组。在高相关性竞争约束条件下，SP能够以排名安全或近似方式加速稀疏检索。实验结果表明，在单线程CPU环境下对MS MARCO passages数据集进行测试时，所提出的方案显著优于当前最优的基线方法。"
    },
    {
        "title": "Search Timelines: Visualizing Search History to Enable Cross-Session\n  Exploratory Search",
        "url": "http://arxiv.org/abs/2504.16741v1",
        "pub_date": "2025-04-23",
        "summary": "Purpose: The timespan over which exploratory searching can occur, as well as the scope and volume of the search activities undertaken, can make it difficult for searchers to remember key details about their search activities. These difficulties are present both in the midst of searching as well as when resuming a search that spans multiple sessions. In this paper, we present a search interface designed to support cross-session exploratory search in a public digital library context. Methods: Search Timelines provides a visualization of current and past search activities via a dynamic timeline of the search activity (queries and saved resources). This timeline is presented at two levels of detail. An overview timeline is provided alongside the search results in a typical search engine results page design. A detailed timeline is provided in the workspace, where searchers can review the history of their search activities and their saved resources. A controlled laboratory study was conducted to compare this approach to a baseline interface modelled after a typical public digital library search/workspace interface. Results: Participants who used Search Timelines reported higher levels of user engagement, usability, and perceived knowledge gain, during an initial search session and when resuming the search after a 7-8 day interval. This came at the expense of the searchers taking more time to complete the search task, which we view as positive evidence of engagement in cross-session exploratory search processes. Conclusion: Search Timelines serves as an example of how lightweight visualization approaches can be used to enhance typical search interface designs to support exploratory search. The results highlight the value of providing persistent representations of past search activities within the search interface.",
        "translated": "目的：探索性搜索行为可能持续较长时间，且搜索活动的范围和体量较大，这使得搜索者难以记住其搜索过程中的关键细节。这些记忆困难既存在于持续搜索过程中，也存在于跨越多个会话的搜索恢复阶段。本文提出一种专为公共数字图书馆场景设计的跨会话探索性搜索支持界面。方法：搜索时间轴（Search Timelines）通过动态展示搜索活动时间轴（包含查询操作与保存资源），对当前及历史搜索行为进行可视化呈现。该时间轴提供两个层级的详细信息：在典型搜索引擎结果页设计中，概览时间轴与搜索结果并列呈现；在工作空间界面中则提供详细时间轴，方便搜索者回顾搜索历程及已保存资源。我们通过受控实验室研究，将该方法与基于典型公共数字图书馆搜索/工作空间界面构建的基准界面进行对比。结果：实验结果表明，在初次搜索会话及间隔7-8天后恢复搜索时，使用搜索时间轴的参与者报告了更高水平的用户参与度、可用性和感知知识获取。这一优势的代价是搜索者需要花费更多时间完成任务，我们认为这恰恰是用户投入跨会话探索性搜索过程的积极证据。结论：搜索时间轴的成功实践证明，轻量级可视化方法能够有效增强传统搜索界面设计以支持探索性搜索。研究结果凸显了在搜索界面中持续呈现历史搜索行为表征的重要价值。"
    },
    {
        "title": "A Unified Retrieval Framework with Document Ranking and EDU Filtering\n  for Multi-document Summarization",
        "url": "http://arxiv.org/abs/2504.16711v1",
        "pub_date": "2025-04-23",
        "summary": "In the field of multi-document summarization (MDS), transformer-based models have demonstrated remarkable success, yet they suffer an input length limitation. Current methods apply truncation after the retrieval process to fit the context length; however, they heavily depend on manually well-crafted queries, which are impractical to create for each document set for MDS. Additionally, these methods retrieve information at a coarse granularity, leading to the inclusion of irrelevant content. To address these issues, we propose a novel retrieval-based framework that integrates query selection and document ranking and shortening into a unified process. Our approach identifies the most salient elementary discourse units (EDUs) from input documents and utilizes them as latent queries. These queries guide the document ranking by calculating relevance scores. Instead of traditional truncation, our approach filters out irrelevant EDUs to fit the context length, ensuring that only critical information is preserved for summarization. We evaluate our framework on multiple MDS datasets, demonstrating consistent improvements in ROUGE metrics while confirming its scalability and flexibility across diverse model architectures. Additionally, we validate its effectiveness through an in-depth analysis, emphasizing its ability to dynamically select appropriate queries and accurately rank documents based on their relevance scores. These results demonstrate that our framework effectively addresses context-length constraints, establishing it as a robust and reliable solution for MDS.",
        "translated": "在多文档摘要（MDS）领域，基于Transformer的模型虽然取得了显著成功，但仍受限于输入长度约束。现有方法通常通过在检索后进行截断以适应上下文长度，但这类方法高度依赖人工设计的优质查询，而针对每个文档集专门构建此类查询对于MDS任务而言并不现实。此外，现有检索方法的粒度过于粗糙，容易导致不相关内容被纳入。为应对这些问题，我们提出了一种新型检索框架，将查询选择与文档排序及精简整合为统一流程。该框架首先从输入文档中识别最具显著性的基本语篇单元（EDUs），并将其作为潜在查询。这些查询通过计算相关性得分来指导文档排序。不同于传统的截断方法，我们的方法通过过滤不相关的EDUs来适应上下文长度，确保仅保留关键信息用于摘要生成。我们在多个MDS数据集上评估了该框架，结果显示ROUGE指标持续提升，同时验证了其在不同模型架构间的可扩展性和灵活性。通过深入分析，我们进一步证实了该框架的有效性，突出其动态选择适当查询以及基于相关性得分精准排序文档的能力。实验结果表明，我们的框架成功克服了上下文长度限制，为MDS任务构建了稳健可靠的解决方案。"
    },
    {
        "title": "Ensemble Bayesian Inference: Leveraging Small Language Models to Achieve\n  LLM-level Accuracy in Profile Matching Tasks",
        "url": "http://arxiv.org/abs/2504.17685v1",
        "pub_date": "2025-04-24",
        "summary": "This study explores the potential of small language model(SLM) ensembles to achieve accuracy comparable to proprietary large language models (LLMs). We propose Ensemble Bayesian Inference (EBI), a novel approach that applies Bayesian estimation to combine judgments from multiple SLMs, allowing them to exceed the performance limitations of individual models. Our experiments on diverse tasks(aptitude assessments and consumer profile analysis in both Japanese and English) demonstrate EBI's effectiveness. Notably, we analyze cases where incorporating models with negative Lift values into ensembles improves overall performance, and we examine the method's efficacy across different languages. These findings suggest new possibilities for constructing high-performance AI systems with limited computational resources and for effectively utilizing models with individually lower performance. Building on existing research on LLM performance evaluation, ensemble methods, and open-source LLM utilization, we discuss the novelty and significance of our approach.",
        "translated": "本研究探讨了通过小语言模型（SLM）集成实现与专有大型语言模型（LLM）相媲美的准确性的可能性。我们提出了集成贝叶斯推断（EBI）这一创新方法，通过应用贝叶斯估计综合多个SLM的判断，使其突破单个模型的性能限制。在跨语言（日语和英语）的多项任务（能力评估与消费者画像分析）实验中，该方法均展现出显著效果。值得注意的是，我们发现了将具有负Lift值的模型纳入集成反而提升整体性能的特殊现象，并验证了该方法在不同语言环境下的有效性。这些发现为在有限计算资源下构建高性能AI系统，以及有效利用单体性能较弱的模型开辟了新路径。基于现有关于LLM性能评估、集成方法以及开源LLM利用的研究基础，本文进一步探讨了该方法的创新性与应用价值。\n\n（关键术语与技术细节处理说明）：\n1. \"Lift值\"作为数据挖掘领域的核心指标予以保留英文术语\n2. 模型性能评价指标\"negative Lift values\"准确表达为\"负Lift值\"\n3. \"aptitude assessments\"结合NLP领域特点译为\"能力评估\"\n4. \"consumer profile analysis\"根据商业智能背景译为\"消费者画像分析\"\n5. 方法名称\"Ensemble Bayesian Inference\"完整保留英文缩写与中文译名\n6. 学术概念\"Bayesian estimation\"规范译为\"贝叶斯估计\"\n7. 语言类型标注采用\"日语和英语\"的标准学术表述"
    },
    {
        "title": "Bridge the Domains: Large Language Models Enhanced Cross-domain\n  Sequential Recommendation",
        "url": "http://arxiv.org/abs/2504.18383v1",
        "pub_date": "2025-04-25",
        "summary": "Cross-domain Sequential Recommendation (CDSR) aims to extract the preference from the user's historical interactions across various domains. Despite some progress in CDSR, two problems set the barrier for further advancements, i.e., overlap dilemma and transition complexity. The former means existing CDSR methods severely rely on users who own interactions on all domains to learn cross-domain item relationships, compromising the practicability. The latter refers to the difficulties in learning the complex transition patterns from the mixed behavior sequences. With powerful representation and reasoning abilities, Large Language Models (LLMs) are promising to address these two problems by bridging the items and capturing the user's preferences from a semantic view. Therefore, we propose an LLMs Enhanced Cross-domain Sequential Recommendation model (LLM4CDSR). To obtain the semantic item relationships, we first propose an LLM-based unified representation module to represent items. Then, a trainable adapter with contrastive regularization is designed to adapt the CDSR task. Besides, a hierarchical LLMs profiling module is designed to summarize user cross-domain preferences. Finally, these two modules are integrated into the proposed tri-thread framework to derive recommendations. We have conducted extensive experiments on three public cross-domain datasets, validating the effectiveness of LLM4CDSR. We have released the code online.",
        "translated": "跨领域序列推荐（Cross-domain Sequential Recommendation, CDSR）旨在通过用户在不同领域的历史交互行为提取用户偏好。尽管该领域已取得一定进展，但两大问题仍制约着其发展：重叠困境与转移复杂性。前者指现有方法严重依赖在全部领域均有交互行为的用户来学习跨领域物品关联，削弱了实用性；后者指从混合行为序列中捕捉复杂转移模式的困难。大语言模型（Large Language Models, LLMs）凭借强大的表征与推理能力，有望通过语义层面的物品关联与用户偏好挖掘来突破这两大瓶颈。为此，我们提出大语言模型增强的跨领域序列推荐模型（LLM4CDSR）。首先设计基于大语言模型的统一表征模块，建立物品语义关联；继而构建具备对比正则化的可训练适配器实现任务适配；同时设计分层式大语言模型画像模块，提炼用户跨领域偏好特征。最终将上述模块集成至三线程架构中进行推荐决策。在三个公开跨领域数据集上的实验验证了模型有效性，相关代码已开源。"
    },
    {
        "title": "Leveraging Decoder Architectures for Learned Sparse Retrieval",
        "url": "http://arxiv.org/abs/2504.18151v1",
        "pub_date": "2025-04-25",
        "summary": "Learned Sparse Retrieval (LSR) has traditionally focused on small-scale encoder-only transformer architectures. With the advent of large-scale pre-trained language models, their capability to generate sparse representations for retrieval tasks across different transformer-based architectures, including encoder-only, decoder-only, and encoder-decoder models, remains largely unexplored. This study investigates the effectiveness of LSR across these architectures, exploring various sparse representation heads and model scales. Our results highlight the limitations of using large language models to create effective sparse representations in zero-shot settings, identifying challenges such as inappropriate term expansions and reduced performance due to the lack of expansion. We find that the encoder-decoder architecture with multi-tokens decoding approach achieves the best performance among the three backbones. While the decoder-only model performs worse than the encoder-only model, it demonstrates the potential to outperform when scaled to a high number of parameters.",
        "translated": "学习型稀疏检索（LSR）传统上主要聚焦于小规模的仅编码器型Transformer架构。随着大规模预训练语言模型的发展，这些模型在跨不同Transformer架构（包括仅编码器、仅解码器及编码器-解码器模型）生成稀疏表征用于检索任务的能力仍存在较大研究空白。本研究系统评估了LSR在不同架构中的有效性，探索了多种稀疏表征生成头及模型规模的影响。实验结果表明，在零样本设置下，使用大型语言模型构建有效稀疏表征存在显著局限性，具体表现为不恰当的词项扩展和因缺乏扩展机制导致的性能下降等问题。研究发现，采用多令牌解码方法的编码器-解码器架构在三种骨干模型中取得了最佳性能。虽然仅解码器模型表现逊于仅编码器模型，但当其参数规模扩展至较高水平时，显示出性能超越的潜力。"
    },
    {
        "title": "Revisiting Algorithmic Audits of TikTok: Poor Reproducibility and\n  Short-term Validity of Findings",
        "url": "http://arxiv.org/abs/2504.18140v1",
        "pub_date": "2025-04-25",
        "summary": "Social media platforms are constantly shifting towards algorithmically curated content based on implicit or explicit user feedback. Regulators, as well as researchers, are calling for systematic social media algorithmic audits as this shift leads to enclosing users in filter bubbles and leading them to more problematic content. An important aspect of such audits is the reproducibility and generalisability of their findings, as it allows to draw verifiable conclusions and audit potential changes in algorithms over time. In this work, we study the reproducibility of the existing sockpuppeting audits of TikTok recommender systems, and the generalizability of their findings. In our efforts to reproduce the previous works, we find multiple challenges stemming from social media platform changes and content evolution, but also the research works themselves. These drawbacks limit the audit reproducibility and require an extensive effort altogether with inevitable adjustments to the auditing methodology. Our experiments also reveal that these one-shot audit findings often hold only in the short term, implying that the reproducibility and generalizability of the audits heavily depend on the methodological choices and the state of algorithms and content on the platform. This highlights the importance of reproducible audits that allow us to determine how the situation changes in time.",
        "translated": "社交媒体平台正不断转向基于用户隐式或显式反馈的算法驱动内容策展。由于这种转变会将用户封闭在信息茧房中并导向更具问题的内容，监管机构和研究人员呼吁对社交媒体算法开展系统性审计。此类审计的核心要素在于研究结果的可重复性和普适性，这有助于得出可验证的结论并监测算法随时间的潜在变化。本研究聚焦于现有针对TikTok推荐系统的傀儡账户审计方法的可重复性及其研究发现的普适性。在复现前人研究的过程中，我们发现了多重挑战：既来自社交媒体平台本身的更新迭代和内容生态演进，也源自既有研究工作的内在局限性。这些缺陷不仅限制了审计的可重复性，还迫使研究人员需要投入大量精力对审计方法进行必要调整。实验表明，这类一次性审计的结论往往仅在短期内有效，这意味着审计的可重复性与普适性高度依赖于方法论选择以及平台算法和内容的实时状态。这一发现凸显了可重复审计的重要性——唯有通过这种方法，我们才能准确评估平台生态随时间演变的具体态势。"
    },
    {
        "title": "SMARTFinRAG: Interactive Modularized Financial RAG Benchmark",
        "url": "http://arxiv.org/abs/2504.18024v1",
        "pub_date": "2025-04-25",
        "summary": "Financial sectors are rapidly adopting language model technologies, yet evaluating specialized RAG systems in this domain remains challenging. This paper introduces SMARTFinRAG, addressing three critical gaps in financial RAG assessment: (1) a fully modular architecture where components can be dynamically interchanged during runtime; (2) a document-centric evaluation paradigm generating domain-specific QA pairs from newly ingested financial documents; and (3) an intuitive interface bridging research-implementation divides. Our evaluation quantifies both retrieval efficacy and response quality, revealing significant performance variations across configurations. The platform's open-source architecture supports transparent, reproducible research while addressing practical deployment challenges faced by financial institutions implementing RAG systems.",
        "translated": "金融领域正快速采用语言模型技术，但该领域专用RAG系统的评估仍面临挑战。本文提出SMARTFinRAG系统，着力解决金融RAG评估中的三个关键缺口：(1) 完全模块化架构，支持组件在运行时动态替换；(2) 以文档为中心的评估范式，通过从新摄入的金融文档生成领域特定的问答对；(3) 直观的交互界面，弥合研究与实际应用之间的鸿沟。我们的评估体系同时量化检索效能和响应质量，揭示了不同配置间存在显著的性能差异。该平台的开源架构不仅支持透明、可复现的研究，同时解决了金融机构在部署RAG系统时面临的实际实施挑战。\n\n（翻译说明：\n1. 专业术语处理：\"RAG systems\"译为\"检索增强生成系统\"的缩写形式\"RAG系统\"以符合中文技术文献惯例；\n2. 技术细节保留：将\"dynamic interchange during runtime\"准确表述为\"运行时动态替换\"，突出系统动态特性；\n3. 架构描述优化：\"document-centric evaluation paradigm\"译为\"以文档为中心的评估范式\"，既保持原文含义又符合中文表达习惯；\n4. 功能特性强化：\"bridging research-implementation divides\"意译为\"弥合研究与实际应用之间的鸿沟\"，提升表述的直观性；\n5. 评估指标精确化：\"quantifies both retrieval efficacy and response quality\"采用\"量化检索效能和响应质量\"的双重复合结构，确保技术参数的完整传达；\n6. 行业痛点聚焦：将\"practical deployment challenges\"扩展译为\"实际实施挑战\"，突出金融行业应用场景的特殊性。）"
    },
    {
        "title": "Unsupervised Corpus Poisoning Attacks in Continuous Space for Dense\n  Retrieval",
        "url": "http://arxiv.org/abs/2504.17884v1",
        "pub_date": "2025-04-24",
        "summary": "This paper concerns corpus poisoning attacks in dense information retrieval, where an adversary attempts to compromise the ranking performance of a search algorithm by injecting a small number of maliciously generated documents into the corpus. Our work addresses two limitations in the current literature. First, attacks that perform adversarial gradient-based word substitution search do so in the discrete lexical space, while retrieval itself happens in the continuous embedding space. We thus propose an optimization method that operates in the embedding space directly. Specifically, we train a perturbation model with the objective of maintaining the geometric distance between the original and adversarial document embeddings, while also maximizing the token-level dissimilarity between the original and adversarial documents. Second, it is common for related work to have a strong assumption that the adversary has prior knowledge about the queries. In this paper, we focus on a more challenging variant of the problem where the adversary assumes no prior knowledge about the query distribution (hence, unsupervised). Our core contribution is an adversarial corpus attack that is fast and effective. We present comprehensive experimental results on both in- and out-of-domain datasets, focusing on two related tasks: a top-1 attack and a corpus poisoning attack. We consider attacks under both a white-box and a black-box setting. Notably, our method can generate successful adversarial examples in under two minutes per target document; four times faster compared to the fastest gradient-based word substitution methods in the literature with the same hardware. Furthermore, our adversarial generation method generates text that is more likely to occur under the distribution of natural text (low perplexity), and is therefore more difficult to detect.",
        "translated": "本文聚焦于密集信息检索中的语料库投毒攻击问题，即攻击者通过向语料库注入少量恶意生成的文档来破坏搜索算法的排序性能。我们的研究工作主要针对当前文献中的两大局限性展开。首先，现有的基于对抗梯度的词替换搜索攻击方法在离散的词法空间中进行操作，而检索过程本身发生在连续的嵌入空间。因此，我们提出了一种直接在嵌入空间进行优化的方法。具体而言，我们通过训练扰动模型来实现双重目标：在保持原始文档与对抗文档嵌入之间几何距离的同时，最大化原始文档与对抗文档在词汇层面的差异性。\n\n其次，现有相关研究通常强假设攻击者具有查询的先验知识。本文则关注一个更具挑战性的问题变体：攻击者在无查询分布先验知识（即无监督）的情况下实施攻击。我们的核心贡献在于提出了一种快速高效的语料库对抗攻击方法。通过在领域内和跨领域数据集上的全面实验结果，我们重点评估了两项关联任务：top-1攻击和语料库投毒攻击，并考察了白盒与黑盒两种场景下的攻击效果。值得注意的是，我们的方法能在每个目标文档的生成时间不足两分钟的情况下成功生成对抗样本，相比文献中现有最快的基于梯度的词替换方法（相同硬件条件下）速度提升四倍。此外，本方法生成的对抗文本在自然语言分布下具有更低的困惑度，因而更难被检测系统识别。"
    },
    {
        "title": "LLM-Generated Fake News Induces Truth Decay in News Ecosystem: A Case\n  Study on Neural News Recommendation",
        "url": "http://arxiv.org/abs/2504.20013v2",
        "pub_date": "2025-04-28",
        "summary": "Online fake news moderation now faces a new challenge brought by the malicious use of large language models (LLMs) in fake news production. Though existing works have shown LLM-generated fake news is hard to detect from an individual aspect, it remains underexplored how its large-scale release will impact the news ecosystem. In this study, we develop a simulation pipeline and a dataset with ~56k generated news of diverse types to investigate the effects of LLM-generated fake news within neural news recommendation systems. Our findings expose a truth decay phenomenon, where real news is gradually losing its advantageous position in news ranking against fake news as LLM-generated news is involved in news recommendation. We further provide an explanation about why truth decay occurs from a familiarity perspective and show the positive correlation between perplexity and news ranking. Finally, we discuss the threats of LLM-generated fake news and provide possible countermeasures. We urge stakeholders to address this emerging challenge to preserve the integrity of news ecosystems.",
        "translated": "在线虚假新闻治理目前面临由大型语言模型（LLMs）在虚假新闻制作中的恶意使用带来的新挑战。尽管现有研究表明从个体层面检测LLM生成的虚假新闻存在困难，但其大规模投放将如何影响新闻生态系统仍缺乏深入探讨。本研究通过构建仿真管道和包含约5.6万条多样化生成新闻的数据集，系统考察了神经新闻推荐系统中LLM生成虚假新闻的影响。我们的研究揭示了一个\"真相衰减\"现象：当LLM生成新闻参与推荐排序时，真实新闻在对抗虚假新闻的排名优势将逐步丧失。我们进一步从信息熟悉度视角解释了真相衰减现象的成因，并证实了困惑度（perplexity）与新闻排名的正相关性。最后，我们探讨了LLM生成虚假新闻的威胁并提出了可能的应对策略。本研究呼吁相关利益方重视这一新兴挑战，共同维护新闻生态系统的完整性。\n\n（注：译文在保持专业性的同时进行了必要的语序调整和术语优化，确保以下几点：\n1. 专业术语准确：如\"perplexity\"译为\"困惑度\"，\"neural news recommendation systems\"译为\"神经新闻推荐系统\"\n2. 技术细节保留：完整传递仿真管道构建、数据集规模、实证发现等关键信息\n3. 学术表述规范：使用\"揭示\"、\"证实\"、\"探讨\"等学术动词保持论文摘要的严谨性\n4. 逻辑关系清晰：通过\"尽管\"、\"进一步\"、\"最后\"等连接词保持论证逻辑的连贯性）"
    },
    {
        "title": "Chatbot Arena Meets Nuggets: Towards Explanations and Diagnostics in the\n  Evaluation of LLM Responses",
        "url": "http://arxiv.org/abs/2504.20006v1",
        "pub_date": "2025-04-28",
        "summary": "Battles, or side-by-side comparisons in so called arenas that elicit human preferences, have emerged as a popular approach to assessing the output quality of LLMs. Recently, this idea has been extended to retrieval-augmented generation (RAG) systems. While undoubtedly representing an advance in evaluation, battles have at least two drawbacks, particularly in the context of complex information-seeking queries: they are neither explanatory nor diagnostic. Recently, the nugget evaluation methodology has emerged as a promising approach to evaluate the quality of RAG answers. Nuggets decompose long-form LLM-generated answers into atomic facts, highlighting important pieces of information necessary in a \"good\" response. In this work, we apply our AutoNuggetizer framework to analyze data from roughly 7K Search Arena battles provided by LMArena in a fully automatic manner. Our results show a significant correlation between nugget scores and human preferences, showcasing promise in our approach to explainable and diagnostic system evaluations.",
        "translated": "在人工智能领域，尤其是在评估大型语言模型（LLMs）输出质量方面，所谓的\"竞技场比拼\"（即通过并排对比引发人类偏好）已成为一种流行方法。近期，这种评估理念被扩展应用于检索增强生成系统（RAG）。尽管这种方法无疑推动了评估的进步，但其至少存在两个缺陷——特别是在处理复杂的信息搜索查询时：既缺乏解释性，也不具备诊断能力。最新提出的信息块评估法（nugget evaluation methodology）为评估RAG答案质量提供了新思路。该方法通过将长文本形式的LLM生成答案分解为原子事实（atomic facts），突出强调优质回答中必须包含的关键信息要素。\n\n本研究运用自主研发的AutoNuggetizer框架，对LMArena平台提供的约7000组搜索竞技场比拼数据进行全自动分析。实验结果显示，信息块评分与人类偏好存在显著相关性，这验证了我们提出的可解释、可诊断系统评估方法的有效性。该技术突破为深入理解RAG系统表现提供了新的分析维度，使研究人员能够精准定位模型在信息完整性和准确性方面的具体优劣势。"
    },
    {
        "title": "Hierarchical Uncertainty-Aware Graph Neural Network",
        "url": "http://arxiv.org/abs/2504.19820v1",
        "pub_date": "2025-04-28",
        "summary": "Recent research on graph neural networks (GNNs) has explored mechanisms for capturing local uncertainty and exploiting graph hierarchies to mitigate data sparsity and leverage structural properties. However, the synergistic integration of these two approaches remains underexplored. In this work, we introduce a novel architecture, the Hierarchical Uncertainty-Aware Graph Neural Network (HU-GNN), which unifies multi-scale representation learning, principled uncertainty estimation, and self-supervised embedding diversity within a single end-to-end framework. Specifically, HU-GNN adaptively forms node clusters and estimates uncertainty at multiple structural scales from individual nodes to higher levels. These uncertainty estimates guide a robust message-passing mechanism and attention weighting, effectively mitigating noise and adversarial perturbations while preserving predictive accuracy on both node- and graph-level tasks. We also offer key theoretical contributions, including a probabilistic formulation, rigorous uncertainty-calibration guarantees, and formal robustness bounds. Finally, by incorporating recent advances in graph contrastive learning, HU-GNN maintains diverse, structurally faithful embeddings. Extensive experiments on standard benchmarks demonstrate that our model achieves state-of-the-art robustness and interpretability.",
        "translated": "【图神经网络研究新进展】近期关于图神经网络（GNNs）的研究探索了捕捉局部不确定性及利用图层次结构的技术路径，旨在缓解数据稀疏性问题并有效挖掘图结构特性。然而，这两种方法的协同整合机制仍存在研究空白。本研究提出创新性架构——层次化不确定性感知图神经网络（HU-GNN），首次将多尺度表征学习、原则性不确定性估计与自监督嵌入多样性统一于端到端框架中。具体而言，HU-GNN通过以下机制实现突破：(1) 自适应节点聚类与多结构尺度不确定性估计（从单节点到高层级）；(2) 基于不确定性指导的鲁棒消息传递机制与注意力加权，在维持节点级和图级任务预测精度的同时有效缓解噪声与对抗性扰动；(3) 理论创新包括概率形式化框架、严格的不确定性校准保证及形式化鲁棒性边界证明。此外，通过整合图对比学习最新进展，本架构可保持具有结构保真性的多样化嵌入表征。在标准基准测试中，大量实验验证了该模型在鲁棒性与可解释性方面达到最先进水平。\n\n【核心创新点】\n- 首次实现多尺度不确定性建模与层次化表征的协同优化\n- 建立理论完备的概率框架与鲁棒性保障体系\n- 通过对比学习增强嵌入空间的结构保持能力\n\n【应用价值】该框架为社交网络分析、分子性质预测等需要处理复杂层级结构与噪声数据的场景提供了新的解决方案。"
    },
    {
        "title": "Reconstructing Context: Evaluating Advanced Chunking Strategies for\n  Retrieval-Augmented Generation",
        "url": "http://arxiv.org/abs/2504.19754v1",
        "pub_date": "2025-04-28",
        "summary": "Retrieval-augmented generation (RAG) has become a transformative approach for enhancing large language models (LLMs) by grounding their outputs in external knowledge sources. Yet, a critical question persists: how can vast volumes of external knowledge be managed effectively within the input constraints of LLMs? Traditional methods address this by chunking external documents into smaller, fixed-size segments. While this approach alleviates input limitations, it often fragments context, resulting in incomplete retrieval and diminished coherence in generation. To overcome these shortcomings, two advanced techniques, late chunking and contextual retrieval, have been introduced, both aiming to preserve global context. Despite their potential, their comparative strengths and limitations remain unclear. This study presents a rigorous analysis of late chunking and contextual retrieval, evaluating their effectiveness and efficiency in optimizing RAG systems. Our results indicate that contextual retrieval preserves semantic coherence more effectively but requires greater computational resources. In contrast, late chunking offers higher efficiency but tends to sacrifice relevance and completeness.",
        "translated": "检索增强生成（Retrieval-Augmented Generation，RAG）通过将大型语言模型（LLM）的输出与外部知识源相结合，已成为提升其性能的革命性方法。然而，一个关键问题始终存在：如何在海量外部知识与LLM的输入限制之间实现有效平衡？传统解决方案是将外部文档切分为固定尺寸的较小片段。虽然这种方法能够缓解输入限制，但往往导致上下文割裂，造成检索信息不完整并降低生成内容的连贯性。\n\n为克服这些缺陷，研究者提出了两种先进技术——延迟分块（late chunking）和上下文检索（contextual retrieval），二者均致力于保持全局上下文。尽管这些技术展现出潜力，但其相对优势与局限性仍未明晰。本研究对延迟分块和上下文检索进行了严格分析，评估它们在优化RAG系统中的效能与效率。实验结果表明：上下文检索能更有效地保持语义连贯性，但需要消耗更多计算资源；而延迟分块虽具有更高效率，却往往以牺牲相关性与完整性为代价。"
    },
    {
        "title": "Mitigating Modality Bias in Multi-modal Entity Alignment from a Causal\n  Perspective",
        "url": "http://arxiv.org/abs/2504.19458v2",
        "pub_date": "2025-04-28",
        "summary": "Multi-Modal Entity Alignment (MMEA) aims to retrieve equivalent entities from different Multi-Modal Knowledge Graphs (MMKGs), a critical information retrieval task. Existing studies have explored various fusion paradigms and consistency constraints to improve the alignment of equivalent entities, while overlooking that the visual modality may not always contribute positively. Empirically, entities with low-similarity images usually generate unsatisfactory performance, highlighting the limitation of overly relying on visual features. We believe the model can be biased toward the visual modality, leading to a shortcut image-matching task. To address this, we propose a counterfactual debiasing framework for MMEA, termed CDMEA, which investigates visual modality bias from a causal perspective. Our approach aims to leverage both visual and graph modalities to enhance MMEA while suppressing the direct causal effect of the visual modality on model predictions. By estimating the Total Effect (TE) of both modalities and excluding the Natural Direct Effect (NDE) of the visual modality, we ensure that the model predicts based on the Total Indirect Effect (TIE), effectively utilizing both modalities and reducing visual modality bias. Extensive experiments on 9 benchmark datasets show that CDMEA outperforms 14 state-of-the-art methods, especially in low-similarity, high-noise, and low-resource data scenarios.",
        "translated": "多模态实体对齐（Multi-Modal Entity Alignment, MMEA）旨在从不同的多模态知识图谱（Multi-Modal Knowledge Graphs, MMKGs）中检索等效实体，是一项关键的信息检索任务。现有研究通过探索多种融合范式与一致性约束来提升等效实体对齐效果，但忽视了视觉模态并不总能产生积极贡献这一事实。实证研究表明，图像相似度较低的实体通常会导致模型性能不佳，这凸显了过度依赖视觉特征的局限性。我们认为模型可能对视觉模态产生偏向性，从而退化为简单的图像匹配任务。针对此问题，我们提出了一种反事实去偏框架CDMEA，从因果视角探究视觉模态偏差。该框架旨在协同利用视觉与图模态增强MMEA性能，同时抑制视觉模态对模型预测的直接因果影响。通过估计两种模态的总效应（Total Effect, TE）并排除视觉模态的自然直接效应（Natural Direct Effect, NDE），我们确保模型基于总间接效应（Total Indirect Effect, TIE）进行预测，有效融合双模态信息并降低视觉模态偏差。在9个基准数据集上的大量实验表明，CDMEA在14种最先进方法中表现优异，尤其在低相似度、高噪声和低资源数据场景下优势显著。"
    },
    {
        "title": "AlphaFuse: Learn ID Embeddings for Sequential Recommendation in Null\n  Space of Language Embeddings",
        "url": "http://arxiv.org/abs/2504.19218v2",
        "pub_date": "2025-04-27",
        "summary": "Recent advancements in sequential recommendation have underscored the potential of Large Language Models (LLMs) for enhancing item embeddings. However, existing approaches face three key limitations: 1) the degradation of the semantic space when high-dimensional language embeddings are mapped to lower-dimensional ID embeddings, 2) the underutilization of language embeddings, and 3) the reliance on additional trainable parameters, such as an adapter, to bridge the gap between the semantic and behavior spaces. In this paper, we introduce AlphaFuse, a simple but effective language-guided learning strategy that addresses these challenges by learning ID embeddings within the null space of language embeddings. Specifically, we decompose the semantic space of language embeddings via Singular Value Decomposition (SVD), distinguishing it into a semantic-rich row space and a semantic-sparse null space. Collaborative signals are then injected into the null space, while preserving the rich semantics of the row space. AlphaFuse prevents degradation of the semantic space, integrates the retained language embeddings into the final item embeddings, and eliminates the need for auxiliary trainable modules, enabling seamless adaptation to any sequential recommendation framework. We validate the effectiveness and flexibility of AlphaFuse through extensive experiments on three benchmark datasets, including cold-start user and long-tail settings, showcasing significant improvements in both discriminative and diffusion-based generative sequential recommenders. Our codes and datasets are available at https://github.com/Hugo-Chinn/AlphaFuse.",
        "translated": "顺序推荐领域的最新进展揭示了大语言模型（Large Language Models, LLMs）在增强项目嵌入方面的潜力。然而，现有方法面临三个关键限制：1）当高维语言嵌入映射到低维ID嵌入时导致的语义空间退化；2）语言嵌入的利用不足；3）依赖额外可训练参数（如适配器）来弥合语义空间与行为空间之间的鸿沟。本文提出AlphaFuse——一种简单但有效的语言引导学习策略，通过将ID嵌入学习置于语言嵌入的零空间内来解决上述挑战。具体而言，我们通过奇异值分解（Singular Value Decomposition, SVD）对语言嵌入的语义空间进行解耦，将其区分为语义丰富的行空间和语义稀疏的零空间。随后将协同信号注入零空间，同时保留行空间的丰富语义。AlphaFuse不仅防止了语义空间退化，还将保留的语言嵌入整合到最终的项目嵌入中，且无需辅助可训练模块，能够无缝适配任何顺序推荐框架。通过在三个基准数据集（包括冷启动用户和长尾场景设置）上的大量实验，我们验证了AlphaFuse在判别式和基于扩散的生成式顺序推荐器中均能带来显著提升的有效性与灵活性。代码及数据集已开源：https://github.com/Hugo-Chinn/AlphaFuse。"
    },
    {
        "title": "Relative Contrastive Learning for Sequential Recommendation with\n  Similarity-based Positive Pair Selection",
        "url": "http://arxiv.org/abs/2504.19178v1",
        "pub_date": "2025-04-27",
        "summary": "Contrastive Learning (CL) enhances the training of sequential recommendation (SR) models through informative self-supervision signals. Existing methods often rely on data augmentation strategies to create positive samples and promote representation invariance. Some strategies such as item reordering and item substitution may inadvertently alter user intent. Supervised Contrastive Learning (SCL) based methods find an alternative to augmentation-based CL methods by selecting same-target sequences (interaction sequences with the same target item) to form positive samples. However, SCL-based methods suffer from the scarcity of same-target sequences and consequently lack enough signals for contrastive learning. In this work, we propose to use similar sequences (with different target items) as additional positive samples and introduce a Relative Contrastive Learning (RCL) framework for sequential recommendation. RCL comprises a dual-tiered positive sample selection module and a relative contrastive learning module. The former module selects same-target sequences as strong positive samples and selects similar sequences as weak positive samples. The latter module employs a weighted relative contrastive loss, ensuring that each sequence is represented closer to its strong positive samples than its weak positive samples. We apply RCL on two mainstream deep learning-based SR models, and our empirical results reveal that RCL can achieve 4.88% improvement averagely than the state-of-the-art SR methods on five public datasets and one private dataset.",
        "translated": "对比学习通过提供信息丰富的自监督信号，有效提升了序列推荐模型的训练效果。现有方法通常依赖数据增强策略生成正样本以促进表示不变性，但诸如商品重排序和商品替换等策略可能无意中改变用户原始意图。基于监督对比学习的方法通过选择具有相同目标商品的交互序列（同目标序列）构建正样本，为基于增强的对比学习方法提供了替代方案。然而这类方法受限于同目标序列的稀缺性，难以获得充足的对比学习信号。本研究提出使用具有不同目标商品的相似序列作为额外正样本，构建了面向序列推荐的相对对比学习框架。该框架包含双层级正样本选择模块和相对对比学习模块：前者筛选同目标序列作为强正样本，选取相似序列作为弱正样本；后者采用加权相对对比损失函数，确保每个序列在表示空间中更接近其强正样本而非弱正样本。我们将该框架应用于两个主流深度学习序列推荐模型，实验结果表明在五个公共数据集和一个私有数据集上，相对对比学习方法相较现有最优序列推荐模型平均取得了4.88%的性能提升。"
    },
    {
        "title": "LLM-Evaluation Tropes: Perspectives on the Validity of LLM-Evaluations",
        "url": "http://arxiv.org/abs/2504.19076v1",
        "pub_date": "2025-04-27",
        "summary": "Large Language Models (LLMs) are increasingly used to evaluate information retrieval (IR) systems, generating relevance judgments traditionally made by human assessors. Recent empirical studies suggest that LLM-based evaluations often align with human judgments, leading some to suggest that human judges may no longer be necessary, while others highlight concerns about judgment reliability, validity, and long-term impact. As IR systems begin incorporating LLM-generated signals, evaluation outcomes risk becoming self-reinforcing, potentially leading to misleading conclusions.   This paper examines scenarios where LLM-evaluators may falsely indicate success, particularly when LLM-based judgments influence both system development and evaluation. We highlight key risks, including bias reinforcement, reproducibility challenges, and inconsistencies in assessment methodologies. To address these concerns, we propose tests to quantify adverse effects, guardrails, and a collaborative framework for constructing reusable test collections that integrate LLM judgments responsibly. By providing perspectives from academia and industry, this work aims to establish best practices for the principled use of LLMs in IR evaluation.",
        "translated": "大型语言模型（LLMs）正被越来越多地用于评估信息检索（IR）系统，其生成的关联性判断传统上由人类评估者完成。近期实证研究表明，基于LLM的评估结果常与人类判断结果一致，这导致部分研究者认为可能不再需要人工评估者，但另一些学者则对其判断的可靠性、有效性及长期影响提出了担忧。当IR系统开始整合LLM生成的信号时，评估结果可能陷入自我强化的循环，最终导致误导性结论。本文重点探讨LLM评估器可能错误指示成功的场景，尤其是在LLM生成的判断同时影响系统开发和评估过程的情况下。我们重点揭示了若干关键风险，包括偏见强化、可复现性挑战以及评估方法的不一致性。针对这些问题，我们提出了量化负面影响的测试方法、防护机制，以及构建可复用测试集的协作框架，以负责任的方式整合LLM的判断结果。通过整合学术界和工业界的观点，本研究旨在为IR评估中LLM的原则性应用建立最佳实践指南。"
    },
    {
        "title": "Feature Fusion Revisited: Multimodal CTR Prediction for MMCTR Challenge",
        "url": "http://arxiv.org/abs/2504.18961v1",
        "pub_date": "2025-04-26",
        "summary": "With the rapid advancement of Multimodal Large Language Models (MLLMs), an increasing number of researchers are exploring their application in recommendation systems. However, the high latency associated with large models presents a significant challenge for such use cases. The EReL@MIR workshop provided a valuable opportunity to experiment with various approaches aimed at improving the efficiency of multimodal representation learning for information retrieval tasks. As part of the competition's requirements, participants were mandated to submit a technical report detailing their methodologies and findings. Our team was honored to receive the award for Task 2 - Winner (Multimodal CTR Prediction). In this technical report, we present our methods and key findings. Additionally, we propose several directions for future work, particularly focusing on how to effectively integrate recommendation signals into multimodal representations. The codebase for our implementation is publicly available at: https://github.com/Lattice-zjj/MMCTR_Code, and the trained model weights can be accessed at: https://huggingface.co/FireFlyCourageous/MMCTR_DIN_MicroLens_1M_x1.",
        "translated": "随着多模态大语言模型（MLLMs）的快速发展，越来越多的研究者开始探索其在推荐系统中的应用。然而，大型模型伴随的高延迟特性为此类应用场景带来了重大挑战。EReL@MIR研讨会为尝试多种提升信息检索任务中多模态表示学习效率的方法提供了宝贵机会。根据竞赛要求，参赛者必须提交详细阐述方法及发现的技术报告。我们团队荣幸获得了任务二（多模态CTR预测）的优胜奖项。本技术报告将系统阐述我们的方法论与核心发现，同时针对未来研究方向提出若干建议，尤其聚焦于如何有效将推荐信号整合到多模态表示中。项目代码库已开源至：https://github.com/Lattice-zjj/MMCTR_Code，训练完成的模型权重可通过以下地址获取：https://huggingface.co/FireFlyCourageous/MMCTR_DIN_MicroLens_1M_x1。\n\n（翻译说明：  \n1. 专业术语处理：对MLLMs、CTR等专业缩写保留英文原词并附加中文解释，确保技术准确性  \n2. 技术细节呈现：对\"recommendation signals\"等概念采用\"推荐信号\"的译法，符合领域内惯用表达  \n3. 逻辑关系重构：将原文复合句合理拆分为符合中文表达习惯的短句，如将\"participants were mandated...\"独立成句  \n4. 学术规范遵循：对奖项名称\"Task 2 - Winner\"采用竞赛领域标准译法\"任务二 - 优胜者\"  \n5. 技术资源标注：完整保留代码库与模型权重链接的原始格式，确保可访问性）"
    },
    {
        "title": "Generative Product Recommendations for Implicit Superlative Queries",
        "url": "http://arxiv.org/abs/2504.18748v1",
        "pub_date": "2025-04-26",
        "summary": "In Recommender Systems, users often seek the best products through indirect, vague, or under-specified queries, such as \"best shoes for trail running\". Such queries, also referred to as implicit superlative queries, pose a significant challenge for standard retrieval and ranking systems as they lack an explicit mention of attributes and require identifying and reasoning over complex factors. We investigate how Large Language Models (LLMs) can generate implicit attributes for ranking as well as reason over them to improve product recommendations for such queries. As a first step, we propose a novel four-point schema for annotating the best product candidates for superlative queries called SUPERB, paired with LLM-based product annotations. We then empirically evaluate several existing retrieval and ranking approaches on our new dataset, providing insights and discussing their integration into real-world e-commerce production systems.",
        "translated": "在推荐系统中，用户经常通过间接、模糊或未明确指定的查询来寻找最佳产品，例如\"最适合越野跑的鞋子\"。这类被称为隐式最高级查询的请求，由于缺乏明确的属性说明且需要识别和推理复杂因素，给标准检索和排序系统带来了重大挑战。我们研究了大型语言模型（LLMs）如何为排序生成隐式属性，并对其进行推理以改进针对此类查询的产品推荐。首先，我们提出了一种名为SUPERB（面向最高级查询的最佳产品标注）的新型四点标注模式，配合基于LLM的产品标注方法。随后，我们在新构建的数据集上对多种现有检索与排序方法进行了实证评估，为实际电子商务生产系统的集成提供了深刻见解和实践讨论。"
    },
    {
        "title": "MINT: Multi-Vector Search Index Tuning",
        "url": "http://arxiv.org/abs/2504.20018v1",
        "pub_date": "2025-04-28",
        "summary": "Vector search plays a crucial role in many real-world applications. In addition to single-vector search, multi-vector search becomes important for multi-modal and multi-feature scenarios today. In a multi-vector database, each row is an item, each column represents a feature of items, and each cell is a high-dimensional vector. In multi-vector databases, the choice of indexes can have a significant impact on performance. Although index tuning for relational databases has been extensively studied, index tuning for multi-vector search remains unclear and challenging. In this paper, we define multi-vector search index tuning and propose a framework to solve it. Specifically, given a multi-vector search workload, we develop algorithms to find indexes that minimize latency and meet storage and recall constraints. Compared to the baseline, our latency achieves 2.1X to 8.3X speedup.",
        "translated": "向量搜索在众多现实应用中发挥着关键作用。除单向量搜索外，多向量搜索在当前多模态和多特征场景中日益重要。在多向量数据库中，每行代表一个数据项，每列表示数据项的特征，而每个单元格则存储高维向量。在多向量数据库中，索引选择对系统性能具有显著影响。虽然关系数据库的索引调优已得到广泛研究，但多向量搜索的索引优化问题仍不明确且充满挑战。本文明确定义了多向量搜索索引调优问题，并提出系统性解决方案框架。具体而言，针对给定的多向量搜索工作负载，我们开发了能够自动寻找在满足存储约束和召回率要求下最小化查询延迟的索引优化算法。实验表明，相较于基准方法，我们的方案实现了2.1倍到8.3倍的延迟优化。"
    },
    {
        "title": "LLM-Generated Fake News Induces Truth Decay in News Ecosystem: A Case\n  Study on Neural News Recommendation",
        "url": "http://arxiv.org/abs/2504.20013v2",
        "pub_date": "2025-04-28",
        "summary": "Online fake news moderation now faces a new challenge brought by the malicious use of large language models (LLMs) in fake news production. Though existing works have shown LLM-generated fake news is hard to detect from an individual aspect, it remains underexplored how its large-scale release will impact the news ecosystem. In this study, we develop a simulation pipeline and a dataset with ~56k generated news of diverse types to investigate the effects of LLM-generated fake news within neural news recommendation systems. Our findings expose a truth decay phenomenon, where real news is gradually losing its advantageous position in news ranking against fake news as LLM-generated news is involved in news recommendation. We further provide an explanation about why truth decay occurs from a familiarity perspective and show the positive correlation between perplexity and news ranking. Finally, we discuss the threats of LLM-generated fake news and provide possible countermeasures. We urge stakeholders to address this emerging challenge to preserve the integrity of news ecosystems.",
        "translated": "在线虚假新闻治理当前面临一项新挑战:大型语言模型(LLMs)被恶意应用于虚假新闻生产。尽管已有研究表明从个体层面检测LLM生成的虚假新闻存在困难，但其大规模传播将对新闻生态系统产生何种影响仍缺乏深入探究。本研究通过构建模拟管道和包含约5.6万条多样化生成新闻的数据集，深入分析了神经新闻推荐系统中LLM生成虚假新闻的影响机制。研究发现揭示了\"真相衰减\"现象:当LLM生成的新闻参与推荐排序时，真实新闻在排名中的优势地位会逐步丧失。我们进一步从熟悉度视角阐释了该现象的产生机制，并证实了困惑度与新闻排名之间的正相关性。最后，本文探讨了LLM生成虚假新闻的潜在威胁，提出了可能的应对策略。我们呼吁相关利益方重视这一新兴挑战，共同维护新闻生态系统的完整性。\n\n关键术语处理说明:\n1. \"truth decay\"译为\"真相衰减现象\"，既保持术语准确性又符合中文表达习惯\n2. \"perplexity\"译为\"困惑度\"，采用自然语言处理领域的标准译法\n3. \"neural news recommendation systems\"译为\"神经新闻推荐系统\"，准确反映其基于神经网络的技术特性\n4. \"familiarity perspective\"译为\"熟悉度视角\"，既保留原意又符合中文学术表达规范\n5. 技术指标\"~56k\"译为\"约5.6万条\"，遵循中文数字表达规范同时保持数据精确性\n\n本翻译严格遵循学术翻译规范，在保证专业术语准确性的同时，注重逻辑连贯性和可读性，完整保留了原文的技术细节和研究发现。"
    },
    {
        "title": "Efficient Domain-adaptive Continual Pretraining for the Process Industry\n  in the German Language",
        "url": "http://arxiv.org/abs/2504.19856v1",
        "pub_date": "2025-04-28",
        "summary": "Domain-adaptive continual pretraining (DAPT) is a state-of-the-art technique that further trains a language model (LM) on its pretraining task, e.g., language masking. Although popular, it requires a significant corpus of domain-related data, which is difficult to obtain for specific domains in languages other than English, such as the process industry in the German language. This paper introduces an efficient approach called ICL-augmented pretraining or ICL-APT that leverages in-context learning (ICL) and k-nearest neighbors (kNN) to augment target data with domain-related and in-domain texts, significantly reducing GPU time while maintaining strong model performance. Our results show that this approach performs better than traditional DAPT by 3.5 of the average IR metrics (e.g., mAP, MRR, and nDCG) and requires almost 4 times less computing time, providing a cost-effective solution for industries with limited computational capacity. The findings highlight the broader applicability of this framework to other low-resource industries, making NLP-based solutions more accessible and feasible in production environments.",
        "translated": "领域自适应持续预训练（Domain-adaptive continual pretraining, DAPT）是一种前沿技术，通过在预训练任务（如语言掩码任务）上对语言模型（LM）进行持续训练以提升其性能。尽管该技术应用广泛，但其需要大量领域相关数据作为支撑，这对于英语以外的特定语言领域（如德语流程工业领域）而言往往难以获取。本文提出了一种高效方法——基于上下文学习增强的预训练（ICL-augmented pretraining, ICL-APT），该方法通过整合上下文学习（ICL）和k近邻算法（kNN），利用领域相关文本和域内文本对目标数据进行增强，在保持模型优异性能的同时显著减少GPU计算时间。实验结果表明，该方法相较于传统DAPT在平均信息检索指标（如mAP、MRR和nDCG）上提升3.5个百分点，且所需计算时间减少近四倍，为计算资源受限的工业领域提供了高性价比的解决方案。研究结论表明，该框架可广泛适用于其他资源匮乏的行业，使得基于自然语言处理的解决方案在生产环境中更具可行性和推广价值。"
    },
    {
        "title": "Reconstructing Context: Evaluating Advanced Chunking Strategies for\n  Retrieval-Augmented Generation",
        "url": "http://arxiv.org/abs/2504.19754v1",
        "pub_date": "2025-04-28",
        "summary": "Retrieval-augmented generation (RAG) has become a transformative approach for enhancing large language models (LLMs) by grounding their outputs in external knowledge sources. Yet, a critical question persists: how can vast volumes of external knowledge be managed effectively within the input constraints of LLMs? Traditional methods address this by chunking external documents into smaller, fixed-size segments. While this approach alleviates input limitations, it often fragments context, resulting in incomplete retrieval and diminished coherence in generation. To overcome these shortcomings, two advanced techniques, late chunking and contextual retrieval, have been introduced, both aiming to preserve global context. Despite their potential, their comparative strengths and limitations remain unclear. This study presents a rigorous analysis of late chunking and contextual retrieval, evaluating their effectiveness and efficiency in optimizing RAG systems. Our results indicate that contextual retrieval preserves semantic coherence more effectively but requires greater computational resources. In contrast, late chunking offers higher efficiency but tends to sacrifice relevance and completeness.",
        "translated": "检索增强生成（Retrieval-Augmented Generation，RAG）通过将大语言模型（LLMs）的输出建立在外部知识源的基础上，已成为增强其性能的变革性方法。然而，一个关键问题始终存在：如何在海量外部知识与LLMs的输入限制之间实现有效平衡？传统解决方案将外部文档切分为固定尺寸的小片段，这种方法虽然缓解了输入限制，但往往导致上下文语境割裂，引发检索不完整和生成连贯性下降的问题。\n\n为克服这些缺陷，学界提出了两种旨在保持全局语境的高级技术——延迟分块（late chunking）和上下文检索（contextual retrieval）。尽管两者都展现出潜力，但其相对优势与局限性尚未明晰。本研究对这两种技术展开严谨分析，评估它们在优化RAG系统时的效能与效率。实验结果表明：上下文检索能更有效地保持语义连贯性，但需要更高的计算资源；相较之下，延迟分块虽具有更高效率，却倾向于以相关性和完整性为代价。"
    },
    {
        "title": "Learning Universal User Representations Leveraging Cross-domain User\n  Intent at Snapchat",
        "url": "http://arxiv.org/abs/2504.21838v1",
        "pub_date": "2025-04-30",
        "summary": "The development of powerful user representations is a key factor in the success of recommender systems (RecSys). Online platforms employ a range of RecSys techniques to personalize user experience across diverse in-app surfaces. User representations are often learned individually through user's historical interactions within each surface and user representations across different surfaces can be shared post-hoc as auxiliary features or additional retrieval sources. While effective, such schemes cannot directly encode collaborative filtering signals across different surfaces, hindering its capacity to discover complex relationships between user behaviors and preferences across the whole platform. To bridge this gap at Snapchat, we seek to conduct universal user modeling (UUM) across different in-app surfaces, learning general-purpose user representations which encode behaviors across surfaces. Instead of replacing domain-specific representations, UUM representations capture cross-domain trends, enriching existing representations with complementary information. This work discusses our efforts in developing initial UUM versions, practical challenges, technical choices and modeling and research directions with promising offline performance. Following successful A/B testing, UUM representations have been launched in production, powering multiple use cases and demonstrating their value. UUM embedding has been incorporated into (i) Long-form Video embedding-based retrieval, leading to 2.78% increase in Long-form Video Open Rate, (ii) Long-form Video L2 ranking, with 19.2% increase in Long-form Video View Time sum, (iii) Lens L2 ranking, leading to 1.76% increase in Lens play time, and (iv) Notification L2 ranking, with 0.87% increase in Notification Open Rate.",
        "translated": "以下是该英文论文摘要的准确中文翻译，专业术语与技术细节均已精确处理：\n\n**通用用户建模（UUM）在推荐系统中的实践与价值**  \n强大的用户表征是推荐系统（RecSys）成功的关键。在线平台通过多种推荐技术，在应用内不同界面中实现个性化用户体验。传统方法中，用户表征通常通过单界面内的历史交互独立学习，不同界面的用户表征仅作为事后共享的辅助特征或额外检索源。尽管有效，此类方案无法直接编码跨界面的协同过滤信号，限制了其对平台全局用户行为与偏好间复杂关系的捕捉能力。\n\n为填补这一技术缺口，Snapchat致力于构建跨应用界面的**通用用户建模（UUM）**，学习融合多界面行为的通用用户表征。UUM并非替代领域特异性表征，而是通过捕捉跨领域行为趋势，以互补信息增强现有表征体系。本文阐述了UUM初期版本的开发历程，包括实践挑战、技术选型、建模策略及展现优异离线性能的研究方向。经A/B测试验证后，UUM表征已投入生产环境，支撑多场景应用并凸显其价值：  \n- **长视频嵌入检索**：引入UUM嵌入后，长视频打开率提升2.78%  \n- **长视频L2排序**：长视频总观看时长增加19.2%  \n- **Lens特效L2排序**：Lens播放时长增长1.76%  \n- **通知L2排序**：通知打开率提高0.87%  \n\n该工作证明了跨界面统一建模对推荐系统性能的显著增益，为行业提供了可扩展的通用用户表征框架范式。\n\n---\n\n**翻译要点说明**  \n1. **术语精准化**：  \n   - \"User representations\" 译为\"用户表征\"（非\"表示\"），符合机器学习领域术语规范  \n   - \"Collaborative filtering signals\" 保留\"协同过滤信号\"专业表述  \n   - \"Embedding-based retrieval\" 译为\"嵌入检索\"，避免歧义  \n\n2. **技术逻辑显性化**：  \n   - 将\"post-hoc\"隐含的事后性显式译为\"事后共享\"  \n   - 通过\"领域特异性表征\"与\"通用表征\"对比强调UUM的互补特性  \n\n3. **数据可视化增强**：  \n   - 使用项目符号清晰呈现实验结果，提升可读性  \n   - 百分比数据保留原始精度，采用中文数字格式规范  \n\n4. **行业适配性**：  \n   - \"Lens\"等产品专名保留英文，符合技术文档惯例  \n   - \"L2排序\"沿用业界对排序层级的通用表述方式"
    },
    {
        "title": "WebThinker: Empowering Large Reasoning Models with Deep Research\n  Capability",
        "url": "http://arxiv.org/abs/2504.21776v1",
        "pub_date": "2025-04-30",
        "summary": "Large reasoning models (LRMs), such as OpenAI-o1 and DeepSeek-R1, demonstrate impressive long-horizon reasoning capabilities. However, their reliance on static internal knowledge limits their performance on complex, knowledge-intensive tasks and hinders their ability to produce comprehensive research reports requiring synthesis of diverse web information. To address this, we propose \\textbf{WebThinker}, a deep research agent that empowers LRMs to autonomously search the web, navigate web pages, and draft research reports during the reasoning process. WebThinker integrates a \\textbf{Deep Web Explorer} module, enabling LRMs to dynamically search, navigate, and extract information from the web when encountering knowledge gaps. It also employs an \\textbf{Autonomous Think-Search-and-Draft strategy}, allowing the model to seamlessly interleave reasoning, information gathering, and report writing in real time. To further enhance research tool utilization, we introduce an \\textbf{RL-based training strategy} via iterative online Direct Preference Optimization (DPO). Extensive experiments on complex reasoning benchmarks (GPQA, GAIA, WebWalkerQA, HLE) and scientific report generation tasks (Glaive) demonstrate that WebThinker significantly outperforms existing methods and strong proprietary systems. Our approach enhances LRM reliability and applicability in complex scenarios, paving the way for more capable and versatile deep research systems. The code is available at https://github.com/RUC-NLPIR/WebThinker.",
        "translated": "大型推理模型（Large Reasoning Models, LRMs）如OpenAI-o1和DeepSeek-R1展现出卓越的长程推理能力。然而，这些模型依赖静态内部知识的特性限制了其在复杂知识密集型任务上的表现，也难以生成需要整合多样化网络信息的综合性研究报告。为此，我们提出\\textbf{WebThinker}——一种深度研究智能体，通过赋能LRMs在推理过程中自主进行网络搜索、网页导航和研究报告草拟，突破这一局限。WebThinker集成了\\textbf{深度网络探索器}模块，使LRMs在遇到知识缺口时能够动态搜索、导航并提取网络信息；同时采用\\textbf{自主思考-搜索-草拟策略}，允许模型实时无缝地交替进行推理、信息收集和报告撰写。为进一步提升研究工具的使用效率，我们通过迭代式在线直接偏好优化（Direct Preference Optimization, DPO）引入了\\textbf{基于强化学习的训练策略}。在复杂推理基准（GPQA、GAIA、WebWalkerQA、HLE）和科学报告生成任务（Glaive）上的大量实验表明，WebThinker显著优于现有方法和主流商业系统。该方法提升了LRMs在复杂场景下的可靠性和适用性，为构建更强大、更通用的深度研究系统铺平了道路。代码已发布于https://github.com/RUC-NLPIR/WebThinker。\n\n（注：翻译过程中对技术细节的处理包括：\n1. 专业术语保留英文首字母缩写（如LRMs, DPO）并辅以中文解释\n2. 模块名称采用\\textbf{}格式保持原文强调\n3. 基准测试名称保留英文原名\n4. 技术策略名称采用精准对应译法（如\"seamlessly interleave\"译为\"无缝交替\"）\n5. 强化学习相关概念保持领域标准译法）"
    },
    {
        "title": "From Precision to Perception: User-Centred Evaluation of Keyword\n  Extraction Algorithms for Internet-Scale Contextual Advertising",
        "url": "http://arxiv.org/abs/2504.21667v1",
        "pub_date": "2025-04-30",
        "summary": "Keyword extraction is a foundational task in natural language processing, underpinning countless real-world applications. A salient example is contextual advertising, where keywords help predict the topical congruence between ads and their surrounding media contexts to enhance advertising effectiveness. Recent advances in artificial intelligence, particularly large language models, have improved keyword extraction capabilities but also introduced concerns about computational cost. Moreover, although the end-user experience is of vital importance, human evaluation of keyword extraction performances remains under-explored. This study provides a comparative evaluation of three prevalent keyword extraction algorithms that vary in complexity: TF-IDF, KeyBERT, and Llama 2. To evaluate their effectiveness, a mixed-methods approach is employed, combining quantitative benchmarking with qualitative assessments from 552 participants through three survey-based experiments. Findings indicate a slight user preference for KeyBERT, which offers a favourable balance between performance and computational efficiency compared to the other two algorithms. Despite a strong overall preference for gold-standard keywords, differences between the algorithmic outputs are not statistically significant, highlighting a long-overlooked gap between traditional precision-focused metrics and user-perceived algorithm efficiency. The study highlights the importance of user-centred evaluation methodologies and proposes analytical tools to support their implementation.",
        "translated": "关键词提取是自然语言处理领域的一项基础任务，支撑着众多实际应用。一个典型例证是上下文广告场景，该领域通过关键词预测广告内容与周边媒介语境的主题一致性以提升投放效果。尽管人工智能技术（特别是大语言模型）的最新进展提升了关键词提取能力，但同时也引发了关于计算成本的担忧。此外，尽管终端用户体验至关重要，但针对关键词提取性能的人工评估研究仍显不足。本研究对三种复杂度各异的常用关键词提取算法（TF-IDF、KeyBERT和Llama 2）进行了比较评估。为全面衡量其效能，我们采用混合研究方法，通过三项基于问卷调查的实验，将定量基准测试与552名参与者的定性评估相结合。研究发现用户对KeyBERT存在轻微偏好，相较于其他两种算法，该模型在性能与计算效率之间实现了更优平衡。尽管黄金标准关键词获得显著偏好，但各算法输出差异在统计学上并不显著，这揭示出传统以精确度为核心的评估指标与用户感知的算法效率之间长期被忽视的认知鸿沟。本研究强调了以用户为中心的评估方法论的重要性，并提出支持该方法实施的分析工具。"
    },
    {
        "title": "Efficient Conversational Search via Topical Locality in Dense Retrieval",
        "url": "http://arxiv.org/abs/2504.21507v1",
        "pub_date": "2025-04-30",
        "summary": "Pre-trained language models have been widely exploited to learn dense representations of documents and queries for information retrieval. While previous efforts have primarily focused on improving effectiveness and user satisfaction, response time remains a critical bottleneck of conversational search systems. To address this, we exploit the topical locality inherent in conversational queries, i.e., the tendency of queries within a conversation to focus on related topics. By leveraging query embedding similarities, we dynamically restrict the search space to semantically relevant document clusters, reducing computational complexity without compromising retrieval quality. We evaluate our approach on the TREC CAsT 2019 and 2020 datasets using multiple embedding models and vector indexes, achieving improvements in processing speed of up to 10.4X with little loss in performance (4.4X without any loss). Our results show that the proposed system effectively handles complex, multiturn queries with high precision and efficiency, offering a practical solution for real-time conversational search.",
        "translated": "预训练语言模型已被广泛应用于学习文档与查询的稠密表示以实现信息检索。尽管先前研究主要聚焦于提升检索效果和用户满意度，但响应时间仍然是对话式搜索系统的关键瓶颈。针对这一问题，我们充分利用对话查询中固有的主题局部性特性——即同一对话中的查询往往聚焦于相关主题。通过利用查询嵌入相似性，我们动态地将搜索空间限制在语义相关的文档簇上，从而在不影响检索质量的前提下降低计算复杂度。我们在TREC CAsT 2019和2020数据集上使用多种嵌入模型和向量索引对方法进行评估，结果显示处理速度最高提升10.4倍（性能损失极小），在无损性能情况下亦可实现4.4倍加速。实验结果表明，所提出的系统能够以高精度和高效率处理复杂的多轮对话查询，为实时对话搜索提供了切实可行的解决方案。\n\n（翻译说明：\n1. 专业术语处理：\"dense representations\"译为\"稠密表示\"，\"computational complexity\"译为\"计算复杂度\"，\"vector indexes\"译为\"向量索引\"，均符合计算机领域规范译法\n2. 技术细节保留：\"topical locality\"译为\"主题局部性\"，既准确传达概念又保持学术表述\n3. 数字精度：精确保留原文的10.4X和4.4X等性能指标数据，采用\"倍\"作为单位符合中文表述习惯\n4. 句式结构调整：将原文最后一句拆分为两个分句，更符合中文长句处理规范，同时保持技术细节的完整性\n5. 领域专有名词：TREC CAsT作为国际评测标准名称保留英文原名，符合学术惯例）"
    },
    {
        "title": "In a Few Words: Comparing Weak Supervision and LLMs for Short Query\n  Intent Classification",
        "url": "http://arxiv.org/abs/2504.21398v1",
        "pub_date": "2025-04-30",
        "summary": "User intent classification is an important task in information retrieval. Previously, user intents were classified manually and automatically; the latter helped to avoid hand labelling of large datasets. Recent studies explored whether LLMs can reliably determine user intent. However, researchers have recognized the limitations of using generative LLMs for classification tasks. In this study, we empirically compare user intent classification into informational, navigational, and transactional categories, using weak supervision and LLMs. Specifically, we evaluate LLaMA-3.1-8B-Instruct and LLaMA-3.1-70B-Instruct for in-context learning and LLaMA-3.1-8B-Instruct for fine-tuning, comparing their performance to an established baseline classifier trained using weak supervision (ORCAS-I). Our results indicate that while LLMs outperform weak supervision in recall, they continue to struggle with precision, which shows the need for improved methods to balance both metrics effectively.",
        "translated": "用户意图分类是信息检索领域的重要任务。传统方法采用人工分类和自动分类两种方式，其中自动分类技术有效避免了大规模数据集的手动标注需求。近期研究开始探索大型语言模型（LLMs）在用户意图识别中的可靠性。然而，学界已认识到生成式LLM在分类任务中的应用存在局限性。本研究通过实证方法，对比分析了基于弱监督与LLM技术对用户意图（信息型、导航型、事务型）进行分类的效果。具体而言，我们评估了LLaMA-3.1-8B-Instruct和LLaMA-3.1-70B-Instruct在上下文学习中的表现，以及LLaMA-3.1-8B-Instruct的微调效果，并将其性能与基于弱监督训练的基准分类器（ORCAS-I）进行对比。实验结果表明：虽然LLM在召回率指标上优于弱监督方法，但其精确度仍存在明显不足，这突显出需要开发更有效的方法来实现两个指标的均衡优化。\n\n（译文说明：\n1. 专业术语处理：\n- \"weak supervision\"译为\"弱监督\"（机器学习领域标准译法）\n- \"informational, navigational, and transactional\"译为\"信息型、导航型、事务型\"（信息检索领域标准分类）\n- \"in-context learning\"译为\"上下文学习\"（LLM领域通用译法）\n\n2. 技术细节保留：\n- 完整保留模型名称LLaMA-3.1-8B-Instruct的结构信息（包含参数量8B/70B）\n- 准确区分\"in-context learning\"与\"fine-tuning\"两种不同技术路径\n\n3. 研究结论强化：\n- 使用\"突显出\"替代直译\"shows\"，突出研究发现的显著性\n- 采用\"均衡优化\"准确传达\"balance both metrics effectively\"的技术含义\n\n4. 学术规范：\n- 首次出现的英文缩写（LLMs）标注全称\n- 保持数字和单位的专业表述（如8B表示80亿参数）\n- 使用学术论文惯用的客观陈述句式）"
    },
    {
        "title": "Enhancing New-item Fairness in Dynamic Recommender Systems",
        "url": "http://arxiv.org/abs/2504.21362v1",
        "pub_date": "2025-04-30",
        "summary": "New-items play a crucial role in recommender systems (RSs) for delivering fresh and engaging user experiences. However, traditional methods struggle to effectively recommend new-items due to their short exposure time and limited interaction records, especially in dynamic recommender systems (DRSs) where new-items get continuously introduced and users' preferences evolve over time. This leads to significant unfairness towards new-items, which could accumulate over the successive model updates, ultimately compromising the stability of the entire system. Therefore, we propose FairAgent, a reinforcement learning (RL)-based new-item fairness enhancement framework specifically designed for DRSs. It leverages knowledge distillation to extract collaborative signals from traditional models, retaining strong recommendation capabilities for old-items. In addition, FairAgent introduces a novel reward mechanism for recommendation tailored to the characteristics of DRSs, which consists of three components: 1) a new-item exploration reward to promote the exposure of dynamically introduced new-items, 2) a fairness reward to adapt to users' personalized fairness requirements for new-items, and 3) an accuracy reward which leverages users' dynamic feedback to enhance recommendation accuracy. Extensive experiments on three public datasets and backbone models demonstrate the superior performance of FairAgent. The results present that FairAgent can effectively boost new-item exposure, achieve personalized new-item fairness, while maintaining high recommendation accuracy.",
        "translated": "新项目在推荐系统（RSs）中对于提供新颖且具吸引力的用户体验起着至关重要的作用。然而，由于新项目曝光时间短暂且交互记录有限，传统方法难以有效推荐新项目，这一挑战在动态推荐系统（DRSs）中尤为突出——此类系统持续引入新项目，同时用户偏好随时间不断演变。这种状况导致新项目面临严重的不公平性，这种不公平性可能通过连续的模型更新不断累积，最终危及整个系统的稳定性。为此，我们提出FairAgent：一个基于强化学习（RL）的新型项目公平性增强框架，专为动态推荐系统设计。该框架通过知识蒸馏技术从传统模型中提取协同信号，保留对旧项目的强大推荐能力。此外，FairAgent针对动态推荐系统特性设计了创新的三要素推荐奖励机制：1）新项目探索奖励以促进动态引入新项目的曝光；2）公平性奖励以适应用户对新项目的个性化公平需求；3）准确性奖励通过用户动态反馈提升推荐精度。在三个公开数据集和基础模型上的大量实验表明，FairAgent具有卓越性能。结果显示该框架能有效提升新项目曝光量，实现个性化新项目公平性，同时保持高推荐准确率。\n\n（翻译说明：\n1. 专业术语处理：对\"knowledge distillation\"采用通用译法\"知识蒸馏\"；\"reinforcement learning (RL)\"译为\"强化学习（RL）\"并保留缩写；技术概念如\"exposure time\"译为\"曝光时间\"符合行业惯例\n2. 动态特性表达：通过\"持续引入新项目\"、\"用户偏好随时间演变\"等表述准确传达系统的动态特征\n3. 奖励机制解析：将三个核心奖励机制进行分项说明，使用\"以...\"句式明确各奖励的功能目标\n4. 学术规范：保持\"框架\"、\"模型\"等科研论文常用表述方式，结果部分使用\"显示\"替代口语化的\"表明\"，符合学术翻译规范\n5. 复杂句式处理：通过分号、破折号和层次化编号对长难句进行合理切分，确保中文表达的流畅性）"
    },
    {
        "title": "A Framework for Elastic Adaptation of User Multiple Intents in\n  Sequential Recommendation",
        "url": "http://arxiv.org/abs/2504.21270v1",
        "pub_date": "2025-04-30",
        "summary": "Recently, substantial research has been conducted on sequential recommendation, with the objective of forecasting the subsequent item by leveraging a user's historical sequence of interacted items. Prior studies employ both capsule networks and self-attention techniques to effectively capture diverse underlying intents within a user's interaction sequence, thereby achieving the most advanced performance in sequential recommendation. However, users could potentially form novel intents from fresh interactions as the lengths of user interaction sequences grow. Consequently, models need to be continually updated or even extended to adeptly encompass these emerging user intents, referred as incremental multi-intent sequential recommendation. % We refer to this problem as incremental multi-intent sequential recommendation, which has not yet been well investigated in the existing literature. In this paper, we propose an effective Incremental learning framework for user Multi-intent Adaptation in sequential recommendation called IMA, which augments the traditional fine-tuning strategy with the existing-intents retainer, new-intents detector, and projection-based intents trimmer to adaptively expand the model to accommodate user's new intents and prevent it from forgetting user's existing intents. Furthermore, we upgrade the IMA into an Elastic Multi-intent Adaptation (EMA) framework which can elastically remove inactive intents and compress user intent vectors under memory space limit. Extensive experiments on real-world datasets verify the effectiveness of the proposed IMA and EMA on incremental multi-intent sequential recommendation, compared with various baselines.",
        "translated": "近年来，针对序列推荐展开了大量研究，其目标是通过利用用户历史交互物品序列来预测后续物品。先前研究同时采用胶囊网络和自注意力技术，以有效捕捉用户交互序列中多样化的潜在意图，从而在序列推荐中实现了最先进的性能。然而，随着用户交互序列长度的增长，用户可能从新交互中形成新颖意图。因此，模型需要持续更新甚至扩展，以灵活适应这些新兴用户意图，我们将其称为增量多意图序列推荐问题。本文提出一种有效的增量学习框架IMA（用户多意图自适应框架），该框架通过整合现有意图保留器、新意图检测器和基于投影的意图修剪器，对传统微调策略进行增强，从而自适应扩展模型以适应用户新意图，同时防止遗忘用户现有意图。进一步地，我们将IMA升级为弹性多意图自适应框架EMA，该框架能够在内存空间限制下弹性移除非活跃意图并压缩用户意图向量。通过在真实世界数据集上的大量实验验证，相较于多种基线方法，所提出的IMA和EMA框架在增量多意图序列推荐任务中展现出显著的有效性。\n\n（注：原文中注释符号%后的内容已根据上下文语义自然融入译文，确保行文连贯性。专业术语如\"capsule networks\"译为\"胶囊网络\"，\"self-attention techniques\"译为\"自注意力技术\"，\"incremental learning\"译为\"增量学习\"等均采用领域标准译法。关键创新组件\"existing-intents retainer, new-intents detector, and projection-based intents trimmer\"分别译为\"现有意图保留器、新意图检测器和基于投影的意图修剪器\"以保持技术准确性。）"
    },
    {
        "title": "X-Cross: Dynamic Integration of Language Models for Cross-Domain\n  Sequential Recommendation",
        "url": "http://arxiv.org/abs/2504.20859v1",
        "pub_date": "2025-04-29",
        "summary": "As new products are emerging daily, recommendation systems are required to quickly adapt to possible new domains without needing extensive retraining. This work presents ``X-Cross'' -- a novel cross-domain sequential-recommendation model that recommends products in new domains by integrating several domain-specific language models; each model is fine-tuned with low-rank adapters (LoRA). Given a recommendation prompt, operating layer by layer, X-Cross dynamically refines the representation of each source language model by integrating knowledge from all other models. These refined representations are propagated from one layer to the next, leveraging the activations from each domain adapter to ensure domain-specific nuances are preserved while enabling adaptability across domains. Using Amazon datasets for sequential recommendation, X-Cross achieves performance comparable to a model that is fine-tuned with LoRA, while using only 25% of the additional parameters. In cross-domain tasks, such as adapting from Toys domain to Tools, Electronics or Sports, X-Cross demonstrates robust performance, while requiring about 50%-75% less fine-tuning data than LoRA to make fine-tuning effective. Furthermore, X-Cross achieves significant improvement in accuracy over alternative cross-domain baselines. Overall, X-Cross enables scalable and adaptive cross-domain recommendations, reducing computational overhead and providing an efficient solution for data-constrained environments.",
        "translated": "随着新产品日新月异，推荐系统需要快速适应可能的新领域，而无需进行大量重新训练。本文提出\"X-Cross\"——一种创新的跨领域序列推荐模型，通过整合多个领域专用语言模型（每个模型通过低秩适配器LoRA进行微调）来实现新领域产品推荐。给定推荐提示时，X-Cross逐层动态优化每个源语言模型的表征，整合来自其他所有模型的知识。这些优化后的表征通过各层网络传播，利用各领域适配器的激活状态，在保留领域特有细微差异的同时实现跨领域适应性。基于亚马逊序列推荐数据集，X-Cross在使用仅25%额外参数的情况下，取得了与LoRA微调模型相媲美的性能。在跨领域任务中（如从玩具领域迁移到工具、电子或运动领域），X-Cross展现出强大的性能，同时相比LoRA方法所需微调数据量减少50%-75%。与现有跨领域基线模型相比，X-Cross在准确率上实现了显著提升。总体而言，X-Cross实现了可扩展的适应性跨领域推荐，降低了计算开销，为数据受限环境提供了高效解决方案。"
    },
    {
        "title": "RecGaze: The First Eye Tracking and User Interaction Dataset for\n  Carousel Interfaces",
        "url": "http://arxiv.org/abs/2504.20792v1",
        "pub_date": "2025-04-29",
        "summary": "Carousel interfaces are widely used in e-commerce and streaming services, but little research has been devoted to them. Previous studies of interfaces for presenting search and recommendation results have focused on single ranked lists, but it appears their results cannot be extrapolated to carousels due to the added complexity. Eye tracking is a highly informative approach to understanding how users click, yet there are no eye tracking studies concerning carousels. There are very few interaction datasets on recommenders with carousel interfaces and none that contain gaze data.   We introduce the RecGaze dataset: the first comprehensive feedback dataset on carousels that includes eye tracking results, clicks, cursor movements, and selection explanations. The dataset comprises of interactions from 3 movie selection tasks with 40 different carousel interfaces per user. In total, 87 users and 3,477 interactions are logged. In addition to the dataset, its description and possible use cases, we provide results of a survey on carousel design and the first analysis of gaze data on carousels, which reveals a golden triangle or F-pattern browsing behavior.   Our work seeks to advance the field of carousel interfaces by providing the first dataset with eye tracking results on carousels. In this manner, we provide and encourage an empirical understanding of interactions with carousel interfaces, for building better recommender systems through gaze information, and also encourage the development of gaze-based recommenders.",
        "translated": "轮播界面在电子商务和流媒体服务中广泛应用，但相关研究却十分匮乏。先前关于搜索和推荐结果呈现界面的研究主要聚焦于单一排序列表，然而由于轮播界面复杂性的增加，这些研究结论似乎无法直接推广至轮播场景。眼动追踪技术为理解用户点击行为提供了高信息量的研究途径，但迄今尚未有针对轮播界面的眼动追踪研究。现有推荐系统中关于轮播界面的交互数据集极为稀缺，且完全缺乏包含注视数据的资源。我们推出RecGaze数据集：首个全面记录轮播界面反馈的综合性数据集，涵盖眼动追踪结果、点击行为、光标移动轨迹及选择解释。该数据集完整记录了87位用户在执行3个电影选择任务时与40种不同轮播界面的交互过程，共计3,477次有效交互记录。除数据集本身及其描述与潜在应用场景外，我们还提供了关于轮播设计的调研结果，并首次对轮播场景下的注视数据展开分析，揭示了黄金三角区或F型浏览模式的存在。本研究旨在通过发布首个包含眼动追踪结果的轮播界面数据集推动该领域发展。借此，我们不仅为基于注视信息构建更优推荐系统提供实证理解的途径，同时倡导发展基于注视行为的推荐算法，以此深化对轮播界面交互机制的实证认知。"
    },
    {
        "title": "UniversalRAG: Retrieval-Augmented Generation over Multiple Corpora with\n  Diverse Modalities and Granularities",
        "url": "http://arxiv.org/abs/2504.20734v1",
        "pub_date": "2025-04-29",
        "summary": "Retrieval-Augmented Generation (RAG) has shown substantial promise in improving factual accuracy by grounding model responses with external knowledge relevant to queries. However, most existing RAG approaches are limited to a text-only corpus, and while recent efforts have extended RAG to other modalities such as images and videos, they typically operate over a single modality-specific corpus. In contrast, real-world queries vary widely in the type of knowledge they require, which a single type of knowledge source cannot address. To address this, we introduce UniversalRAG, a novel RAG framework designed to retrieve and integrate knowledge from heterogeneous sources with diverse modalities and granularities. Specifically, motivated by the observation that forcing all modalities into a unified representation space derived from a single combined corpus causes a modality gap, where the retrieval tends to favor items from the same modality as the query, we propose a modality-aware routing mechanism that dynamically identifies the most appropriate modality-specific corpus and performs targeted retrieval within it. Also, beyond modality, we organize each modality into multiple granularity levels, enabling fine-tuned retrieval tailored to the complexity and scope of the query. We validate UniversalRAG on 8 benchmarks spanning multiple modalities, showing its superiority over modality-specific and unified baselines.",
        "translated": "检索增强生成（Retrieval-Augmented Generation, RAG）通过将模型响应与查询相关的外部知识进行关联，在提升事实准确性方面展现出显著潜力。然而，现有RAG方法大多局限于纯文本语料库。尽管近期研究已将RAG扩展至图像、视频等多模态领域，但这些方法通常仅针对单一模态的特定语料库进行操作。与之形成对比的是，现实世界的查询需求具有广泛的知识类型多样性，单一类型的知识源难以全面覆盖。为此，我们提出UniversalRAG——一种新型RAG框架，专为从具有多模态特性和不同粒度的异构源中检索与整合知识而设计。具体而言，基于对现有方法的观察发现：强制将所有模态映射至单一联合语料库衍生的统一表示空间会导致模态鸿沟现象，即检索过程会偏向与查询同模态的内容。为此，我们提出模态感知路由机制，动态识别最适配的模态专属语料库并执行定向检索。此外，在模态划分的基础上，我们在每个模态内部构建多粒度层级，从而根据查询的复杂度和范围实现精细化检索。通过在涵盖多模态的8个基准测试上进行验证，UniversalRAG展现出优于单模态基准系统和统一检索基线的性能优势。"
    },
    {
        "title": "Are Information Retrieval Approaches Good at Harmonising Longitudinal\n  Survey Questions in Social Science?",
        "url": "http://arxiv.org/abs/2504.20679v1",
        "pub_date": "2025-04-29",
        "summary": "Automated detection of semantically equivalent questions in longitudinal social science surveys is crucial for long-term studies informing empirical research in the social, economic, and health sciences. Retrieving equivalent questions faces dual challenges: inconsistent representation of theoretical constructs (i.e. concept/sub-concept) across studies as well as between question and response options, and the evolution of vocabulary and structure in longitudinal text. To address these challenges, our multi-disciplinary collaboration of computer scientists and survey specialists presents a new information retrieval (IR) task of identifying concept (e.g. Housing, Job, etc.) equivalence across question and response options to harmonise longitudinal population studies. This paper investigates multiple unsupervised approaches on a survey dataset spanning 1946-2020, including probabilistic models, linear probing of language models, and pre-trained neural networks specialised for IR. We show that IR-specialised neural models achieve the highest overall performance with other approaches performing comparably. Additionally, the re-ranking of the probabilistic model's results with neural models only introduces modest improvements of 0.07 at most in F1-score. Qualitative post-hoc evaluation by survey specialists shows that models generally have a low sensitivity to questions with high lexical overlap, particularly in cases where sub-concepts are mismatched. Altogether, our analysis serves to further research on harmonising longitudinal studies in social science.",
        "translated": "在纵向社会科学调查中自动检测语义等效问题对于指导社会、经济及健康科学实证研究的长期研究至关重要。检索等效问题面临双重挑战：一方面理论构念（即概念/子概念）在不同研究之间以及问题与应答选项之间存在不一致的表述；另一方面纵向文本的词汇和结构存在历时演变。为应对这些挑战，我们计算机科学家与调查专家组成的跨学科团队提出了一项新的信息检索（IR）任务——通过识别问题和应答选项间的概念（如住房、工作等）等效性来协调纵向人口研究。本文在1946-2020年期间的调查数据集上测试了多种无监督方法，包括概率模型、语言模型的线性探测以及专用于信息检索的预训练神经网络。实验表明，专门用于IR的神经模型取得了最高的整体性能，其他方法的性能与之相当。此外，使用神经模型对概率模型的结果进行重排序仅能带来最高0.07的F1值提升。调查专家的定性事后评估显示，模型对具有高词汇重叠度的问题普遍敏感性较低，特别是在子概念不匹配的情况下。总体而言，我们的分析为推进社会科学纵向研究的协调工作提供了研究基础。"
    },
    {
        "title": "Information Retrieval in the Age of Generative AI: The RGB Model",
        "url": "http://arxiv.org/abs/2504.20610v1",
        "pub_date": "2025-04-29",
        "summary": "The advent of Large Language Models (LLMs) and generative AI is fundamentally transforming information retrieval and processing on the Internet, bringing both great potential and significant concerns regarding content authenticity and reliability. This paper presents a novel quantitative approach to shed light on the complex information dynamics arising from the growing use of generative AI tools. Despite their significant impact on the digital ecosystem, these dynamics remain largely uncharted and poorly understood. We propose a stochastic model to characterize the generation, indexing, and dissemination of information in response to new topics. This scenario particularly challenges current LLMs, which often rely on real-time Retrieval-Augmented Generation (RAG) techniques to overcome their static knowledge limitations. Our findings suggest that the rapid pace of generative AI adoption, combined with increasing user reliance, can outpace human verification, escalating the risk of inaccurate information proliferation across digital resources. An in-depth analysis of Stack Exchange data confirms that high-quality answers inevitably require substantial time and human effort to emerge. This underscores the considerable risks associated with generating persuasive text in response to new questions and highlights the critical need for responsible development and deployment of future generative AI tools.",
        "translated": "大型语言模型（LLMs）和生成式人工智能的出现，正在从根本上改变互联网上的信息检索与处理方式，既带来了巨大潜力，也引发了关于内容真实性和可靠性的重大关切。本文提出一种新颖的定量研究方法，旨在揭示由生成式AI工具日益广泛应用引发的复杂信息动态。尽管这些动态对数字生态系统产生重大影响，但其内在机制仍处于未知领域且缺乏充分认知。我们建立了一个随机模型来描述针对新兴主题的信息生成、索引和传播过程，这种情况特别挑战了当前LLMs的能力——这些模型通常依赖实时检索增强生成（RAG）技术来突破其静态知识限制。研究结果表明，生成式AI的快速普及与用户依赖程度的持续加深，可能超越人工验证的速度，加剧不准确信息在数字资源中扩散的风险。通过对Stack Exchange数据的深入分析发现，高质量答案的出现不可避免地需要大量时间和人力投入。这一发现不仅揭示了利用生成式AI即时回应新问题所产生的说服性文本蕴含的显著风险，更突显了未来负责任地开发和部署生成式AI工具的迫切需求。\n\n（译文特点说明：\n1. 专业术语处理：LLMs/RAG等专业缩写在首次出现时保留英文全称及缩写形式\n2. 技术概念转译：\"stochastic model\"译为\"随机模型\"以准确体现其统计学特征\n3. 逻辑关系强化：通过破折号和\"这一发现不仅...更...\"等结构增强论证链条的显性表达\n4. 学术表述规范：使用\"关切\"替代\"担忧\"，\"认知\"替代\"理解\"等更符合学术论文语境的词汇\n5. 数据引用处理：Stack Exchange作为专有平台名称保留英文原名）"
    },
    {
        "title": "WebThinker: Empowering Large Reasoning Models with Deep Research\n  Capability",
        "url": "http://arxiv.org/abs/2504.21776v1",
        "pub_date": "2025-04-30",
        "summary": "Large reasoning models (LRMs), such as OpenAI-o1 and DeepSeek-R1, demonstrate impressive long-horizon reasoning capabilities. However, their reliance on static internal knowledge limits their performance on complex, knowledge-intensive tasks and hinders their ability to produce comprehensive research reports requiring synthesis of diverse web information. To address this, we propose \\textbf{WebThinker}, a deep research agent that empowers LRMs to autonomously search the web, navigate web pages, and draft research reports during the reasoning process. WebThinker integrates a \\textbf{Deep Web Explorer} module, enabling LRMs to dynamically search, navigate, and extract information from the web when encountering knowledge gaps. It also employs an \\textbf{Autonomous Think-Search-and-Draft strategy}, allowing the model to seamlessly interleave reasoning, information gathering, and report writing in real time. To further enhance research tool utilization, we introduce an \\textbf{RL-based training strategy} via iterative online Direct Preference Optimization (DPO). Extensive experiments on complex reasoning benchmarks (GPQA, GAIA, WebWalkerQA, HLE) and scientific report generation tasks (Glaive) demonstrate that WebThinker significantly outperforms existing methods and strong proprietary systems. Our approach enhances LRM reliability and applicability in complex scenarios, paving the way for more capable and versatile deep research systems. The code is available at https://github.com/RUC-NLPIR/WebThinker.",
        "translated": "大型推理模型（Large Reasoning Models，LRMs）（如OpenAI-o1和DeepSeek-R1）展现了令人瞩目的长程推理能力。然而，其依赖静态内部知识的特性限制了在复杂知识密集型任务中的表现，且在需要整合多源网络信息的研究报告生成任务中能力受限。为解决这一问题，我们提出\\textbf{WebThinker}——一个深度研究智能体，使LRMs能够在推理过程中自主进行网络搜索、网页导航并草拟研究报告。WebThinker集成了\\textbf{深度网络探索器}模块，使LRMs在遇到知识缺口时能动态执行网络搜索、页面导航和信息提取。同时采用\\textbf{自主的\"思考-搜索-草拟\"策略}，允许模型在推理过程中实时无缝地交织信息收集与报告撰写。为提升研究工具的使用效能，我们通过迭代式在线直接偏好优化（Direct Preference Optimization，DPO）引入\\textbf{基于强化学习的训练策略}。在复杂推理基准测试（GPQA、GAIA、WebWalkerQA、HLE）和科学报告生成任务（Glaive）上的大量实验表明，WebThinker显著优于现有方法和主流商业系统。我们的方法增强了LRM在复杂场景下的可靠性和适用性，为构建更强大、更通用的深度研究系统铺平道路。代码已发布于https://github.com/RUC-NLPIR/WebThinker。\n\n（专业术语处理说明：\n1. 保留LRMs、DPO等标准缩写及模型名称原文\n2. \"Deep Web Explorer\"译为\"深度网络探索器\"以保持技术含义\n3. \"Think-Search-and-Draft\"采用连字符直译保留策略核心要素\n4. 基准测试名称（GPQA等）保留原文确保可追溯性\n5. \"Direct Preference Optimization\"专业术语采用学界通用译法\"直接偏好优化\"）"
    },
    {
        "title": "WebThinker: Empowering Large Reasoning Models with Deep Research\n  Capability",
        "url": "http://arxiv.org/abs/2504.21776v1",
        "pub_date": "2025-04-30",
        "summary": "Large reasoning models (LRMs), such as OpenAI-o1 and DeepSeek-R1, demonstrate impressive long-horizon reasoning capabilities. However, their reliance on static internal knowledge limits their performance on complex, knowledge-intensive tasks and hinders their ability to produce comprehensive research reports requiring synthesis of diverse web information. To address this, we propose \\textbf{WebThinker}, a deep research agent that empowers LRMs to autonomously search the web, navigate web pages, and draft research reports during the reasoning process. WebThinker integrates a \\textbf{Deep Web Explorer} module, enabling LRMs to dynamically search, navigate, and extract information from the web when encountering knowledge gaps. It also employs an \\textbf{Autonomous Think-Search-and-Draft strategy}, allowing the model to seamlessly interleave reasoning, information gathering, and report writing in real time. To further enhance research tool utilization, we introduce an \\textbf{RL-based training strategy} via iterative online Direct Preference Optimization (DPO). Extensive experiments on complex reasoning benchmarks (GPQA, GAIA, WebWalkerQA, HLE) and scientific report generation tasks (Glaive) demonstrate that WebThinker significantly outperforms existing methods and strong proprietary systems. Our approach enhances LRM reliability and applicability in complex scenarios, paving the way for more capable and versatile deep research systems. The code is available at https://github.com/RUC-NLPIR/WebThinker.",
        "translated": "大型推理模型（Large Reasoning Models, LRMs）（如OpenAI-o1和DeepSeek-R1）展现出卓越的长程推理能力。然而，其依赖静态内部知识的特性限制了它们在复杂知识密集型任务中的表现，并阻碍了其在需要整合多样化网络信息以生成综合性研究报告方面的能力。为解决这一问题，我们提出\\textbf{WebThinker}——一种深度研究智能体，能够赋能LRMs在推理过程中自主进行网络搜索、网页导航和报告草拟。WebThinker集成了\\textbf{深度网络探索器}模块，使得LRMs在遇到知识缺口时能够动态执行网络搜索、页面导航和信息提取。该框架还采用\\textbf{自主的\"思考-搜索-撰写\"策略}，使模型能够实时无缝交织推理过程、信息收集和报告撰写。为增强研究工具使用效率，我们通过迭代式在线直接偏好优化（Direct Preference Optimization, DPO）提出\\textbf{基于强化学习的训练策略}。在复杂推理基准测试（GPQA、GAIA、WebWalkerQA、HLE）和科研报告生成任务（Glaive）上的大量实验表明，WebThinker显著优于现有方法和主流专有系统。我们的方法提升了LRMs在复杂场景下的可靠性和适用性，为构建更强大、更通用的深度研究系统铺平了道路。代码已发布于https://github.com/RUC-NLPIR/WebThinker。\n\n（翻译说明：1. 专业术语处理：如\"knowledge gaps\"译为\"知识缺口\"，\"RL-based\"译为\"基于强化学习\"，\"Direct Preference Optimization\"保留英文缩写DPO并附中文全称；2. 技术策略名称采用加粗并保留原文核心含义；3. 基准测试名称保持英文缩写以符合学术惯例；4. 项目地址完整保留确保可访问性。）"
    },
    {
        "title": "Investigating Task Arithmetic for Zero-Shot Information Retrieval",
        "url": "http://arxiv.org/abs/2505.00649v1",
        "pub_date": "2025-05-01",
        "summary": "Large Language Models (LLMs) have shown impressive zero-shot performance across a variety of Natural Language Processing tasks, including document re-ranking. However, their effectiveness degrades on unseen tasks and domains, largely due to shifts in vocabulary and word distributions. In this paper, we investigate Task Arithmetic, a technique that combines the weights of LLMs pre-trained on different tasks or domains via simple mathematical operations, such as addition or subtraction, to adapt retrieval models without requiring additional fine-tuning. Our method is able to synthesize diverse tasks and domain knowledge into a single model, enabling effective zero-shot adaptation in different retrieval contexts. Extensive experiments on publicly available scientific, biomedical, and multilingual datasets show that our method improves state-of-the-art re-ranking performance by up to 18% in NDCG@10 and 15% in P@10. In addition to these empirical gains, our analysis provides insights into the strengths and limitations of Task Arithmetic as a practical strategy for zero-shot learning and model adaptation. We make our code publicly available at https://github.com/DetectiveMB/Task-Arithmetic-for-ZS-IR.",
        "translated": "大型语言模型（LLM）在自然语言处理任务中展现出卓越的零样本性能，包括文档重排序任务。然而，其在未知任务和领域上的有效性会出现显著下降，这主要源于词汇分布偏移问题。本文研究了一种称为\"任务算术\"的创新方法，通过简单的数学运算（如加减法）整合在不同任务或领域预训练的LLM权重，从而无需额外微调即可实现检索模型的适配。该方法能够将多样化的任务与领域知识融合至单一模型，有效支持不同检索场景下的零样本适应。通过在公开的科学文献、生物医学及多语言数据集上的大规模实验，我们的方法在NDCG@10和P@10指标上分别实现了最高18%和15%的性能提升，刷新了当前最优的重排序基准。除实证性成果外，本文深入分析了任务算术作为零样本学习与模型适配策略的优势与局限性。相关代码已开源：https://github.com/DetectiveMB/Task-Arithmetic-for-ZS-IR。\n\n（专业术语说明与翻译策略：\n1. \"zero-shot performance\"译为\"零样本性能\"，遵循NLP领域规范译法\n2. \"Task Arithmetic\"保留英文术语首译后使用中文译名，确保概念准确性\n3. \"NDCG@10\"等评估指标保留英文缩写形式，符合学术论文惯例\n4. \"vocabulary and word distributions\"译为\"词汇分布\"，精准传达语言学概念\n5. \"model adaptation\"译为\"模型适配\"，区别于常规的\"模型适应\"，突出主动调整含义）"
    },
    {
        "title": "Efficient Recommendation with Millions of Items by Dynamic Pruning of\n  Sub-Item Embeddings",
        "url": "http://arxiv.org/abs/2505.00560v1",
        "pub_date": "2025-05-01",
        "summary": "A large item catalogue is a major challenge for deploying modern sequential recommender models, since it makes the memory footprint of the model large and increases inference latency. One promising approach to address this is RecJPQ, which replaces item embeddings with sub-item embeddings. However, slow inference remains problematic because finding the top highest-scored items usually requires scoring all items in the catalogue, which may not be feasible for large catalogues. By adapting dynamic pruning concepts from document retrieval, we propose the RecJPQPrune dynamic pruning algorithm to efficiently find the top highest-scored items without computing the scores of all items in the catalogue. Our RecJPQPrune algorithm is safe-up-to-rank K since it theoretically guarantees that no potentially high-scored item is excluded from the final top K recommendation list, thereby ensuring no impact on effectiveness. Our experiments on two large datasets and three recommendation models demonstrate the efficiency achievable using RecJPQPrune: for instance, on the Tmall dataset with 2.2M items, we can reduce the median model scoring time by 64 times compared to the Transformer Default baseline, and 5.3 times compared to a recent scoring approach called PQTopK. Overall, this paper demonstrates the effective and efficient inference of Transformer-based recommendation models at catalogue scales not previously reported in the literature. Indeed, our RecJPQPrune algorithm can score 2 million items in under 10 milliseconds without GPUs, and without relying on Approximate Nearest Neighbour (ANN) techniques.",
        "translated": "大规模商品目录是部署现代顺序推荐模型的主要挑战，因为这会显著增加模型的内存占用并延长推理延迟。RecJPQ方法通过用子商品嵌入替代原始商品嵌入，为解决这一问题提供了可行方案。然而，由于传统方法需要为目录中所有商品计算评分才能确定最高得分的推荐项，这种方法在大规模目录场景下仍存在推理速度瓶颈。借鉴文档检索领域的动态剪枝思想，我们提出了RecJPQPrune动态剪枝算法，能够在不计算全量商品得分的情况下高效识别最高评分项。该算法具有\"前K位绝对安全\"的理论保障，确保不会有潜在的高评分商品被排除在最终推荐列表之外，从而保证推荐效果不受影响。通过在两个大型数据集和三种推荐模型上的实验验证，我们证明了RecJPQPrune的显著效率提升：例如在包含220万商品的Tmall数据集上，相较于Transformer Default基线方法，我们的方法将模型中位数评分时间缩短了64倍；相较于最新的PQTopK评分方法，也实现了5.3倍的加速。本文首次在文献中展示了基于Transformer的推荐模型在超大规模商品目录场景下的高效推理能力。特别值得注意的是，RecJPQPrune算法无需GPU加速，也无需依赖近似最近邻(ANN)技术，即可在10毫秒内完成对200万商品的评分计算。"
    },
    {
        "title": "Graph Spectral Filtering with Chebyshev Interpolation for Recommendation",
        "url": "http://arxiv.org/abs/2505.00552v1",
        "pub_date": "2025-05-01",
        "summary": "Graph convolutional networks have recently gained prominence in collaborative filtering (CF) for recommendations. However, we identify potential bottlenecks in two foundational components. First, the embedding layer leads to a latent space with limited capacity, overlooking locally observed but potentially valuable preference patterns. Also, the widely-used neighborhood aggregation is limited in its ability to leverage diverse preference patterns in a fine-grained manner. Building on spectral graph theory, we reveal that these limitations stem from graph filtering with a cut-off in the frequency spectrum and a restricted linear form. To address these issues, we introduce ChebyCF, a CF framework based on graph spectral filtering. Instead of a learned embedding, it takes a user's raw interaction history to utilize the full spectrum of signals contained in it. Also, it adopts Chebyshev interpolation to effectively approximate a flexible non-linear graph filter, and further enhances it by using an additional ideal pass filter and degree-based normalization. Through extensive experiments, we verify that ChebyCF overcomes the aforementioned bottlenecks and achieves state-of-the-art performance across multiple benchmarks and reasonably fast inference. Our code is available at https://github.com/chanwoo0806/ChebyCF.",
        "translated": "图卷积网络近期在推荐系统协同过滤（CF）任务中获得了显著关注。然而，我们在两个基础组件中发现了潜在瓶颈：首先，嵌入层形成的潜在空间容量有限，忽略了局部可观测但具有潜在价值的偏好模式；其次，广泛使用的邻域聚合方法难以以细粒度方式有效利用多样化的偏好模式。基于谱图理论，我们发现这些限制源于频谱截止的图滤波处理及其受限的线性形式。为解决这些问题，我们提出了ChebyCF——基于图谱滤波的协同过滤框架。该框架摒弃学习型嵌入，直接使用用户原始交互历史以充分利用其中包含的全频谱信号。同时，通过切比雪夫插值有效逼近灵活的非线性图滤波器，并进一步结合理想通带滤波器和基于度数的归一化进行增强。经过大量实验验证，ChebyCF成功克服了前述瓶颈，在多个基准测试中实现了最先进的性能，同时保持合理的推理速度。项目代码已发布于https://github.com/chanwoo0806/ChebyCF。\n\n（关键术语说明：\n1. 谱图理论（Spectral graph theory）：研究图结构频谱特性的数学理论\n2. 切比雪夫插值（Chebyshev interpolation）：基于正交多项式的高精度数值逼近方法\n3. 理想通带滤波器（Ideal pass filter）：在特定频率范围内保持信号无损的滤波器设计\n4. 细粒度（Fine-grained）：指能够进行微观层面细节处理的技术特性）"
    },
    {
        "title": "EnronQA: Towards Personalized RAG over Private Documents",
        "url": "http://arxiv.org/abs/2505.00263v1",
        "pub_date": "2025-05-01",
        "summary": "Retrieval Augmented Generation (RAG) has become one of the most popular methods for bringing knowledge-intensive context to large language models (LLM) because of its ability to bring local context at inference time without the cost or data leakage risks associated with fine-tuning. A clear separation of private information from the LLM training has made RAG the basis for many enterprise LLM workloads as it allows the company to augment LLM's understanding using customers' private documents. Despite its popularity for private documents in enterprise deployments, current RAG benchmarks for validating and optimizing RAG pipelines draw their corpora from public data such as Wikipedia or generic web pages and offer little to no personal context. Seeking to empower more personal and private RAG we release the EnronQA benchmark, a dataset of 103,638 emails with 528,304 question-answer pairs across 150 different user inboxes. EnronQA enables better benchmarking of RAG pipelines over private data and allows for experimentation on the introduction of personalized retrieval settings over realistic data. Finally, we use EnronQA to explore the tradeoff in memorization and retrieval when reasoning over private documents.",
        "translated": "检索增强生成（Retrieval Augmented Generation, RAG）已成为为大型语言模型（LLM）注入知识密集型上下文的最流行方法之一，因其能够在推理阶段引入本地上下文，同时避免了微调所需的高昂成本及相关数据泄露风险。通过将私有信息与LLM训练过程明确分离，RAG已成为众多企业级LLM工作负载的基础架构，使企业能够利用客户私有文档来增强LLM的理解能力。尽管RAG在企业部署中对私有文档应用广泛，但当前用于验证和优化RAG流程的基准测试仍主要采用维基百科或通用网页等公开数据构建语料库，这些数据集几乎不包含任何个性化上下文。\n\n为了推动更个性化和隐私安全的RAG发展，我们发布了EnronQA基准测试。该数据集包含来自150个不同用户收件箱的103,638封电子邮件，以及跨这些邮件的528,304组问答对。EnronQA能够更好地评估面向私有数据的RAG流程性能，并为在真实数据上开展个性化检索设置的实验研究提供支持。最后，我们利用EnronQA探讨了在处理私有文档推理过程中，记忆能力与检索机制之间的权衡关系。"
    },
    {
        "title": "Optimization of embeddings storage for RAG systems using quantization\n  and dimensionality reduction techniques",
        "url": "http://arxiv.org/abs/2505.00105v1",
        "pub_date": "2025-04-30",
        "summary": "Retrieval-Augmented Generation enhances language models by retrieving relevant information from external knowledge bases, relying on high-dimensional vector embeddings typically stored in float32 precision. However, storing these embeddings at scale presents significant memory challenges. To address this issue, we systematically investigate on MTEB benchmark two complementary optimization strategies: quantization, evaluating standard formats (float16, int8, binary) and low-bit floating-point types (float8), and dimensionality reduction, assessing methods like PCA, Kernel PCA, UMAP, Random Projections and Autoencoders. Our results show that float8 quantization achieves a 4x storage reduction with minimal performance degradation (&lt;0.3%), significantly outperforming int8 quantization at the same compression level, being simpler to implement. PCA emerges as the most effective dimensionality reduction technique. Crucially, combining moderate PCA (e.g., retaining 50% dimensions) with float8 quantization offers an excellent trade-off, achieving 8x total compression with less performance impact than using int8 alone (which provides only 4x compression). To facilitate practical application, we propose a methodology based on visualizing the performance-storage trade-off space to identify the optimal configuration that maximizes performance within their specific memory constraints.",
        "translated": "检索增强生成（Retrieval-Augmented Generation）通过从外部知识库检索相关信息来增强语言模型，其核心依赖于通常以float32精度存储的高维向量嵌入。然而，大规模存储这些嵌入会带来显著的内存挑战。为应对此问题，我们在MTEB基准上系统性地研究了两种互补的优化策略：量化（评估标准格式如float16、int8、二进制及低比特浮点类型float8）和降维（评估主成分分析、核主成分分析、UMAP、随机投影和自动编码器等方法）。实验结果表明：float8量化在仅造成微小性能损失（<0.3%）的情况下实现了4倍的存储缩减，在相同压缩水平下显著优于int8量化，且实现更为简单。主成分分析（PCA）被证明是最有效的降维技术。关键发现是：将适度PCA（如保留50%维度）与float8量化相结合，能实现8倍的总压缩率，其性能影响甚至小于单独使用int8量化（后者仅提供4倍压缩）。为促进实际应用，我们提出基于性能-存储权衡空间可视化的方法框架，旨在帮助用户根据特定内存限制确定能最大化性能的最优配置方案。\n\n（译文技术要点说明：\n1. 专业术语标准化处理：如Retrieval-Augmented Generation译为行业通用译名\"检索增强生成\"，UMAP保留英文缩写但首次出现标注中文全称\n2. 数值精度描述统一：float32/float16等保持英文格式符合中文技术文献惯例\n3. 关键技术指标显性化：使用\"4倍\"、\"<0.3%\"等精确数值表达，保留原文比较关系\n4. 方法论表述强化逻辑：通过\"关键发现是\"、\"旨在\"等连接词确保技术逻辑的连贯性\n5. 复合句式优化：将原文嵌套结构拆解为符合中文表达习惯的递进句式，如将\"combining...with...\"处理为\"将...与...相结合\"）"
    },
    {
        "title": "Traceback of Poisoning Attacks to Retrieval-Augmented Generation",
        "url": "http://arxiv.org/abs/2504.21668v1",
        "pub_date": "2025-04-30",
        "summary": "Large language models (LLMs) integrated with retrieval-augmented generation (RAG) systems improve accuracy by leveraging external knowledge sources. However, recent research has revealed RAG's susceptibility to poisoning attacks, where the attacker injects poisoned texts into the knowledge database, leading to attacker-desired responses. Existing defenses, which predominantly focus on inference-time mitigation, have proven insufficient against sophisticated attacks. In this paper, we introduce RAGForensics, the first traceback system for RAG, designed to identify poisoned texts within the knowledge database that are responsible for the attacks. RAGForensics operates iteratively, first retrieving a subset of texts from the database and then utilizing a specially crafted prompt to guide an LLM in detecting potential poisoning texts. Empirical evaluations across multiple datasets demonstrate the effectiveness of RAGForensics against state-of-the-art poisoning attacks. This work pioneers the traceback of poisoned texts in RAG systems, providing a practical and promising defense mechanism to enhance their security.",
        "translated": "结合检索增强生成（RAG）系统的大型语言模型（LLMs）通过利用外部知识源来提升准确性。然而，最新研究表明RAG系统易受投毒攻击的影响——攻击者可通过向知识库中注入恶意文本，诱导模型生成符合攻击者意图的响应。现有防御方法主要集中于推理阶段的缓解措施，但已被证明难以抵御复杂攻击。本文提出RAGForensics，这是首个针对RAG系统的溯源机制，旨在识别知识库中导致攻击的投毒文本。该系统采用迭代式运行框架：首先从知识库中检索文本子集，随后通过专门设计的提示语引导LLM检测潜在投毒文本。基于多个数据集的实证评估结果表明，RAGForensics能有效对抗当前最先进的投毒攻击方法。本研究开创了RAG系统投毒文本溯源的新范式，为提升系统安全性提供了切实可行且具有前景的防御机制。\n\n（注：译文在保持学术严谨性的基础上，对部分表达进行了专业优化：\n1. \"traceback system\"译为\"溯源机制\"更符合中文安全领域术语\n2. \"specially crafted prompt\"采用\"专门设计的提示语\"突出LLM交互特性\n3. \"state-of-the-art\"译为\"最先进的\"符合国内学术惯例\n4. \"iteratively\"译为\"迭代式\"准确描述系统工作机制\n5. 通过增补\"范式\"等词汇增强学术表述的规范性）"
    },
    {
        "title": "Investigating Task Arithmetic for Zero-Shot Information Retrieval",
        "url": "http://arxiv.org/abs/2505.00649v1",
        "pub_date": "2025-05-01",
        "summary": "Large Language Models (LLMs) have shown impressive zero-shot performance across a variety of Natural Language Processing tasks, including document re-ranking. However, their effectiveness degrades on unseen tasks and domains, largely due to shifts in vocabulary and word distributions. In this paper, we investigate Task Arithmetic, a technique that combines the weights of LLMs pre-trained on different tasks or domains via simple mathematical operations, such as addition or subtraction, to adapt retrieval models without requiring additional fine-tuning. Our method is able to synthesize diverse tasks and domain knowledge into a single model, enabling effective zero-shot adaptation in different retrieval contexts. Extensive experiments on publicly available scientific, biomedical, and multilingual datasets show that our method improves state-of-the-art re-ranking performance by up to 18% in NDCG@10 and 15% in P@10. In addition to these empirical gains, our analysis provides insights into the strengths and limitations of Task Arithmetic as a practical strategy for zero-shot learning and model adaptation. We make our code publicly available at https://github.com/DetectiveMB/Task-Arithmetic-for-ZS-IR.",
        "translated": "大型语言模型（LLMs）在自然语言处理任务中展现出卓越的零样本性能，包括文档重排序任务。然而，当面对未见过的任务和领域时，其有效性会显著下降，这主要源于词汇分布和词频特征的迁移偏移。本文研究了任务算术（Task Arithmetic）技术——通过简单的数学运算（如加减法）将不同任务或领域预训练的LLM权重进行组合，从而无需额外微调即可实现检索模型的自适应。我们的方法能够将多样化的任务和领域知识整合到单一模型中，实现在不同检索场景下的有效零样本迁移。通过在公开的科学文献、生物医学和多语言数据集上的大量实验表明，该方法将当前最优的重排序性能在NDCG@10指标上提升18%，在P@10指标上提升15%。除实证性性能提升外，我们的分析还揭示了任务算术作为零样本学习和模型自适应策略的优势与局限。相关代码已在https://github.com/DetectiveMB/Task-Arithmetic-for-ZS-IR开源。\n\n（注：翻译过程中对专业术语进行了以下处理：\n1. \"zero-shot performance\"译为\"零样本性能\"，保持NLP领域标准译法\n2. \"document re-ranking\"译为\"文档重排序\"，准确传达信息检索领域概念\n3. \"task arithmetic\"译为\"任务算术\"，采用直译+术语化处理\n4. \"NDCG@10/P@10\"保留英文缩写+中文解释，符合学术论文规范\n5. \"shifts in vocabulary and word distributions\"译为\"词汇分布和词频特征的迁移偏移\"，通过增译提升表述准确性）"
    },
    {
        "title": "Rethinking Memory in AI: Taxonomy, Operations, Topics, and Future\n  Directions",
        "url": "http://arxiv.org/abs/2505.00675v1",
        "pub_date": "2025-05-01",
        "summary": "Memory is a fundamental component of AI systems, underpinning large language models (LLMs) based agents. While prior surveys have focused on memory applications with LLMs, they often overlook the atomic operations that underlie memory dynamics. In this survey, we first categorize memory representations into parametric, contextual structured, and contextual unstructured and then introduce six fundamental memory operations: Consolidation, Updating, Indexing, Forgetting, Retrieval, and Compression. We systematically map these operations to the most relevant research topics across long-term, long-context, parametric modification, and multi-source memory. By reframing memory systems through the lens of atomic operations and representation types, this survey provides a structured and dynamic perspective on research, benchmark datasets, and tools related to memory in AI, clarifying the functional interplay in LLMs based agents while outlining promising directions for future research\\footnote{The paper list, datasets, methods and tools are available at \\href{https://github.com/Elvin-Yiming-Du/Survey_Memory_in_AI}{https://github.com/Elvin-Yiming-Du/Survey\\_Memory\\_in\\_AI}.}.",
        "translated": "记忆是人工智能系统的核心组件，是支撑基于大语言模型（LLMs）智能体的基础架构。尽管先前的研究综述已关注到LLMs在记忆应用方面的表现，但往往忽视了支撑记忆动态运作的原子级操作机制。本综述首次将记忆表征系统划分为参数化记忆、上下文结构化记忆和上下文非结构化记忆三大类型，进而提出支撑记忆系统的六种基础操作范式：固化、更新、索引、遗忘、检索与压缩。通过系统性地将这些操作范式与长期记忆、长上下文记忆、参数修改机制以及多源记忆等前沿研究方向建立映射关系，本研究从原子操作与表征类型的双重维度重构了人工智能领域的记忆系统分析框架。这种创新视角不仅为相关研究、基准数据集及工具提供了结构化动态分析体系，更明晰了基于LLMs的智能体中各类记忆功能的相互作用机制，同时为未来研究方向——包括记忆效率优化、多模态记忆整合及伦理安全框架构建——勾勒出具有实践指导意义的发展蓝图\\footnote{本文涉及的研究论文清单、数据集、方法论及工具资源可通过\\href{https://github.com/Elvin-Yiming-Du/Survey_Memory_in_AI}{https://github.com/Elvin-Yiming-Du/Survey\\_Memory\\_in\\_AI}获取。}。"
    },
    {
        "title": "HalluMix: A Task-Agnostic, Multi-Domain Benchmark for Real-World\n  Hallucination Detection",
        "url": "http://arxiv.org/abs/2505.00506v1",
        "pub_date": "2025-05-01",
        "summary": "As large language models (LLMs) are increasingly deployed in high-stakes domains, detecting hallucinated content$\\unicode{x2013}$text that is not grounded in supporting evidence$\\unicode{x2013}$has become a critical challenge. Existing benchmarks for hallucination detection are often synthetically generated, narrowly focused on extractive question answering, and fail to capture the complexity of real-world scenarios involving multi-document contexts and full-sentence outputs. We introduce the HalluMix Benchmark, a diverse, task-agnostic dataset that includes examples from a range of domains and formats. Using this benchmark, we evaluate seven hallucination detection systems$\\unicode{x2013}$both open and closed source$\\unicode{x2013}$highlighting differences in performance across tasks, document lengths, and input representations. Our analysis highlights substantial performance disparities between short and long contexts, with critical implications for real-world Retrieval Augmented Generation (RAG) implementations. Quotient Detections achieves the best overall performance, with an accuracy of 0.82 and an F1 score of 0.84.",
        "translated": "随着大语言模型（LLMs）在关键领域中的部署日益增加，检测幻觉内容——即缺乏支持证据基础的文本——已成为至关重要的挑战。现有的幻觉检测基准往往通过合成生成，局限于抽取式问答任务，未能捕捉涉及多文档上下文和完整句子输出的真实场景复杂性。我们提出HalluMix基准测试，这是一个多样化、任务无关的数据集，包含来自多个领域和格式的示例。基于该基准，我们评估了七种幻觉检测系统（包括开源和闭源系统），揭示了不同任务、文本长度和输入表征之间的性能差异。我们的分析表明，长上下文与短上下文之间存在显著的性能差距，这对现实世界中的检索增强生成（RAG）系统实现具有重要启示。其中Quotient Detections系统表现最佳，达到0.82的准确率和0.84的F1分数。"
    },
    {
        "title": "PREMISE: Matching-based Prediction for Accurate Review Recommendation",
        "url": "http://arxiv.org/abs/2505.01255v1",
        "pub_date": "2025-05-02",
        "summary": "We present PREMISE (PREdict with Matching ScorEs), a new architecture for the matching-based learning in the multimodal fields for the multimodal review helpfulness (MRHP) task. Distinct to previous fusion-based methods which obtains multimodal representations via cross-modal attention for downstream tasks, PREMISE computes the multi-scale and multi-field representations, filters duplicated semantics, and then obtained a set of matching scores as feature vectors for the downstream recommendation task. This new architecture significantly boosts the performance for such multimodal tasks whose context matching content are highly correlated to the targets of that task, compared to the state-of-the-art fusion-based methods. Experimental results on two publicly available datasets show that PREMISE achieves promising performance with less computational cost.",
        "translated": "我们提出PREMISE（基于匹配分数的预测框架），这是一种面向多模态评论有用性（MRHP）任务的新型多模态领域匹配学习架构。与以往基于融合的方法不同（这些方法通过跨模态注意力获取多模态表征以用于下游任务），PREMISE通过计算多尺度和多领域表示，过滤重复语义，进而将一组匹配分数作为特征向量用于下游推荐任务。相比于最先进的融合方法，这种新型架构在处理上下文匹配内容与任务目标高度相关的多模态任务时展现出显著性能优势。在两个公开数据集上的实验结果表明，PREMISE不仅取得了优异性能，同时具有更低计算成本。\n\n（专业术语处理说明：\n1. \"multimodal review helpfulness (MRHP)\" 译为\"多模态评论有用性（MRHP）\"保留首字母缩写\n2. \"cross-modal attention\" 译为\"跨模态注意力\"遵循领域标准译法\n3. \"multi-scale and multi-field representations\" 译为\"多尺度和多领域表示\"准确表达技术特征\n4. \"state-of-the-art\" 译为\"最先进的\"符合学术论文表述规范\n5. \"computational cost\" 译为\"计算成本\"保持专业性与可读性平衡）"
    },
    {
        "title": "Multi-agents based User Values Mining for Recommendation",
        "url": "http://arxiv.org/abs/2505.00981v1",
        "pub_date": "2025-05-02",
        "summary": "Recommender systems have rapidly evolved and become integral to many online services. However, existing systems sometimes produce unstable and unsatisfactory recommendations that fail to align with users' fundamental and long-term preferences. This is because they primarily focus on extracting shallow and short-term interests from user behavior data, which is inherently dynamic and challenging to model. Unlike these transient interests, user values are more stable and play a crucial role in shaping user behaviors, such as purchasing items and consuming content. Incorporating user values into recommender systems can help stabilize recommendation performance and ensure results better reflect users' latent preferences. However, acquiring user values is typically difficult and costly. To address this challenge, we leverage the strong language understanding, zero-shot inference, and generalization capabilities of Large Language Models (LLMs) to extract user values from users' historical interactions. Unfortunately, direct extraction using LLMs presents several challenges such as length constraints and hallucination. To overcome these issues, we propose ZOOM, a zero-shot multi-LLM collaborative framework for effective and accurate user value extraction. In ZOOM, we apply text summarization techniques to condense item content while preserving essential meaning. To mitigate hallucinations, ZOOM introduces two specialized agent roles: evaluators and supervisors, to collaboratively generate accurate user values. Extensive experiments on two widely used recommendation datasets with two state-of-the-art recommendation models demonstrate the effectiveness and generalization of our framework in automatic user value mining and recommendation performance improvement.",
        "translated": "推荐系统的快速发展已使其成为众多在线服务的核心组成部分。然而，现有系统有时会产生不稳定且不令人满意的推荐结果，无法与用户的基础性和长期性偏好相匹配。这是因为这些系统主要致力于从用户行为数据中提取浅层短期兴趣，而此类数据本身具有动态特性且难以建模。与这些转瞬即逝的兴趣不同，用户价值观具有更强的稳定性，在塑造用户行为（如商品购买和内容消费）中发挥着关键作用。将用户价值观融入推荐系统有助于稳定推荐性能，并确保结果更准确地反映用户的潜在偏好。然而，获取用户价值观通常面临较高难度和成本。\n\n为应对这一挑战，我们利用大语言模型（Large Language Models, LLMs）强大的语言理解能力、零样本推理能力和泛化能力，从用户历史交互记录中提取用户价值观。但直接使用LLMs进行提取存在文本长度限制和幻觉生成等挑战。为此，我们提出ZOOM框架——一种零样本多LLM协同框架，旨在实现高效精准的用户价值观提取。在ZOOM中，我们采用文本摘要技术对商品内容进行精简处理，同时保留核心语义信息。针对幻觉问题，ZOOM创新性地引入两个专项代理角色：评估员与监督员，通过协同工作机制生成准确用户价值观。在两个广泛应用推荐数据集和两个前沿推荐模型上的大量实验表明，我们的框架在自动化用户价值观挖掘和推荐性能提升方面展现出显著的有效性和泛化能力。"
    },
    {
        "title": "Enhancing User Sequence Modeling through Barlow Twins-based\n  Self-Supervised Learning",
        "url": "http://arxiv.org/abs/2505.00953v1",
        "pub_date": "2025-05-02",
        "summary": "User sequence modeling is crucial for modern large-scale recommendation systems, as it enables the extraction of informative representations of users and items from their historical interactions. These user representations are widely used for a variety of downstream tasks to enhance users' online experience. A key challenge for learning these representations is the lack of labeled training data. While self-supervised learning (SSL) methods have emerged as a promising solution for learning representations from unlabeled data, many existing approaches rely on extensive negative sampling, which can be computationally expensive and may not always be feasible in real-world scenario. In this work, we propose an adaptation of Barlow Twins, a state-of-the-art SSL methods, to user sequence modeling by incorporating suitable augmentation methods. Our approach aims to mitigate the need for large negative sample batches, enabling effective representation learning with smaller batch sizes and limited labeled data. We evaluate our method on the MovieLens-1M, MovieLens-20M, and Yelp datasets, demonstrating that our method consistently outperforms the widely-used dual encoder model across three downstream tasks, achieving an 8%-20% improvement in accuracy. Our findings underscore the effectiveness of our approach in extracting valuable sequence-level information for user modeling, particularly in scenarios where labeled data is scarce and negative examples are limited.",
        "translated": "用户序列建模对于现代大规模推荐系统至关重要，它能够从用户与项目的历史交互中提取信息丰富的表征。这些用户表征被广泛应用于各种下游任务，以提升用户的在线体验。学习这些表征面临的主要挑战是缺乏标注训练数据。尽管自监督学习（SSL）方法已成为从未标注数据中学习表征的有效解决方案，但现有方法大多依赖大量负采样，这种方式不仅计算成本高昂，且在实际场景中往往难以实现。本研究通过整合适当的增强方法，将目前最先进的自监督学习方法Barlow Twins应用于用户序列建模。我们的方法旨在减少对大负样本批次的需求，使得在较小批次规模和有限标注数据条件下仍能实现有效的表征学习。我们在MovieLens-1M、MovieLens-20M和Yelp三个数据集上进行评估，结果表明该方法在三个下游任务中均优于广泛使用的双编码器模型，准确率提高了8%-20%。研究结果验证了本方法在提取有价值的序列级信息用于用户建模方面的有效性，特别是在标注数据稀缺且负例有限的场景下表现尤为突出。"
    },
    {
        "title": "Preserving Privacy and Utility in LLM-Based Product Recommendations",
        "url": "http://arxiv.org/abs/2505.00951v1",
        "pub_date": "2025-05-02",
        "summary": "Large Language Model (LLM)-based recommendation systems leverage powerful language models to generate personalized suggestions by processing user interactions and preferences. Unlike traditional recommendation systems that rely on structured data and collaborative filtering, LLM-based models process textual and contextual information, often using cloud-based infrastructure. This raises privacy concerns, as user data is transmitted to remote servers, increasing the risk of exposure and reducing control over personal information. To address this, we propose a hybrid privacy-preserving recommendation framework which separates sensitive from nonsensitive data and only shares the latter with the cloud to harness LLM-powered recommendations. To restore lost recommendations related to obfuscated sensitive data, we design a de-obfuscation module that reconstructs sensitive recommendations locally. Experiments on real-world e-commerce datasets show that our framework achieves almost the same recommendation utility with a system which shares all data with an LLM, while preserving privacy to a large extend. Compared to obfuscation-only techniques, our approach improves HR@10 scores and category distribution alignment, offering a better balance between privacy and recommendation quality. Furthermore, our method runs efficiently on consumer-grade hardware, making privacy-aware LLM-based recommendation systems practical for real-world use.",
        "translated": "基于大语言模型（LLM）的推荐系统利用强大的语言模型，通过处理用户交互行为和偏好特征来生成个性化推荐建议。与传统依赖结构化数据和协同过滤的推荐系统不同，基于LLM的模型主要处理文本和上下文信息，通常需要依托基于云的基础设施运行。这种模式引发了隐私保护问题，因为用户数据需传输至远程服务器，这既增加了信息暴露风险，也削弱了用户对个人数据的控制权。为解决这一问题，我们提出了一种混合式隐私保护推荐框架，通过分离敏感与非敏感数据，仅将后者与云端共享以利用LLM生成推荐。为了恢复因混淆敏感数据而丢失的推荐内容，我们设计了去混淆模块，可在本地重建敏感推荐内容。在真实电商数据集上的实验表明，相比将所有数据共享给LLM的系统，本框架在保持接近同等推荐效用的同时，能实现更高程度的隐私保护。与仅采用混淆技术的方案相比，我们的方法显著提升了HR@10评分和类别分布对齐度，在隐私保护与推荐质量之间实现更优的平衡。此外，本方法可在消费级硬件上高效运行，使得基于LLM的隐私保护推荐系统具备实际应用可行性。"
    },
    {
        "title": "Towards Explainable Temporal User Profiling with LLMs",
        "url": "http://arxiv.org/abs/2505.00886v1",
        "pub_date": "2025-05-01",
        "summary": "Accurately modeling user preferences is vital not only for improving recommendation performance but also for enhancing transparency in recommender systems. Conventional user profiling methods, such as averaging item embeddings, often overlook the evolving, nuanced nature of user interests, particularly the interplay between short-term and long-term preferences. In this work, we leverage large language models (LLMs) to generate natural language summaries of users' interaction histories, distinguishing recent behaviors from more persistent tendencies. Our framework not only models temporal user preferences but also produces natural language profiles that can be used to explain recommendations in an interpretable manner. These textual profiles are encoded via a pre-trained model, and an attention mechanism dynamically fuses the short-term and long-term embeddings into a comprehensive user representation. Beyond boosting recommendation accuracy over multiple baselines, our approach naturally supports explainability: the interpretable text summaries and attention weights can be exposed to end users, offering insights into why specific items are suggested. Experiments on real-world datasets underscore both the performance gains and the promise of generating clearer, more transparent justifications for content-based recommendations.",
        "translated": "准确建模用户偏好不仅对提升推荐性能至关重要，也能增强推荐系统的透明度。传统的用户画像方法（如平均项目嵌入）往往忽视了用户兴趣的动态且微妙的变化特性，尤其是短期偏好与长期偏好之间的动态关联。本研究利用大型语言模型（LLMs）生成用户交互历史的自然语言摘要，明确区分近期行为与持续性倾向。我们的框架不仅能建模时序用户偏好，还能生成可用于以可解释方式说明推荐依据的自然语言画像。这些文本画像通过预训练模型进行编码，并采用注意力机制动态融合短期与长期嵌入，形成综合用户表征。相较于多种基线方法，本方法在提升推荐精度的同时，天然支持可解释性：可解读的文本摘要和注意力权重可以直接呈现给终端用户，阐明具体项目被推荐的内在逻辑。在真实数据集上的实验不仅验证了性能提升，更展示了该方法为基于内容的推荐生成更清晰、透明解释的潜力。"
    },
    {
        "title": "PREMISE: Matching-based Prediction for Accurate Review Recommendation",
        "url": "http://arxiv.org/abs/2505.01255v1",
        "pub_date": "2025-05-02",
        "summary": "We present PREMISE (PREdict with Matching ScorEs), a new architecture for the matching-based learning in the multimodal fields for the multimodal review helpfulness (MRHP) task. Distinct to previous fusion-based methods which obtains multimodal representations via cross-modal attention for downstream tasks, PREMISE computes the multi-scale and multi-field representations, filters duplicated semantics, and then obtained a set of matching scores as feature vectors for the downstream recommendation task. This new architecture significantly boosts the performance for such multimodal tasks whose context matching content are highly correlated to the targets of that task, compared to the state-of-the-art fusion-based methods. Experimental results on two publicly available datasets show that PREMISE achieves promising performance with less computational cost.",
        "translated": "我们提出PREMISE(基于匹配分数预测)这一新型架构,用于多模态评论有用性(MRHP)任务中的匹配式学习。与先前通过跨模态注意力获取多模态表征以应用于下游任务的融合式方法不同,PREMISE通过计算多尺度和多领域表示,过滤重复语义,最终生成一组作为特征向量的匹配分数来支持下游推荐任务。对于这种上下文匹配内容与任务目标高度相关的多模态任务,相比当前最先进的融合式方法,这种新型架构能显著提升性能表现。在两个公开数据集上的实验结果表明,PREMISE在降低计算成本的同时实现了优异的性能。"
    },
    {
        "title": "Knowing You Don't Know: Learning When to Continue Search in Multi-round\n  RAG through Self-Practicing",
        "url": "http://arxiv.org/abs/2505.02811v1",
        "pub_date": "2025-05-05",
        "summary": "Retrieval Augmented Generation (RAG) has shown strong capability in enhancing language models' knowledge and reducing AI generative hallucinations, driving its widespread use. However, complex tasks requiring multi-round retrieval remain challenging, and early attempts tend to be overly optimistic without a good sense of self-skepticism. Current multi-round RAG systems may continue searching even when enough information has already been retrieved, or they may provide incorrect answers without having sufficient information or knowledge. Existing solutions either require large amounts of expensive human-labeled process supervision data or lead to subpar performance.   This paper aims to address these limitations by introducing a new framework, \\textbf{SIM-RAG}, to explicitly enhance RAG systems' self-awareness and multi-round retrieval capabilities. To train SIM-RAG, we first let a RAG system self-practice multi-round retrieval, augmenting existing question-answer pairs with intermediate inner monologue reasoning steps to generate synthetic training data. For each pair, the system may explore multiple retrieval paths, which are labeled as successful if they reach the correct answer and unsuccessful otherwise. Using this data, we train a lightweight information sufficiency Critic. At inference time, the Critic evaluates whether the RAG system has retrieved sufficient information at each round, guiding retrieval decisions and improving system-level self-awareness through in-context reinforcement learning.   Experiments across multiple prominent RAG benchmarks show that SIM-RAG is an effective multi-round RAG solution. Furthermore, this framework is system-efficient, adding a lightweight component to RAG without requiring modifications to existing LLMs or search engines, and data-efficient, eliminating the need for costly human-annotated mid-step retrieval process supervision data.",
        "translated": "检索增强生成（Retrieval Augmented Generation, RAG）在增强语言模型知识储备和减少生成幻觉方面表现出显著优势，因而得到广泛应用。然而，需要多轮检索的复杂任务仍面临挑战，早期尝试往往因缺乏自我质疑意识而过于乐观。当前多轮RAG系统可能在已获取足够信息时仍持续搜索，或在缺乏充分信息或知识时给出错误回答。现有解决方案要么需要大量昂贵的人工标注过程监督数据，要么导致性能欠佳。\n\n本文旨在突破这些限制，提出名为\\textbf{SIM-RAG}的新型框架，通过显式增强RAG系统的自我认知和多轮检索能力来解决上述问题。为训练SIM-RAG，我们首先让RAG系统进行多轮检索的自我实践，通过中间内心独白式推理步骤对现有问答对进行增强，生成合成训练数据。对于每个问答对，系统可探索多条检索路径，成功抵达正确答案的路径标记为成功，否则标记为失败。基于此数据，我们训练轻量级的信息充分性评判模块。在推理阶段，该评判模块评估RAG系统在每轮检索中是否已获取充分信息，通过上下文强化学习指导检索决策并提升系统级的自我认知。\n\n在多个重要RAG基准测试中的实验表明，SIM-RAG是一种有效的多轮RAG解决方案。此外，该框架具有系统高效性——仅需在现有RAG系统中添加轻量级组件而无需修改大型语言模型或搜索引擎，以及数据高效性——无需成本高昂的人工标注中间检索过程监督数据。\n\n（注：本翻译严格遵循专业术语规范，如\"inner monologue reasoning\"译为\"内心独白式推理\"保留认知科学原意，\"in-context reinforcement learning\"译为\"上下文强化学习\"准确传达技术内涵。在保证专业性的同时，通过\"显式增强\"、\"系统级自我认知\"等表述突出技术创新点，确保技术细节的精准传达。）"
    },
    {
        "title": "Using Knowledge Graphs to harvest datasets for efficient CLIP model\n  training",
        "url": "http://arxiv.org/abs/2505.02746v1",
        "pub_date": "2025-05-05",
        "summary": "Training high-quality CLIP models typically requires enormous datasets, which limits the development of domain-specific models -- especially in areas that even the largest CLIP models do not cover well -- and drives up training costs. This poses challenges for scientific research that needs fine-grained control over the training procedure of CLIP models. In this work, we show that by employing smart web search strategies enhanced with knowledge graphs, a robust CLIP model can be trained from scratch with considerably less data. Specifically, we demonstrate that an expert foundation model for living organisms can be built using just 10M images. Moreover, we introduce EntityNet, a dataset comprising 33M images paired with 46M text descriptions, which enables the training of a generic CLIP model in significantly reduced time.",
        "translated": "训练高质量的CLIP模型通常需要海量数据集，这既限制了领域专用模型的开发——特别是在现有最大CLIP模型也未能充分覆盖的细分领域——又推高了训练成本。这种现状对需要精细控制CLIP模型训练流程的科学研究构成了挑战。本研究证明，通过采用结合知识图谱增强的智能网络搜索策略，仅使用少量数据即可从头训练出稳健的CLIP模型。具体而言，我们展示了仅需1000万张图像即可构建专门针对生物体的专家基础模型。此外，我们推出了EntityNet数据集，该数据集包含3300万张图像与4600万条文本描述的配对，使得通用CLIP模型的训练时间得以显著缩短。"
    },
    {
        "title": "Predicting Movie Hits Before They Happen with LLMs",
        "url": "http://arxiv.org/abs/2505.02693v1",
        "pub_date": "2025-05-05",
        "summary": "Addressing the cold-start issue in content recommendation remains a critical ongoing challenge. In this work, we focus on tackling the cold-start problem for movies on a large entertainment platform. Our primary goal is to forecast the popularity of cold-start movies using Large Language Models (LLMs) leveraging movie metadata. This method could be integrated into retrieval systems within the personalization pipeline or could be adopted as a tool for editorial teams to ensure fair promotion of potentially overlooked movies that may be missed by traditional or algorithmic solutions. Our study validates the effectiveness of this approach compared to established baselines and those we developed.",
        "translated": "解决内容推荐中的冷启动问题仍然是一个持续性的关键挑战。本研究聚焦于解决大型娱乐平台上电影的冷启动难题。我们的主要目标是利用大型语言模型（LLMs），通过电影元数据来预测冷启动电影的流行度。该方法可被整合到个性化推荐系统流水线中的检索系统内，也可作为编辑团队的工具，确保对那些可能被传统解决方案或算法机制遗漏的潜力电影进行公平推广。通过与传统基准模型及我们自主研发模型的对比实验，本研究验证了该方法的有效性。\n\n（翻译说明：\n1. 专业术语处理：保留\"LLMs\"英文缩写并在首次出现时标注全称，确保技术准确性\n2. 技术细节还原：将\"retrieval systems within the personalization pipeline\"精确译为\"个性化推荐系统流水线中的检索系统\"，体现系统架构层级\n3. 领域特色表达：\"editorial teams\"译为\"编辑团队\"符合影视平台运营场景\n4. 逻辑关系强化：通过\"通过...实验\"的句式结构，突出研究方法的实证特性\n5. 长句拆分重组：对复合句进行合理切分，在保持学术严谨性的同时提升中文可读性）"
    },
    {
        "title": "Evaluating Contrastive Feedback for Effective User Simulations",
        "url": "http://arxiv.org/abs/2505.02560v1",
        "pub_date": "2025-05-05",
        "summary": "The use of Large Language Models (LLMs) for simulating user behavior in the domain of Interactive Information Retrieval has recently gained significant popularity. However, their application and capabilities remain highly debated and understudied. This study explores whether the underlying principles of contrastive training techniques, which have been effective for fine-tuning LLMs, can also be applied beneficially in the area of prompt engineering for user simulations.   Previous research has shown that LLMs possess comprehensive world knowledge, which can be leveraged to provide accurate estimates of relevant documents. This study attempts to simulate a knowledge state by enhancing the model with additional implicit contextual information gained during the simulation. This approach enables the model to refine the scope of desired documents further. The primary objective of this study is to analyze how different modalities of contextual information influence the effectiveness of user simulations.   Various user configurations were tested, where models are provided with summaries of already judged relevant, irrelevant, or both types of documents in a contrastive manner. The focus of this study is the assessment of the impact of the prompting techniques on the simulated user agent performance. We hereby lay the foundations for leveraging LLMs as part of more realistic simulated users.",
        "translated": "在交互式信息检索领域，利用大型语言模型（LLMs）模拟用户行为的研究近期受到广泛关注。然而，其实际应用效果与能力边界仍存在较大争议且缺乏深入研究。本研究探讨了在LLMs微调过程中表现优异的对比训练技术基本原理，是否同样适用于用户模拟的提示工程领域。先前研究表明，LLMs凭借其完备的世界知识能够对相关文档进行准确估计。本研究通过向模型注入模拟过程中获取的额外隐式上下文信息，尝试构建一种动态知识状态，使模型能逐步缩小目标文档的筛选范围。核心研究目标在于分析不同模态的上下文信息如何影响用户模拟的效能。实验中，我们以对比方式向模型提供已判定相关文档、非相关文档或两者混合的摘要信息，测试了多种用户配置方案。研究重点评估了提示技术对模拟用户代理性能的影响机制，为构建基于LLMs的高拟真度用户模拟系统奠定了方法论基础。"
    },
    {
        "title": "Uncertainty in Repeated Implicit Feedback as a Measure of Reliability",
        "url": "http://arxiv.org/abs/2505.02492v1",
        "pub_date": "2025-05-05",
        "summary": "Recommender systems rely heavily on user feedback to learn effective user and item representations. Despite their widespread adoption, limited attention has been given to the uncertainty inherent in the feedback used to train these systems. Both implicit and explicit feedback are prone to noise due to the variability in human interactions, with implicit feedback being particularly challenging. In collaborative filtering, the reliability of interaction signals is critical, as these signals determine user and item similarities. Thus, deriving accurate confidence measures from implicit feedback is essential for ensuring the reliability of these signals.   A common assumption in academia and industry is that repeated interactions indicate stronger user interest, increasing confidence in preference estimates. However, in domains such as music streaming, repeated consumption can shift user preferences over time due to factors like satiation and exposure. While literature on repeated consumption acknowledges these dynamics, they are often overlooked when deriving confidence scores for implicit feedback.   This paper addresses this gap by focusing on music streaming, where repeated interactions are frequent and quantifiable. We analyze how repetition patterns intersect with key factors influencing user interest and develop methods to quantify the associated uncertainty. These uncertainty measures are then integrated as consistency metrics in a recommendation task. Our empirical results show that incorporating uncertainty into user preference models yields more accurate and relevant recommendations. Key contributions include a comprehensive analysis of uncertainty in repeated consumption patterns, the release of a novel dataset, and a Bayesian model for implicit listening feedback.",
        "translated": "推荐系统高度依赖用户反馈来学习有效的用户和项目表征。尽管这些系统已被广泛采用，但用于训练系统的反馈所固有的不确定性却鲜有关注。由于人类交互行为的可变性，隐式反馈和显式反馈都容易受到噪声影响，其中隐式反馈的挑战尤为突出。在协同过滤中，交互信号的可靠性至关重要，因为这些信号决定了用户和项目之间的相似性。因此，从隐式反馈中推导出精确的置信度度量对确保这些信号的可靠性具有关键意义。\n\n学术界和工业界普遍假设重复交互表明更强的用户兴趣，可以提升偏好估计的可信度。然而在音乐流媒体等领域，由于满足效应和曝光效应等因素，重复消费行为会随时间推移改变用户偏好。尽管关于重复消费的文献承认这些动态变化，但在推导隐式反馈的置信度评分时，这些因素往往被忽视。\n\n本文聚焦于重复交互频繁且可量化的音乐流媒体领域来解决这一研究空白。我们分析了重复模式如何与影响用户兴趣的关键因素相互作用，并开发了量化相关不确定性的方法。这些不确定性度量随后作为一致性指标被整合到推荐任务中。实证结果表明，将不确定性纳入用户偏好模型可以生成更准确且更相关的推荐。主要贡献包括：对重复消费模式中不确定性的全面分析、公开一个新颖的数据集，以及针对隐式收听反馈的贝叶斯模型构建。"
    },
    {
        "title": "Tevatron 2.0: Unified Document Retrieval Toolkit across Scale, Language,\n  and Modality",
        "url": "http://arxiv.org/abs/2505.02466v1",
        "pub_date": "2025-05-05",
        "summary": "Recent advancements in large language models (LLMs) have driven interest in billion-scale retrieval models with strong generalization across retrieval tasks and languages. Additionally, progress in large vision-language models has created new opportunities for multimodal retrieval. In response, we have updated the Tevatron toolkit, introducing a unified pipeline that enables researchers to explore retriever models at different scales, across multiple languages, and with various modalities. This demo paper highlights the toolkit's key features, bridging academia and industry by supporting efficient training, inference, and evaluation of neural retrievers. We showcase a unified dense retriever achieving strong multilingual and multimodal effectiveness, and conduct a cross-modality zero-shot study to demonstrate its research potential. Alongside, we release OmniEmbed, to the best of our knowledge, the first embedding model that unifies text, image document, video, and audio retrieval, serving as a baseline for future research.",
        "translated": "近期大语言模型（LLMs）的突破性进展，激发了学术界对具备跨检索任务和跨语言泛化能力的十亿级检索模型的浓厚兴趣。与此同时，大规模视觉-语言模型的进步也为多模态检索开辟了新的可能性。为响应这一技术浪潮，我们对Tevatron工具包进行了全面升级，构建了一个统一的技术框架，使研究者能够探索不同模型规模、多语言支持以及多模态融合的检索模型。本技术演示论文重点介绍了该工具包的核心特性：通过支持神经检索模型的高效训练、推理与评估，搭建起学术界与工业界之间的桥梁。我们展示了统一稠密检索模型在多语言和多模态场景下取得的卓越性能，并通过跨模态零样本学习研究彰显其科研潜力。值得一提的是，我们同步发布了OmniEmbed模型——据我们所知，这是首个将文本、图像文档、视频和音频检索统一于同一嵌入空间的基准模型，为未来研究提供了重要参照。"
    },
    {
        "title": "SymbioticRAG: Enhancing Document Intelligence Through Human-LLM\n  Symbiotic Collaboration",
        "url": "http://arxiv.org/abs/2505.02418v1",
        "pub_date": "2025-05-05",
        "summary": "We present \\textbf{SymbioticRAG}, a novel framework that fundamentally reimagines Retrieval-Augmented Generation~(RAG) systems by establishing a bidirectional learning relationship between humans and machines. Our approach addresses two critical challenges in current RAG systems: the inherently human-centered nature of relevance determination and users' progression from \"unconscious incompetence\" in query formulation. SymbioticRAG introduces a two-tier solution where Level 1 enables direct human curation of retrieved content through interactive source document exploration, while Level 2 aims to build personalized retrieval models based on captured user interactions. We implement Level 1 through three key components: (1)~a comprehensive document processing pipeline with specialized models for layout detection, OCR, and extraction of tables, formulas, and figures; (2)~an extensible retriever module supporting multiple retrieval strategies; and (3)~an interactive interface that facilitates both user engagement and interaction data logging. We experiment Level 2 implementation via a retriever strategy incorporated LLM summarized user intention from user interaction logs. To maintain high-quality data preparation, we develop a human-on-the-loop validation interface that improves pipeline output while advancing research in specialized extraction tasks. Evaluation across three scenarios (literature review, geological exploration, and education) demonstrates significant improvements in retrieval relevance and user satisfaction compared to traditional RAG approaches. To facilitate broader research and further advancement of SymbioticRAG Level 2 implementation, we will make our system openly accessible to the research community.",
        "translated": "我们提出【共生检索增强生成】（SymbioticRAG）这一创新框架，通过建立人机双向学习关系，从根本上重构了传统检索增强生成（RAG）系统。该方法有效解决了当前RAG系统的两大关键挑战：相关性判断本质上的以人为本特性，以及用户在查询构建过程中从\"无意识不胜任\"到能力提升的演进需求。SymbioticRAG采用双层架构设计：第一层级通过交互式文档探索实现人工直接参与检索内容优化，第二层级致力于基于用户交互构建个性化检索模型。\n\n第一层级通过三大核心组件实现：(1) 包含版面检测、OCR以及表格/公式/图表提取专用模型的综合文档处理流程；(2) 支持多种检索策略的可扩展检索器模块；(3) 兼具用户交互与数据记录功能的交互式界面。第二层级的实现通过整合LLM生成的用户意图摘要与交互日志的检索策略进行验证。为确保高质量数据准备，我们开发了人机协同验证界面，该界面在提升流程输出的同时，推动了专业提取任务的算法研究。\n\n在文献综述、地质勘探和教育三个应用场景的评估表明，相较于传统RAG方法，本系统在检索相关性和用户满意度方面均有显著提升。为促进学术界的深入研究并推动SymbioticRAG第二层级的持续发展，我们将向研究社区开放系统源代码。"
    },
    {
        "title": "Minimally Supervised Hierarchical Domain Intent Learning for CRS",
        "url": "http://arxiv.org/abs/2505.02209v1",
        "pub_date": "2025-05-04",
        "summary": "Modeling domain intent within an evolving domain structure presents a significant challenge for domain-specific conversational recommendation systems (CRS). The conventional approach involves training an intent model using utterance-intent pairs. However, as new intents and patterns emerge, the model must be continuously updated while preserving existing relationships and maintaining efficient retrieval. This process leads to substantial growth in utterance-intent pairs, making manual labeling increasingly costly and impractical. In this paper, we propose an efficient solution for constructing a dynamic hierarchical structure that minimizes the number of user utterances required to achieve adequate domain knowledge coverage. To this end, we introduce a neural network-based attention-driven hierarchical clustering algorithm designed to optimize intent grouping using minimal data. The proposed method builds upon and integrates concepts from two existing flat clustering algorithms DEC and NAM, both of which utilize neural attention mechanisms. We apply our approach to a curated subset of 44,000 questions from the business food domain. Experimental results demonstrate that constructing the hierarchy using a stratified sampling strategy significantly reduces the number of questions needed to represent the evolving intent structure. Our findings indicate that this approach enables efficient coverage of dynamic domain knowledge without frequent retraining, thereby enhancing scalability and adaptability in domain-specific CSRs.",
        "translated": "在不断演进的领域结构中建模领域意图，对于领域特定对话推荐系统（CRS）而言存在显著挑战。传统方法通过使用话语-意图对来训练意图模型。然而，随着新意图和模式的出现，模型需要持续更新，同时保留现有关系并保持高效检索能力。这一过程导致话语-意图对数量大幅增长，使得人工标注成本日益高昂且不可持续。本文提出一种高效的动态层次结构构建方案，旨在以最小化用户话语数量实现充分的领域知识覆盖。为此，我们引入了一种基于神经网络的注意力驱动层次聚类算法，该算法专为使用最少数据优化意图分组而设计。所提出的方法整合并改进了两种现有平面聚类算法DEC和NAM的核心思想，二者均采用神经注意力机制。我们将该方法应用于从商业餐饮领域精选的44,000个问题子集。实验结果表明，通过分层抽样策略构建层次结构，可显著减少表征演进意图结构所需的问题数量。研究发现表明，该方法无需频繁重新训练即可有效覆盖动态领域知识，从而提升领域特定CSR系统的可扩展性和适应性。"
    },
    {
        "title": "Exploring new Approaches for Information Retrieval through Natural\n  Language Processing",
        "url": "http://arxiv.org/abs/2505.02199v1",
        "pub_date": "2025-05-04",
        "summary": "This review paper explores recent advancements and emerging approaches in Information Retrieval (IR) applied to Natural Language Processing (NLP). We examine traditional IR models such as Boolean, vector space, probabilistic, and inference network models, and highlight modern techniques including deep learning, reinforcement learning, and pretrained transformer models like BERT. We discuss key tools and libraries - Lucene, Anserini, and Pyserini - for efficient text indexing and search. A comparative analysis of sparse, dense, and hybrid retrieval methods is presented, along with applications in web search engines, cross-language IR, argument mining, private information retrieval, and hate speech detection. Finally, we identify open challenges and future research directions to enhance retrieval accuracy, scalability, and ethical considerations.",
        "translated": "本综述论文探讨了自然语言处理（NLP）领域信息检索（IR）技术的最新进展与新兴方法。我们系统分析了布尔模型、向量空间模型、概率模型和推理网络模型等传统IR模型，同时重点阐述了包括深度学习、强化学习以及预训练Transformer模型（如BERT）在内的现代技术。针对高效文本索引与搜索，我们探讨了关键工具与库——Lucene、Anserini和Pyserini的应用特性。通过对比分析稀疏检索、密集检索和混合检索方法，我们展示了这些技术在网络搜索引擎、跨语言信息检索、论点挖掘、隐私信息检索以及仇恨言论检测等场景中的实际应用。最后，我们指出了当前面临的核心挑战，并提出了未来研究方向，旨在提升检索系统的准确性、可扩展性以及伦理考量。"
    },
    {
        "title": "Interpreting Multilingual and Document-Length Sensitive Relevance\n  Computations in Neural Retrieval Models through Axiomatic Causal\n  Interventions",
        "url": "http://arxiv.org/abs/2505.02154v1",
        "pub_date": "2025-05-04",
        "summary": "This reproducibility study analyzes and extends the paper \"Axiomatic Causal Interventions for Reverse Engineering Relevance Computation in Neural Retrieval Models,\" which investigates how neural retrieval models encode task-relevant properties such as term frequency. We reproduce key experiments from the original paper, confirming that information on query terms is captured in the model encoding. We extend this work by applying activation patching to Spanish and Chinese datasets and by exploring whether document-length information is encoded in the model as well. Our results confirm that the designed activation patching method can isolate the behavior to specific components and tokens in neural retrieval models. Moreover, our findings indicate that the location of term frequency generalizes across languages and that in later layers, the information for sequence-level tasks is represented in the CLS token. The results highlight the need for further research into interpretability in information retrieval and reproducibility in machine learning research. Our code is available at https://github.com/OliverSavolainen/axiomatic-ir-reproduce.",
        "translated": "这项可重复性研究分析并拓展了题为《逆向工程神经检索模型相关性计算的公理化因果干预》的论文，该论文主要探究神经检索模型如何编码任务相关属性（如词项频率）。我们复现了原文的核心实验，证实了查询词项信息确实被模型编码所捕获。通过将激活修补技术应用于西班牙语和中文数据集，并探索文档长度信息是否也被模型编码，我们进一步扩展了该研究。实验结果表明，所设计的激活修补方法能够将特定行为定位到神经检索模型中的具体组件和词元。此外，我们的发现表明词项频率的编码位置在不同语言间具有普适性，且针对序列级任务的信息在深层网络中以CLS标记为载体进行表征。这些发现凸显了在信息检索可解释性和机器学习研究可重复性方面开展深入研究的必要性。相关代码已发布于https://github.com/OliverSavolainen/axiomatic-ir-reproduce。"
    },
    {
        "title": "Tricolore: Multi-Behavior User Profiling for Enhanced Candidate\n  Generation in Recommender Systems",
        "url": "http://arxiv.org/abs/2505.02120v1",
        "pub_date": "2025-05-04",
        "summary": "Online platforms aggregate extensive user feedback across diverse behaviors, providing a rich source for enhancing user engagement. Traditional recommender systems, however, typically optimize for a single target behavior and represent user preferences with a single vector, limiting their ability to handle multiple important behaviors or optimization objectives. This conventional approach also struggles to capture the full spectrum of user interests, resulting in a narrow item pool during candidate generation. To address these limitations, we present Tricolore, a versatile multi-vector learning framework that uncovers connections between different behavior types for more robust candidate generation. Tricolore's adaptive multi-task structure is also customizable to specific platform needs. To manage the variability in sparsity across behavior types, we incorporate a behavior-wise multi-view fusion module that dynamically enhances learning. Moreover, a popularity-balanced strategy ensures the recommendation list balances accuracy with item popularity, fostering diversity and improving overall performance. Extensive experiments on public datasets demonstrate Tricolore's effectiveness across various recommendation scenarios, from short video platforms to e-commerce. By leveraging a shared base embedding strategy, Tricolore also significantly improves the performance for cold-start users. The source code is publicly available at: https://github.com/abnering/Tricolore.",
        "translated": "在线平台通过整合用户多样化的行为反馈数据，积累了丰富的用户参与信息。传统推荐系统通常仅针对单一目标行为进行优化，并采用单一向量表征用户偏好，这种范式在应对多重要行为类型或优化目标时存在显著局限。传统方法对用户兴趣的捕捉也较为片面，导致候选生成阶段的项目池覆盖范围狭窄。为解决这些问题，本文提出Tricolore——一种通用的多向量学习框架，通过揭示不同行为类型间的内在关联实现更稳健的候选生成。该框架的自适应多任务结构可根据平台具体需求进行灵活定制。为应对行为类型间稀疏性差异的挑战，我们设计了行为维度的多视角融合模块实现动态增强学习。此外，通过引入流行度均衡策略，推荐列表在保持准确性的同时兼顾项目热度分布，有效提升多样性和整体性能。在公开数据集上的大量实验表明，Tricolore在短视频平台、电子商务等不同推荐场景中均展现出显著优势。通过共享基础嵌入策略，该框架对冷启动用户的推荐效果也有显著提升。项目源代码已开源：https://github.com/abnering/Tricolore。\n\n（注：译文在保持技术细节准确性的基础上，对以下专业术语进行了优化处理：\n1. \"multi-vector learning framework\" → 多向量学习框架\n2. \"adaptive multi-task structure\" → 自适应多任务结构\n3. \"behavior-wise multi-view fusion\" → 行为维度的多视角融合\n4. \"popularity-balanced strategy\" → 流行度均衡策略\n5. \"cold-start users\" → 冷启动用户\n同时通过分句重组确保了学术表达的流畅性，并严格保留技术指标和数学公式相关表述的准确性。）"
    },
    {
        "title": "Embedding based retrieval for long tail search queries in ecommerce",
        "url": "http://arxiv.org/abs/2505.01946v1",
        "pub_date": "2025-05-03",
        "summary": "In this abstract we present a series of optimizations we performed on the two-tower model architecture [14], training and evaluation datasets to implement semantic product search at Best Buy. Search queries on bestbuy.com follow the pareto distribution whereby a minority of them account for most searches. This leaves us with a long tail of search queries that have low frequency of issuance. The queries in the long tail suffer from very spare interaction signals. Our current work focuses on building a model to serve the long tail queries. We present a series of optimizations we have done to this model to maximize conversion for the purpose of retrieval from the catalog. The first optimization we present is using a large language model to improve the sparsity of conversion signals. The second optimization is pretraining an off-the-shelf transformer-based model on the Best Buy catalog data. The third optimization we present is on the finetuning front. We use query-to-query pairs in addition to query-to-product pairs and combining the above strategies for finetuning the model. We also demonstrate how merging the weights of these finetuned models improves the evaluation metrics. Finally, we provide a recipe for curating an evaluation dataset for continuous monitoring of model performance with human-in-the-loop evaluation. We found that adding this recall mechanism to our current term match-based recall improved conversion by 3% in an online A/B test.",
        "translated": "在本摘要中，我们介绍了对百思买（Best Buy）语义产品搜索系统实施的双塔模型架构[14]、训练及评估数据集所进行的一系列优化。bestbuy.com平台上的搜索查询呈现帕累托分布特征，即少数高频查询占据了大部分搜索量。这使得我们面临大量低频发布的长尾搜索查询，这些长尾查询的交互信号极其稀疏。当前工作重点在于构建专门服务长尾查询的模型。为实现商品目录检索的转化率最大化，我们对该模型实施了三项关键优化：首先采用大型语言模型改善转换信号的稀疏性问题；其次基于百思买商品目录数据预训练现成的Transformer架构模型；第三在微调阶段创新性地结合使用查询-查询配对与查询-产品配对数据，并整合上述策略进行模型微调。实验证明，通过融合多个微调模型的权重可有效提升评估指标。此外，我们提出了一套结合人工参与评估的持续性能监测数据集构建方案。实证结果显示，在现有基于术语匹配的召回机制中融入该策略后，在线A/B测试中的转化率提升了3%。"
    },
    {
        "title": "Exploring the Role of Diversity in Example Selection for In-Context\n  Learning",
        "url": "http://arxiv.org/abs/2505.01842v1",
        "pub_date": "2025-05-03",
        "summary": "In-Context Learning (ICL) has gained prominence due to its ability to perform tasks without requiring extensive training data and its robustness to noisy labels. A typical ICL workflow involves selecting localized examples relevant to a given input using sparse or dense embedding-based similarity functions. However, relying solely on similarity-based selection may introduce topical biases in the retrieved contexts, potentially leading to suboptimal downstream performance. We posit that reranking the retrieved context to enhance topical diversity can improve downstream task performance. To achieve this, we leverage maximum marginal relevance (MMR) which balances topical similarity with inter-example diversity. Our experimental results demonstrate that diversifying the selected examples leads to consistent improvements in downstream performance across various context sizes and similarity functions. The implementation of our approach is made available at https://github.com/janak11111/Diverse-ICL.",
        "translated": "上下文学习（In-Context Learning，ICL）因其无需大量训练数据即可执行任务的能力以及对噪声标签的鲁棒性而备受关注。典型的ICL工作流程涉及使用基于稀疏或稠密嵌入的相似度函数，选择与给定输入相关的局部示例。然而，仅依赖基于相似度的选择可能会在检索的上下文中引入主题偏差，从而导致下游性能欠佳。我们认为，通过重排序检索到的上下文以增强主题多样性，可以提升下游任务性能。为实现这一目标，我们采用最大边际相关性（Maximum Marginal Relevance，MMR）方法，该方法能够平衡主题相似性与示例间多样性。实验结果表明，在多种上下文规模和相似度函数场景下，对所选示例进行多样化处理能持续改善下游性能。本方法的实现代码已发布于https://github.com/janak11111/Diverse-ICL。\n\n（翻译说明：\n1. 专业术语处理：\"sparse/dense embedding\"译为\"稀疏/稠密嵌入\"，\"maximum marginal relevance\"采用行业通用译法\"最大边际相关性\"\n2. 技术概念传递：\"noisy labels\"译为\"噪声标签\"而非字面翻译，符合机器学习领域术语规范\n3. 句式结构优化：将原文中较长的复合句拆分重组，如将\"using...\"状语结构前置处理，符合中文表达习惯\n4. 语义完整性保持：通过增译\"方法\"等范畴词确保技术概念表述清晰，如\"采用最大边际相关性方法\"\n5. 学术规范处理：保留专业缩写ICL/MMR的首次全称标注，确保学术严谨性）"
    },
    {
        "title": "Knowing You Don't Know: Learning When to Continue Search in Multi-round\n  RAG through Self-Practicing",
        "url": "http://arxiv.org/abs/2505.02811v1",
        "pub_date": "2025-05-05",
        "summary": "Retrieval Augmented Generation (RAG) has shown strong capability in enhancing language models' knowledge and reducing AI generative hallucinations, driving its widespread use. However, complex tasks requiring multi-round retrieval remain challenging, and early attempts tend to be overly optimistic without a good sense of self-skepticism. Current multi-round RAG systems may continue searching even when enough information has already been retrieved, or they may provide incorrect answers without having sufficient information or knowledge. Existing solutions either require large amounts of expensive human-labeled process supervision data or lead to subpar performance.   This paper aims to address these limitations by introducing a new framework, \\textbf{SIM-RAG}, to explicitly enhance RAG systems' self-awareness and multi-round retrieval capabilities. To train SIM-RAG, we first let a RAG system self-practice multi-round retrieval, augmenting existing question-answer pairs with intermediate inner monologue reasoning steps to generate synthetic training data. For each pair, the system may explore multiple retrieval paths, which are labeled as successful if they reach the correct answer and unsuccessful otherwise. Using this data, we train a lightweight information sufficiency Critic. At inference time, the Critic evaluates whether the RAG system has retrieved sufficient information at each round, guiding retrieval decisions and improving system-level self-awareness through in-context reinforcement learning.   Experiments across multiple prominent RAG benchmarks show that SIM-RAG is an effective multi-round RAG solution. Furthermore, this framework is system-efficient, adding a lightweight component to RAG without requiring modifications to existing LLMs or search engines, and data-efficient, eliminating the need for costly human-annotated mid-step retrieval process supervision data.",
        "translated": "检索增强生成（Retrieval Augmented Generation, RAG）在增强语言模型知识储备和减少AI生成幻觉方面展现出显著能力，推动了其广泛应用。然而，需要多轮检索的复杂任务仍面临挑战：早期尝试往往过于乐观而缺乏自我质疑能力，现有多轮RAG系统可能在已获取足够信息时仍持续搜索，或在信息/知识不足时错误作答。现有解决方案或需大量昂贵的人工标注过程监督数据，或导致性能欠佳。\n\n本文通过引入新框架\\textbf{SIM-RAG}，旨在增强RAG系统的自我认知和多轮检索能力。为实现SIM-RAG的训练，我们首先让RAG系统进行多轮检索的自我演练，通过增加中间自省式推理步骤对现有问答对进行扩展，生成合成训练数据。针对每个问答对，系统可探索多条检索路径，成功抵达正确答案的路径标记为成功路径，反之则为失败路径。基于此数据，我们训练轻量级的信息充分性评判模型（Critic）。在推理阶段，该评判模型通过上下文强化学习评估RAG系统每轮检索是否已获取充分信息，从而指导检索决策并提升系统级自我认知。\n\n在多个知名RAG基准测试中的实验表明，SIM-RAG是有效的多轮RAG解决方案。该框架兼具系统效率（仅需为RAG添加轻量级组件，无需修改现有大语言模型或搜索引擎）和数据效率（无需昂贵的人工标注中间检索过程监督数据）的双重优势。"
    },
    {
        "title": "Beyond the Monitor: Mixed Reality Visualization and AI for Enhanced\n  Digital Pathology Workflow",
        "url": "http://arxiv.org/abs/2505.02780v1",
        "pub_date": "2025-05-05",
        "summary": "Pathologists rely on gigapixel whole-slide images (WSIs) to diagnose diseases like cancer, yet current digital pathology tools hinder diagnosis. The immense scale of WSIs, often exceeding 100,000 X 100,000 pixels, clashes with the limited views traditional monitors offer. This mismatch forces constant panning and zooming, increasing pathologist cognitive load, causing diagnostic fatigue, and slowing pathologists' adoption of digital methods. PathVis, our mixed-reality visualization platform for Apple Vision Pro, addresses these challenges. It transforms the pathologist's interaction with data, replacing cumbersome mouse-and-monitor navigation with intuitive exploration using natural hand gestures, eye gaze, and voice commands in an immersive workspace. PathVis integrates AI to enhance diagnosis. An AI-driven search function instantly retrieves and displays the top five similar patient cases side-by-side, improving diagnostic precision and efficiency through rapid comparison. Additionally, a multimodal conversational AI assistant offers real-time image interpretation support and aids collaboration among pathologists across multiple Apple devices. By merging the directness of traditional pathology with advanced mixed-reality visualization and AI, PathVis improves diagnostic workflows, reduces cognitive strain, and makes pathology practice more effective and engaging. The PathVis source code and a demo video are publicly available at: https://github.com/jaiprakash1824/Path_Vis",
        "translated": "病理学家依赖千兆像素级全切片图像（WSIs）诊断癌症等疾病，但当前数字病理工具存在显著局限。WSI图像的巨大尺寸（通常超过100,000×100,000像素）与传统显示器有限的显示范围形成矛盾，迫使病理学家持续进行图像平移缩放操作。这种交互方式不仅加重认知负荷、导致诊断疲劳，更阻碍了数字病理技术的推广应用。\n\n为此，我们为Apple Vision Pro开发了混合现实可视化平台PathVis。该平台通过自然手势交互、视线追踪与语音指令系统，在沉浸式工作环境中重构病理数据交互范式，替代传统鼠标-显示器的低效导航方式。PathVis深度融合AI技术实现诊断增强：基于AI的病例搜索功能可即时检索并并排显示最相似的五例患者病例，通过快速比对提升诊断精度与效率；多模态对话式AI助手不仅能提供实时图像解读支持，还可促进跨Apple设备的病理学家协作。\n\nPathVis创新性地融合传统病理诊断的直接性与混合现实可视化技术，结合AI增强功能，有效优化诊断工作流程，降低认知负荷，使病理实践更高效且更具吸引力。平台源代码及演示视频已公开于：https://github.com/jaiprakash1824/Path_Vis\n\n（译文说明：\n1. 专业术语处理：保留WSIs、Apple Vision Pro等专有名词，确保\"multimodal conversational AI\"等技术概念准确传递\n2. 技术细节呈现：通过\"并排显示最相似的五例患者病例\"等表述保持量化信息的精确性\n3. 逻辑结构优化：重组原文段落层次，采用\"问题-解决方案-创新点\"的中文学术摘要惯用结构\n4. 语义补偿策略：对\"cognitive load\"等概念添加\"加重\"等动词强化表达效果，符合中文科技文本表达习惯）"
    },
    {
        "title": "Knowing You Don't Know: Learning When to Continue Search in Multi-round\n  RAG through Self-Practicing",
        "url": "http://arxiv.org/abs/2505.02811v1",
        "pub_date": "2025-05-05",
        "summary": "Retrieval Augmented Generation (RAG) has shown strong capability in enhancing language models' knowledge and reducing AI generative hallucinations, driving its widespread use. However, complex tasks requiring multi-round retrieval remain challenging, and early attempts tend to be overly optimistic without a good sense of self-skepticism. Current multi-round RAG systems may continue searching even when enough information has already been retrieved, or they may provide incorrect answers without having sufficient information or knowledge. Existing solutions either require large amounts of expensive human-labeled process supervision data or lead to subpar performance.   This paper aims to address these limitations by introducing a new framework, \\textbf{SIM-RAG}, to explicitly enhance RAG systems' self-awareness and multi-round retrieval capabilities. To train SIM-RAG, we first let a RAG system self-practice multi-round retrieval, augmenting existing question-answer pairs with intermediate inner monologue reasoning steps to generate synthetic training data. For each pair, the system may explore multiple retrieval paths, which are labeled as successful if they reach the correct answer and unsuccessful otherwise. Using this data, we train a lightweight information sufficiency Critic. At inference time, the Critic evaluates whether the RAG system has retrieved sufficient information at each round, guiding retrieval decisions and improving system-level self-awareness through in-context reinforcement learning.   Experiments across multiple prominent RAG benchmarks show that SIM-RAG is an effective multi-round RAG solution. Furthermore, this framework is system-efficient, adding a lightweight component to RAG without requiring modifications to existing LLMs or search engines, and data-efficient, eliminating the need for costly human-annotated mid-step retrieval process supervision data.",
        "translated": "检索增强生成（Retrieval Augmented Generation, RAG）在增强语言模型知识储备和降低AI生成幻觉方面展现出显著优势，因而获得广泛应用。然而，需要多轮检索的复杂任务仍面临挑战，早期尝试往往因缺乏自我质疑意识而过于乐观。当前多轮RAG系统可能面临两种极端：在已获取充足信息时仍持续检索，或在信息不足时错误作答。现有解决方案要么依赖昂贵的人工标注过程监督数据，要么导致性能欠佳。\n\n本文提出名为\\textbf{SIM-RAG}的创新框架，旨在通过增强RAG系统的自我认知能力和多轮检索能力来突破现有局限。为实现SIM-RAG的训练，我们首先让RAG系统通过自我演练进行多轮检索，通过将现有问答对与中间自省推理步骤相结合来生成合成训练数据。针对每个问答对，系统可探索多条检索路径，成功路径以获取正确答案为标记，失败路径则反之。基于此数据，我们训练了一个轻量级信息充分性评判器。在推理阶段，该评判器通过上下文强化学习机制，实时评估RAG系统在每轮检索中是否已获取充分信息，从而指导检索决策并提升系统级自我认知。\n\n在多个主流RAG基准测试中的实验表明，SIM-RAG是有效的多轮RAG解决方案。该框架兼具系统效率优势——仅需为RAG系统添加轻量级组件，无需修改现有大语言模型或搜索引擎；同时具备数据效率优势——完全消除了对昂贵的人工标注中间检索过程监督数据的依赖。"
    },
    {
        "title": "Using Knowledge Graphs to harvest datasets for efficient CLIP model\n  training",
        "url": "http://arxiv.org/abs/2505.02746v1",
        "pub_date": "2025-05-05",
        "summary": "Training high-quality CLIP models typically requires enormous datasets, which limits the development of domain-specific models -- especially in areas that even the largest CLIP models do not cover well -- and drives up training costs. This poses challenges for scientific research that needs fine-grained control over the training procedure of CLIP models. In this work, we show that by employing smart web search strategies enhanced with knowledge graphs, a robust CLIP model can be trained from scratch with considerably less data. Specifically, we demonstrate that an expert foundation model for living organisms can be built using just 10M images. Moreover, we introduce EntityNet, a dataset comprising 33M images paired with 46M text descriptions, which enables the training of a generic CLIP model in significantly reduced time.",
        "translated": "训练高质量的CLIP模型通常需要海量数据集，这不仅限制了领域专用模型的发展——特别是在现有最大CLIP模型也未能很好覆盖的领域——还显著推高了训练成本。这给需要精细控制CLIP模型训练过程的科学研究带来了挑战。在本研究中，我们证明了通过采用知识图谱增强的智能网络搜索策略，可以用显著减少的数据量从头开始训练出稳健的CLIP模型。具体而言，我们展示了仅需1000万张图像即可构建面向生物体的专家基础模型。此外，我们提出了EntityNet数据集，该数据集包含3300万张图像与4600万条文本描述配对，使得训练通用CLIP模型所需时间得以显著缩短。"
    },
    {
        "title": "Predicting Movie Hits Before They Happen with LLMs",
        "url": "http://arxiv.org/abs/2505.02693v1",
        "pub_date": "2025-05-05",
        "summary": "Addressing the cold-start issue in content recommendation remains a critical ongoing challenge. In this work, we focus on tackling the cold-start problem for movies on a large entertainment platform. Our primary goal is to forecast the popularity of cold-start movies using Large Language Models (LLMs) leveraging movie metadata. This method could be integrated into retrieval systems within the personalization pipeline or could be adopted as a tool for editorial teams to ensure fair promotion of potentially overlooked movies that may be missed by traditional or algorithmic solutions. Our study validates the effectiveness of this approach compared to established baselines and those we developed.",
        "translated": "解决内容推荐中的冷启动问题仍是一项持续性的关键挑战。本研究聚焦于应对大型娱乐平台上电影内容的冷启动难题。我们的核心目标是通过利用大语言模型（Large Language Models, LLMs）结合电影元数据，对冷启动电影的流行度进行预测。该方法可集成至个性化推荐流程中的检索系统，也可作为编辑团队的工具，确保对那些可能被传统方法或算法解决方案遗漏但具有潜力的电影进行公平推广。通过与传统基准模型及我们自行开发的模型对比，本研究验证了该方法的有效性。"
    }
]