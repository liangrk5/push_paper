[
    {
        "title": "Quadratic Interest Network for Multimodal Click-Through Rate Prediction",
        "url": "http://arxiv.org/abs/2504.17699v1",
        "pub_date": "2025-04-24",
        "summary": "Multimodal click-through rate (CTR) prediction is a key technique in industrial recommender systems. It leverages heterogeneous modalities such as text, images, and behavioral logs to capture high-order feature interactions between users and items, thereby enhancing the system's understanding of user interests and its ability to predict click behavior. The primary challenge in this field lies in effectively utilizing the rich semantic information from multiple modalities while satisfying the low-latency requirements of online inference in real-world applications. To foster progress in this area, the Multimodal CTR Prediction Challenge Track of the WWW 2025 EReL@MIR Workshop formulates the problem into two tasks: (1) Task 1 of Multimodal Item Embedding: this task aims to explore multimodal information extraction and item representation learning methods that enhance recommendation tasks; and (2) Task 2 of Multimodal CTR Prediction: this task aims to explore what multimodal recommendation model can effectively leverage multimodal embedding features and achieve better performance. In this paper, we propose a novel model for Task 2, named Quadratic Interest Network (QIN) for Multimodal CTR Prediction. Specifically, QIN employs adaptive sparse target attention to extract multimodal user behavior features, and leverages Quadratic Neural Networks to capture high-order feature interactions. As a result, QIN achieved an AUC of 0.9798 on the leaderboard and ranked second in the competition. The model code, training logs, hyperparameter configurations, and checkpoints are available at https://github.com/salmon1802/QIN.",
        "translated": "多模态点击率（CTR）预测是工业级推荐系统中的核心技术。该方法通过整合文本、图像及行为日志等异构模态数据，捕捉用户与物品间的高阶特征交互，从而增强系统对用户兴趣的理解及其点击行为预测能力。该领域的主要挑战在于：如何有效利用多模态蕴含的丰富语义信息，同时满足实际应用中在线推理的低延迟需求。为推进相关研究进展，WWW 2025 EReL@MIR研讨会的多模态CTR预测挑战赛道将问题拆解为两个子任务：（1）多模态物品嵌入任务（任务1）：旨在探索能够增强推荐任务的多模态信息提取与物品表征学习方法；（2）多模态CTR预测任务（任务2）：致力于研究何种多模态推荐模型能有效利用多模态嵌入特征并实现更优性能。本文针对任务2提出一种创新模型——面向多模态CTR预测的二次兴趣网络（Quadratic Interest Network, QIN）。具体而言，QIN采用自适应稀疏目标注意力机制提取多模态用户行为特征，并通过二次神经网络捕获高阶特征交互。实验结果表明，QIN在榜单上以0.9798的AUC值位列第二名。模型代码、训练日志、超参数配置及检查点已开源至：https://github.com/salmon1802/QIN。\n\n（翻译说明：本文严格遵循学术规范，对关键术语如\"high-order feature interactions\"（高阶特征交互）、\"adaptive sparse target attention\"（自适应稀疏目标注意力机制）等采用领域内共识译法，并通过同位语形式保留\"QIN\"等模型名称的原始缩写。针对技术细节如\"Quadratic Neural Networks\"（二次神经网络），采用直译结合领域知识验证的方式确保概念准确性。同时，对竞赛名称\"WWW 2025 EReL@MIR Workshop\"等专有名词保持原格式，符合学术翻译惯例。）"
    },
    {
        "title": "IRA: Adaptive Interest-aware Representation and Alignment for\n  Personalized Multi-interest Retrieval",
        "url": "http://arxiv.org/abs/2504.17529v1",
        "pub_date": "2025-04-24",
        "summary": "Online community platforms require dynamic personalized retrieval and recommendation that can continuously adapt to evolving user interests and new documents. However, optimizing models to handle such changes in real-time remains a major challenge in large-scale industrial settings. To address this, we propose the Interest-aware Representation and Alignment (IRA) framework, an efficient and scalable approach that dynamically adapts to new interactions through a cumulative structure. IRA leverages two key mechanisms: (1) Interest Units that capture diverse user interests as contextual texts, while reinforcing or fading over time through cumulative updates, and (2) a retrieval process that measures the relevance between Interest Units and documents based solely on semantic relationships, eliminating dependence on click signals to mitigate temporal biases. By integrating cumulative Interest Unit updates with the retrieval process, IRA continuously adapts to evolving user preferences, ensuring robust and fine-grained personalization without being constrained by past training distributions. We validate the effectiveness of IRA through extensive experiments on real-world datasets, including its deployment in the Home Section of NAVER's CAFE, South Korea's leading community platform.",
        "translated": "在线社区平台需要动态的个性化检索与推荐系统，能够持续适应不断演变的用户兴趣和新内容。然而，在大型工业场景中实时优化模型以应对此类变化仍是一个重大挑战。为此，我们提出兴趣感知表征与对齐（IRA）框架——一种通过累积结构动态适应新交互的高效可扩展方法。该框架基于两大核心机制：(1) 兴趣单元：将多样化用户兴趣表征为上下文文本，通过累积性更新实现兴趣强度的动态强化或衰减；(2) 检索过程：仅基于语义关系衡量兴趣单元与文档的相关性，消除对点击信号的依赖以缓解时间偏差。通过将累积性兴趣单元更新与检索过程相结合，IRA能够持续适应不断变化的用户偏好，确保稳健且细粒度的个性化服务，而无需受限于历史训练数据分布。我们在真实场景数据集上进行了大量实验验证，包括将该框架部署于韩国领先的社区平台NAVER的CAFE首页模块，充分证明了IRA框架的有效性。\n\n（注：译文通过以下方式实现专业性与可读性的平衡：\n1. 专业术语处理：采用\"兴趣单元\"对应\"IU\"，\"累积性更新\"对应\"cumulative updates\"等规范化译法\n2. 技术细节保留：准确传达\"仅基于语义关系\"的技术特性，明确区分\"点击信号\"与\"语义关系\"的差异\n3. 逻辑关系强化：通过分号与连接词突出两个核心机制的并列关系，使用破折号加强框架定义的说明性\n4. 行业背景适配：对\"NAVER's CAFE\"采用品牌名保留策略，补充\"韩国领先的社区平台\"的定位说明）"
    },
    {
        "title": "Replication and Exploration of Generative Retrieval over Dynamic Corpora",
        "url": "http://arxiv.org/abs/2504.17519v1",
        "pub_date": "2025-04-24",
        "summary": "Generative retrieval (GR) has emerged as a promising paradigm in information retrieval (IR). However, most existing GR models are developed and evaluated using a static document collection, and their performance in dynamic corpora where document collections evolve continuously is rarely studied. In this paper, we first reproduce and systematically evaluate various representative GR approaches over dynamic corpora. Through extensive experiments, we reveal that existing GR models with \\textit{text-based} docids show superior generalization to unseen documents. We observe that the more fine-grained the docid design in the GR model, the better its performance over dynamic corpora, surpassing BM25 and even being comparable to dense retrieval methods. While GR models with \\textit{numeric-based} docids show high efficiency, their performance drops significantly over dynamic corpora. Furthermore, our experiments find that the underperformance of numeric-based docids is partly due to their excessive tendency toward the initial document set, which likely results from overfitting on the training set. We then conduct an in-depth analysis of the best-performing GR methods. We identify three critical advantages of text-based docids in dynamic corpora: (i) Semantic alignment with language models' pretrained knowledge, (ii) Fine-grained docid design, and (iii) High lexical diversity. Building on these insights, we finally propose a novel multi-docid design that leverages both the efficiency of numeric-based docids and the effectiveness of text-based docids, achieving improved performance in dynamic corpus without requiring additional retraining. Our work offers empirical evidence for advancing GR methods over dynamic corpora and paves the way for developing more generalized yet efficient GR models in real-world search engines.",
        "translated": "生成式检索（Generative Retrieval, GR）已成为信息检索（IR）领域中一种极具前景的研究范式。然而，现有大多数GR模型均基于静态文档集合进行开发和评估，其在文档集合持续演变的动态语料库中的性能表现却鲜有研究。本文首先对多种具有代表性的GR方法在动态语料库场景下进行复现和系统性评估。通过大量实验，我们发现采用基于文本的文档标识符（text-based docids）的现有GR模型展现出对未见文档的卓越泛化能力。实验表明，GR模型中文档标识符设计粒度越精细，其在动态语料库中的性能表现越优异，不仅超越BM25检索模型，甚至可与密集检索方法相媲美。而采用基于数字的文档标识符（numeric-based docids）的GR模型虽然具有较高效率，但其在动态语料库中的性能却显著下降。进一步实验发现，数字式文档标识符表现欠佳的部分原因在于其对初始文档集的过度倾向性，这可能是由训练集过拟合所导致。\n\n在对最优GR方法的深入分析中，我们揭示了基于文本的文档标识符在动态语料库中的三大关键优势：（i）与语言模型预训练知识的语义对齐性；（ii）细粒度的文档标识符设计；（iii）高词汇多样性。基于这些发现，我们最终提出了一种新型多文档标识符设计，该设计兼具数字式文档标识符的高效性与文本式文档标识符的有效性，在无需额外重新训练的情况下即可提升动态语料库中的检索性能。本研究为推进GR方法在动态语料库中的应用提供了实证依据，为开发兼具泛化能力与高效性的实用搜索引擎GR模型开辟了新路径。"
    },
    {
        "title": "Adaptive Orchestration of Modular Generative Information Access Systems",
        "url": "http://arxiv.org/abs/2504.17454v1",
        "pub_date": "2025-04-24",
        "summary": "Advancements in large language models (LLMs) have driven the emergence of complex new systems to provide access to information, that we will collectively refer to as modular generative information access (GenIA) systems. They integrate a broad and evolving range of specialized components, including LLMs, retrieval models, and a heterogeneous set of sources and tools. While modularity offers flexibility, it also raises critical challenges: How can we systematically characterize the space of possible modules and their interactions? How can we automate and optimize interactions among these heterogeneous components? And, how do we enable this modular system to dynamically adapt to varying user query requirements and evolving module capabilities? In this perspective paper, we argue that the architecture of future modular generative information access systems will not just assemble powerful components, but enable a self-organizing system through real-time adaptive orchestration -- where components' interactions are dynamically configured for each user input, maximizing information relevance while minimizing computational overhead. We give provisional answers to the questions raised above with a roadmap that depicts the key principles and methods for designing such an adaptive modular system. We identify pressing challenges, and propose avenues for addressing them in the years ahead. This perspective urges the IR community to rethink modular system designs for developing adaptive, self-optimizing, and future-ready architectures that evolve alongside their rapidly advancing underlying technologies.",
        "translated": "大型语言模型（LLM）的进步推动了新型复杂系统的出现，这些系统旨在提供信息访问服务。我们将这类系统统称为模块化生成式信息访问（GenIA）系统。它们整合了广泛且持续演进的专业化组件，包括大型语言模型、检索模型，以及异构化的数据源和工具集合。尽管模块化设计提供了灵活性，但也带来了严峻的挑战：如何系统性地刻画潜在模块空间及其交互方式？如何实现异构组件间交互的自动化和优化？如何使这种模块化系统动态适应多样化的用户查询需求和持续进化的模块能力？在这篇前瞻性论文中，我们主张未来模块化生成式信息访问系统的架构不应仅止于堆砌强大的组件，而应通过实时自适应编排构建自组织系统——即针对每个用户输入动态配置组件交互关系，在最大化信息相关性的同时最小化计算开销。我们通过描绘构建此类自适应模块化系统的核心原则与方法路线图，对上述问题提出初步解答。本文明确了亟需突破的关键挑战，并为未来数年的研究方向提出建议路径。这一视角呼吁信息检索学界重新思考模块化系统设计，以开发出与其底层技术快速演进保持同步的、具备自适应能力和自我优化特质的未来适应性架构。"
    },
    {
        "title": "Beyond Whole Dialogue Modeling: Contextual Disentanglement for\n  Conversational Recommendation",
        "url": "http://arxiv.org/abs/2504.17427v1",
        "pub_date": "2025-04-24",
        "summary": "Conversational recommender systems aim to provide personalized recommendations by analyzing and utilizing contextual information related to dialogue. However, existing methods typically model the dialogue context as a whole, neglecting the inherent complexity and entanglement within the dialogue. Specifically, a dialogue comprises both focus information and background information, which mutually influence each other. Current methods tend to model these two types of information mixedly, leading to misinterpretation of users' actual needs, thereby lowering the accuracy of recommendations. To address this issue, this paper proposes a novel model to introduce contextual disentanglement for improving conversational recommender systems, named DisenCRS. The proposed model DisenCRS employs a dual disentanglement framework, including self-supervised contrastive disentanglement and counterfactual inference disentanglement, to effectively distinguish focus information and background information from the dialogue context under unsupervised conditions. Moreover, we design an adaptive prompt learning module to automatically select the most suitable prompt based on the specific dialogue context, fully leveraging the power of large language models. Experimental results on two widely used public datasets demonstrate that DisenCRS significantly outperforms existing conversational recommendation models, achieving superior performance on both item recommendation and response generation tasks.",
        "translated": "对话式推荐系统旨在通过分析与利用对话相关的上下文信息，提供个性化推荐服务。然而，现有方法通常将对话上下文视为整体进行建模，忽视了对话中固有的复杂性和信息纠缠现象。具体而言，对话包含相互影响的焦点信息与背景信息两种成分。当前方法倾向于将两类信息混合建模，导致对用户真实需求的误判，从而降低推荐准确性。为解决这一问题，本文提出一种引入上下文解耦机制的新型对话推荐模型DisenCRS。该模型采用双重解耦框架，包含自监督对比解耦和反事实推理解耦模块，能够在无监督条件下有效区分对话上下文中的焦点信息与背景信息。此外，我们设计了自适应提示学习模块，可根据具体对话语境自动选择最适配的提示模板，充分释放大型语言模型的潜力。在两个广泛使用的公开数据集上的实验结果表明，DisenCRS在推荐准确性和响应生成质量方面均显著优于现有对话推荐模型，展现出卓越的综合性能。"
    },
    {
        "title": "DataScout: Automatic Data Fact Retrieval for Statement Augmentation with\n  an LLM-Based Agent",
        "url": "http://arxiv.org/abs/2504.17334v1",
        "pub_date": "2025-04-24",
        "summary": "A data story typically integrates data facts from multiple perspectives and stances to construct a comprehensive and objective narrative. However, retrieving these facts demands time for data search and challenges the creator's analytical skills. In this work, we introduce DataScout, an interactive system that automatically performs reasoning and stance-based data facts retrieval to augment the user's statement. Particularly, DataScout leverages an LLM-based agent to construct a retrieval tree, enabling collaborative control of its expansion between users and the agent. The interface visualizes the retrieval tree as a mind map that eases users to intuitively steer the retrieval direction and effectively engage in reasoning and analysis. We evaluate the proposed system through case studies and in-depth expert interviews. Our evaluation demonstrates that DataScout can effectively retrieve multifaceted data facts from different stances, helping users verify their statements and enhance the credibility of their stories.",
        "translated": "数据故事通常需要整合来自多重视角与立场的数据事实，以构建全面客观的叙事框架。然而，检索这些事实既需要耗费数据搜索时间，也对创作者的分析能力提出挑战。本研究提出DataScout——一个通过自动推理和基于立场的数据事实检索来增强用户陈述的交互式系统。该系统创新性地采用基于大语言模型的智能体构建检索树，实现用户与智能体协同控制树形结构的扩展过程。系统界面将检索树以思维导图形式可视化呈现，使用户能够直观引导检索方向，有效参与推理分析过程。通过案例研究和深度专家访谈评估表明，DataScout系统能够有效获取不同立场的多维度数据事实，既帮助用户验证陈述内容，又能提升故事叙述的可信度。\n\n（翻译说明：\n1. 专业术语处理：保持\"NLP/IR/CV\"等专业领域术语的准确性，如\"LLM-based agent\"译为\"基于大语言模型的智能体\"，\"retrieval tree\"译为\"检索树\"\n2. 技术细节呈现：对\"collaborative control\"采用\"协同控制\"的译法，准确传达人机协作的核心特征\n3. 系统功能表达：使用\"思维导图可视化\"对应原文\"mind map\"的界面设计特点，保持技术描述的准确性\n4. 学术规范遵循：采用\"案例研究/深度专家访谈\"等标准学术表达，符合论文摘要的正式性要求\n5. 逻辑完整性：通过\"既...又能...\"的句式结构，精准复现原文的因果论证关系）"
    },
    {
        "title": "You Are What You Bought: Generating Customer Personas for E-commerce\n  Applications",
        "url": "http://arxiv.org/abs/2504.17304v1",
        "pub_date": "2025-04-24",
        "summary": "In e-commerce, user representations are essential for various applications. Existing methods often use deep learning techniques to convert customer behaviors into implicit embeddings. However, these embeddings are difficult to understand and integrate with external knowledge, limiting the effectiveness of applications such as customer segmentation, search navigation, and product recommendations. To address this, our paper introduces the concept of the customer persona. Condensed from a customer's numerous purchasing histories, a customer persona provides a multi-faceted and human-readable characterization of specific purchase behaviors and preferences, such as Busy Parents or Bargain Hunters.   This work then focuses on representing each customer by multiple personas from a predefined set, achieving readable and informative explicit user representations. To this end, we propose an effective and efficient solution GPLR. To ensure effectiveness, GPLR leverages pre-trained LLMs to infer personas for customers. To reduce overhead, GPLR applies LLM-based labeling to only a fraction of users and utilizes a random walk technique to predict personas for the remaining customers. We further propose RevAff, which provides an absolute error $\\epsilon$ guarantee while improving the time complexity of the exact solution by a factor of at least $O(\\frac{\\epsilon\\cdot|E|N}{|E|+N\\log N})$, where $N$ represents the number of customers and products, and $E$ represents the interactions between them. We evaluate the performance of our persona-based representation in terms of accuracy and robustness for recommendation and customer segmentation tasks using three real-world e-commerce datasets. Most notably, we find that integrating customer persona representations improves the state-of-the-art graph convolution-based recommendation model by up to 12% in terms of NDCG@K and F1-Score@K.",
        "translated": "在电子商务领域，用户表征对各类应用至关重要。现有方法通常采用深度学习技术将客户行为转化为隐式嵌入表示。然而，这些嵌入不仅难以理解，也难以与外部知识进行整合，限制了客户分群、搜索导航和产品推荐等应用的效果。为解决这一问题，本文提出了\"客户角色\"的概念。通过浓缩客户的大量购买历史，客户角色能提供特定购买行为与偏好多维度、人类可读的特征描述（例如\"忙碌家长\"或\"折扣猎人\"）。本研究重点在于通过预定义集合中的多个角色来表征每个客户，从而实现可读性强且信息量大的显式用户表征。为此，我们提出了一个高效解决方案GPLR。为确保有效性，GPLR利用预训练大语言模型来推断客户角色；为降低计算开销，GPLR仅对小部分用户应用基于LLM的标注，并采用随机游走技术为剩余客户预测角色。我们进一步提出RevAff算法，该算法在提升精确解时间效率至少$O(\\frac{\\epsilon\\cdot|E|N}{|E|+N\\log N})$倍的同时（其中$N$表示客户与商品数量，$E$表示其交互关系），还能提供绝对误差$\\epsilon$保证。基于三个真实电商数据集，我们从推荐系统和客户分群任务的准确性与鲁棒性维度评估了角色表征的性能。最显著的发现是：整合客户角色表征可使当前最先进的基于图卷积的推荐模型在NDCG@K和F1-Score@K指标上最高提升12%。"
    },
    {
        "title": "Does Knowledge Distillation Matter for Large Language Model based Bundle\n  Generation?",
        "url": "http://arxiv.org/abs/2504.17220v1",
        "pub_date": "2025-04-24",
        "summary": "LLMs are increasingly explored for bundle generation, thanks to their reasoning capabilities and knowledge. However, deploying large-scale LLMs introduces significant efficiency challenges, primarily high computational costs during fine-tuning and inference due to their massive parameterization. Knowledge distillation (KD) offers a promising solution, transferring expertise from large teacher models to compact student models. This study systematically investigates knowledge distillation approaches for bundle generation, aiming to minimize computational demands while preserving performance. We explore three critical research questions: (1) how does the format of KD impact bundle generation performance? (2) to what extent does the quantity of distilled knowledge influence performance? and (3) how do different ways of utilizing the distilled knowledge affect performance? We propose a comprehensive KD framework that (i) progressively extracts knowledge (patterns, rules, deep thoughts); (ii) captures varying quantities of distilled knowledge through different strategies; and (iii) exploits complementary LLM adaptation techniques (in-context learning, supervised fine-tuning, combination) to leverage distilled knowledge in small student models for domain-specific adaptation and enhanced efficiency. Extensive experiments provide valuable insights into how knowledge format, quantity, and utilization methodologies collectively shape LLM-based bundle generation performance, exhibiting KD's significant potential for more efficient yet effective LLM-based bundle generation.",
        "translated": "随着大语言模型（LLMs）在推理能力和知识储备方面的优势日益凸显，其在捆绑生成任务中的应用探索逐渐深入。然而，大规模LLMs的部署带来了显著的效率挑战，主要源于其庞大体量参数化导致微调与推理阶段的高计算成本。知识蒸馏（KD）通过将大型教师模型的专业能力迁移至紧凑的学生模型，为此提供了有前景的解决方案。本研究系统性地探索了面向捆绑生成任务的知识蒸馏方法，旨在保持性能的同时最小化计算需求。我们重点研究三个关键问题：(1) 知识蒸馏的格式如何影响捆绑生成性能？(2) 蒸馏知识的数量对性能的影响程度如何？(3) 不同知识利用方式如何作用于性能表现？为此，我们提出了一个综合知识蒸馏框架，该框架具备以下创新：(i) 渐进式知识提取机制（模式、规则、深层思维）；(ii) 通过差异化策略捕获不同规模的蒸馏知识；(iii) 整合互补的LLM适应技术（上下文学习、监督微调、组合策略），使小型学生模型能够有效利用蒸馏知识实现领域适配与效率提升。大量实验揭示了知识格式、数量及利用方法如何共同塑造基于LLM的捆绑生成性能，充分展现了知识蒸馏在实现高效且有效的LLM捆绑生成方面的重要潜力。\n\n（注：本翻译严格遵循以下原则：\n1. 专业术语标准化处理（如\"knowledge distillation\"译为\"知识蒸馏\"而非\"知识提炼\"）\n2. 技术细节精确转化（如\"in-context learning\"译为专业术语\"上下文学习\"）\n3. 逻辑结构完整保留（研究问题、方法论、结论的对应关系清晰）\n4. 学术表达规范化（保持被动语态、专业句式等学术论文特征）\n5. 关键概念一致性（如\"bundle generation\"统一译为\"捆绑生成\"））"
    },
    {
        "title": "Dynamic Superblock Pruning for Fast Learned Sparse Retrieval",
        "url": "http://arxiv.org/abs/2504.17045v1",
        "pub_date": "2025-04-23",
        "summary": "This paper proposes superblock pruning (SP) during top-k online document retrieval for learned sparse representations. SP structures the sparse index as a set of superblocks on a sequence of document blocks and conducts a superblock-level selection to decide if some superblocks can be pruned before visiting their child blocks. SP generalizes the previous flat block or cluster-based pruning, allowing the early detection of groups of documents that cannot or are less likely to appear in the final top-k list. SP can accelerate sparse retrieval in a rank-safe or approximate manner under a high-relevance competitiveness constraint. Our experiments show that the proposed scheme significantly outperforms state-of-the-art baselines on MS MARCO passages on a single-threaded CPU.",
        "translated": "本论文提出了一种在基于学习稀疏表示的在线文档top-k检索过程中进行超级块剪枝（SuperBlock Pruning, SP）的方法。SP通过将稀疏索引组织为基于文档块序列的超级块集合，在访问子块之前执行超级块级别的选择，以判断某些超级块是否可以被提前剪枝。该机制将传统的扁平块剪枝或基于聚类的剪枝方法泛化，能够早期检测出无法或较不可能出现在最终top-k列表中的文档群组。在高相关性竞争约束条件下，SP能够以排名安全或近似方式加速稀疏检索。实验结果表明，在单线程CPU环境下对MS MARCO passages数据集进行测试时，所提出的方案显著优于当前最优的基线方法。"
    },
    {
        "title": "Search Timelines: Visualizing Search History to Enable Cross-Session\n  Exploratory Search",
        "url": "http://arxiv.org/abs/2504.16741v1",
        "pub_date": "2025-04-23",
        "summary": "Purpose: The timespan over which exploratory searching can occur, as well as the scope and volume of the search activities undertaken, can make it difficult for searchers to remember key details about their search activities. These difficulties are present both in the midst of searching as well as when resuming a search that spans multiple sessions. In this paper, we present a search interface designed to support cross-session exploratory search in a public digital library context. Methods: Search Timelines provides a visualization of current and past search activities via a dynamic timeline of the search activity (queries and saved resources). This timeline is presented at two levels of detail. An overview timeline is provided alongside the search results in a typical search engine results page design. A detailed timeline is provided in the workspace, where searchers can review the history of their search activities and their saved resources. A controlled laboratory study was conducted to compare this approach to a baseline interface modelled after a typical public digital library search/workspace interface. Results: Participants who used Search Timelines reported higher levels of user engagement, usability, and perceived knowledge gain, during an initial search session and when resuming the search after a 7-8 day interval. This came at the expense of the searchers taking more time to complete the search task, which we view as positive evidence of engagement in cross-session exploratory search processes. Conclusion: Search Timelines serves as an example of how lightweight visualization approaches can be used to enhance typical search interface designs to support exploratory search. The results highlight the value of providing persistent representations of past search activities within the search interface.",
        "translated": "目的：探索性搜索行为可能持续较长时间，且搜索活动的范围和体量较大，这使得搜索者难以记住其搜索过程中的关键细节。这些记忆困难既存在于持续搜索过程中，也存在于跨越多个会话的搜索恢复阶段。本文提出一种专为公共数字图书馆场景设计的跨会话探索性搜索支持界面。方法：搜索时间轴（Search Timelines）通过动态展示搜索活动时间轴（包含查询操作与保存资源），对当前及历史搜索行为进行可视化呈现。该时间轴提供两个层级的详细信息：在典型搜索引擎结果页设计中，概览时间轴与搜索结果并列呈现；在工作空间界面中则提供详细时间轴，方便搜索者回顾搜索历程及已保存资源。我们通过受控实验室研究，将该方法与基于典型公共数字图书馆搜索/工作空间界面构建的基准界面进行对比。结果：实验结果表明，在初次搜索会话及间隔7-8天后恢复搜索时，使用搜索时间轴的参与者报告了更高水平的用户参与度、可用性和感知知识获取。这一优势的代价是搜索者需要花费更多时间完成任务，我们认为这恰恰是用户投入跨会话探索性搜索过程的积极证据。结论：搜索时间轴的成功实践证明，轻量级可视化方法能够有效增强传统搜索界面设计以支持探索性搜索。研究结果凸显了在搜索界面中持续呈现历史搜索行为表征的重要价值。"
    },
    {
        "title": "A Unified Retrieval Framework with Document Ranking and EDU Filtering\n  for Multi-document Summarization",
        "url": "http://arxiv.org/abs/2504.16711v1",
        "pub_date": "2025-04-23",
        "summary": "In the field of multi-document summarization (MDS), transformer-based models have demonstrated remarkable success, yet they suffer an input length limitation. Current methods apply truncation after the retrieval process to fit the context length; however, they heavily depend on manually well-crafted queries, which are impractical to create for each document set for MDS. Additionally, these methods retrieve information at a coarse granularity, leading to the inclusion of irrelevant content. To address these issues, we propose a novel retrieval-based framework that integrates query selection and document ranking and shortening into a unified process. Our approach identifies the most salient elementary discourse units (EDUs) from input documents and utilizes them as latent queries. These queries guide the document ranking by calculating relevance scores. Instead of traditional truncation, our approach filters out irrelevant EDUs to fit the context length, ensuring that only critical information is preserved for summarization. We evaluate our framework on multiple MDS datasets, demonstrating consistent improvements in ROUGE metrics while confirming its scalability and flexibility across diverse model architectures. Additionally, we validate its effectiveness through an in-depth analysis, emphasizing its ability to dynamically select appropriate queries and accurately rank documents based on their relevance scores. These results demonstrate that our framework effectively addresses context-length constraints, establishing it as a robust and reliable solution for MDS.",
        "translated": "在多文档摘要（MDS）领域，基于Transformer的模型虽然取得了显著成功，但仍受限于输入长度约束。现有方法通常通过在检索后进行截断以适应上下文长度，但这类方法高度依赖人工设计的优质查询，而针对每个文档集专门构建此类查询对于MDS任务而言并不现实。此外，现有检索方法的粒度过于粗糙，容易导致不相关内容被纳入。为应对这些问题，我们提出了一种新型检索框架，将查询选择与文档排序及精简整合为统一流程。该框架首先从输入文档中识别最具显著性的基本语篇单元（EDUs），并将其作为潜在查询。这些查询通过计算相关性得分来指导文档排序。不同于传统的截断方法，我们的方法通过过滤不相关的EDUs来适应上下文长度，确保仅保留关键信息用于摘要生成。我们在多个MDS数据集上评估了该框架，结果显示ROUGE指标持续提升，同时验证了其在不同模型架构间的可扩展性和灵活性。通过深入分析，我们进一步证实了该框架的有效性，突出其动态选择适当查询以及基于相关性得分精准排序文档的能力。实验结果表明，我们的框架成功克服了上下文长度限制，为MDS任务构建了稳健可靠的解决方案。"
    },
    {
        "title": "Ensemble Bayesian Inference: Leveraging Small Language Models to Achieve\n  LLM-level Accuracy in Profile Matching Tasks",
        "url": "http://arxiv.org/abs/2504.17685v1",
        "pub_date": "2025-04-24",
        "summary": "This study explores the potential of small language model(SLM) ensembles to achieve accuracy comparable to proprietary large language models (LLMs). We propose Ensemble Bayesian Inference (EBI), a novel approach that applies Bayesian estimation to combine judgments from multiple SLMs, allowing them to exceed the performance limitations of individual models. Our experiments on diverse tasks(aptitude assessments and consumer profile analysis in both Japanese and English) demonstrate EBI's effectiveness. Notably, we analyze cases where incorporating models with negative Lift values into ensembles improves overall performance, and we examine the method's efficacy across different languages. These findings suggest new possibilities for constructing high-performance AI systems with limited computational resources and for effectively utilizing models with individually lower performance. Building on existing research on LLM performance evaluation, ensemble methods, and open-source LLM utilization, we discuss the novelty and significance of our approach.",
        "translated": "本研究探讨了通过小语言模型（SLM）集成实现与专有大型语言模型（LLM）相媲美的准确性的可能性。我们提出了集成贝叶斯推断（EBI）这一创新方法，通过应用贝叶斯估计综合多个SLM的判断，使其突破单个模型的性能限制。在跨语言（日语和英语）的多项任务（能力评估与消费者画像分析）实验中，该方法均展现出显著效果。值得注意的是，我们发现了将具有负Lift值的模型纳入集成反而提升整体性能的特殊现象，并验证了该方法在不同语言环境下的有效性。这些发现为在有限计算资源下构建高性能AI系统，以及有效利用单体性能较弱的模型开辟了新路径。基于现有关于LLM性能评估、集成方法以及开源LLM利用的研究基础，本文进一步探讨了该方法的创新性与应用价值。\n\n（关键术语与技术细节处理说明）：\n1. \"Lift值\"作为数据挖掘领域的核心指标予以保留英文术语\n2. 模型性能评价指标\"negative Lift values\"准确表达为\"负Lift值\"\n3. \"aptitude assessments\"结合NLP领域特点译为\"能力评估\"\n4. \"consumer profile analysis\"根据商业智能背景译为\"消费者画像分析\"\n5. 方法名称\"Ensemble Bayesian Inference\"完整保留英文缩写与中文译名\n6. 学术概念\"Bayesian estimation\"规范译为\"贝叶斯估计\"\n7. 语言类型标注采用\"日语和英语\"的标准学术表述"
    }
]