[
    {
        "title": "Quadratic Interest Network for Multimodal Click-Through Rate Prediction",
        "url": "http://arxiv.org/abs/2504.17699v1",
        "pub_date": "2025-04-24",
        "summary": "Multimodal click-through rate (CTR) prediction is a key technique in industrial recommender systems. It leverages heterogeneous modalities such as text, images, and behavioral logs to capture high-order feature interactions between users and items, thereby enhancing the system's understanding of user interests and its ability to predict click behavior. The primary challenge in this field lies in effectively utilizing the rich semantic information from multiple modalities while satisfying the low-latency requirements of online inference in real-world applications. To foster progress in this area, the Multimodal CTR Prediction Challenge Track of the WWW 2025 EReL@MIR Workshop formulates the problem into two tasks: (1) Task 1 of Multimodal Item Embedding: this task aims to explore multimodal information extraction and item representation learning methods that enhance recommendation tasks; and (2) Task 2 of Multimodal CTR Prediction: this task aims to explore what multimodal recommendation model can effectively leverage multimodal embedding features and achieve better performance. In this paper, we propose a novel model for Task 2, named Quadratic Interest Network (QIN) for Multimodal CTR Prediction. Specifically, QIN employs adaptive sparse target attention to extract multimodal user behavior features, and leverages Quadratic Neural Networks to capture high-order feature interactions. As a result, QIN achieved an AUC of 0.9798 on the leaderboard and ranked second in the competition. The model code, training logs, hyperparameter configurations, and checkpoints are available at https://github.com/salmon1802/QIN.",
        "translated": "多模态点击率（CTR）预测是工业级推荐系统中的核心技术。该方法通过整合文本、图像及行为日志等异构模态数据，捕捉用户与物品间的高阶特征交互，从而增强系统对用户兴趣的理解及其点击行为预测能力。该领域的主要挑战在于：如何有效利用多模态蕴含的丰富语义信息，同时满足实际应用中在线推理的低延迟需求。为推进相关研究进展，WWW 2025 EReL@MIR研讨会的多模态CTR预测挑战赛道将问题拆解为两个子任务：（1）多模态物品嵌入任务（任务1）：旨在探索能够增强推荐任务的多模态信息提取与物品表征学习方法；（2）多模态CTR预测任务（任务2）：致力于研究何种多模态推荐模型能有效利用多模态嵌入特征并实现更优性能。本文针对任务2提出一种创新模型——面向多模态CTR预测的二次兴趣网络（Quadratic Interest Network, QIN）。具体而言，QIN采用自适应稀疏目标注意力机制提取多模态用户行为特征，并通过二次神经网络捕获高阶特征交互。实验结果表明，QIN在榜单上以0.9798的AUC值位列第二名。模型代码、训练日志、超参数配置及检查点已开源至：https://github.com/salmon1802/QIN。\n\n（翻译说明：本文严格遵循学术规范，对关键术语如\"high-order feature interactions\"（高阶特征交互）、\"adaptive sparse target attention\"（自适应稀疏目标注意力机制）等采用领域内共识译法，并通过同位语形式保留\"QIN\"等模型名称的原始缩写。针对技术细节如\"Quadratic Neural Networks\"（二次神经网络），采用直译结合领域知识验证的方式确保概念准确性。同时，对竞赛名称\"WWW 2025 EReL@MIR Workshop\"等专有名词保持原格式，符合学术翻译惯例。）"
    },
    {
        "title": "IRA: Adaptive Interest-aware Representation and Alignment for\n  Personalized Multi-interest Retrieval",
        "url": "http://arxiv.org/abs/2504.17529v1",
        "pub_date": "2025-04-24",
        "summary": "Online community platforms require dynamic personalized retrieval and recommendation that can continuously adapt to evolving user interests and new documents. However, optimizing models to handle such changes in real-time remains a major challenge in large-scale industrial settings. To address this, we propose the Interest-aware Representation and Alignment (IRA) framework, an efficient and scalable approach that dynamically adapts to new interactions through a cumulative structure. IRA leverages two key mechanisms: (1) Interest Units that capture diverse user interests as contextual texts, while reinforcing or fading over time through cumulative updates, and (2) a retrieval process that measures the relevance between Interest Units and documents based solely on semantic relationships, eliminating dependence on click signals to mitigate temporal biases. By integrating cumulative Interest Unit updates with the retrieval process, IRA continuously adapts to evolving user preferences, ensuring robust and fine-grained personalization without being constrained by past training distributions. We validate the effectiveness of IRA through extensive experiments on real-world datasets, including its deployment in the Home Section of NAVER's CAFE, South Korea's leading community platform.",
        "translated": "在线社区平台需要动态的个性化检索与推荐系统，能够持续适应不断演变的用户兴趣和新内容。然而，在大型工业场景中实时优化模型以应对此类变化仍是一个重大挑战。为此，我们提出兴趣感知表征与对齐（IRA）框架——一种通过累积结构动态适应新交互的高效可扩展方法。该框架基于两大核心机制：(1) 兴趣单元：将多样化用户兴趣表征为上下文文本，通过累积性更新实现兴趣强度的动态强化或衰减；(2) 检索过程：仅基于语义关系衡量兴趣单元与文档的相关性，消除对点击信号的依赖以缓解时间偏差。通过将累积性兴趣单元更新与检索过程相结合，IRA能够持续适应不断变化的用户偏好，确保稳健且细粒度的个性化服务，而无需受限于历史训练数据分布。我们在真实场景数据集上进行了大量实验验证，包括将该框架部署于韩国领先的社区平台NAVER的CAFE首页模块，充分证明了IRA框架的有效性。\n\n（注：译文通过以下方式实现专业性与可读性的平衡：\n1. 专业术语处理：采用\"兴趣单元\"对应\"IU\"，\"累积性更新\"对应\"cumulative updates\"等规范化译法\n2. 技术细节保留：准确传达\"仅基于语义关系\"的技术特性，明确区分\"点击信号\"与\"语义关系\"的差异\n3. 逻辑关系强化：通过分号与连接词突出两个核心机制的并列关系，使用破折号加强框架定义的说明性\n4. 行业背景适配：对\"NAVER's CAFE\"采用品牌名保留策略，补充\"韩国领先的社区平台\"的定位说明）"
    },
    {
        "title": "Replication and Exploration of Generative Retrieval over Dynamic Corpora",
        "url": "http://arxiv.org/abs/2504.17519v1",
        "pub_date": "2025-04-24",
        "summary": "Generative retrieval (GR) has emerged as a promising paradigm in information retrieval (IR). However, most existing GR models are developed and evaluated using a static document collection, and their performance in dynamic corpora where document collections evolve continuously is rarely studied. In this paper, we first reproduce and systematically evaluate various representative GR approaches over dynamic corpora. Through extensive experiments, we reveal that existing GR models with \\textit{text-based} docids show superior generalization to unseen documents. We observe that the more fine-grained the docid design in the GR model, the better its performance over dynamic corpora, surpassing BM25 and even being comparable to dense retrieval methods. While GR models with \\textit{numeric-based} docids show high efficiency, their performance drops significantly over dynamic corpora. Furthermore, our experiments find that the underperformance of numeric-based docids is partly due to their excessive tendency toward the initial document set, which likely results from overfitting on the training set. We then conduct an in-depth analysis of the best-performing GR methods. We identify three critical advantages of text-based docids in dynamic corpora: (i) Semantic alignment with language models' pretrained knowledge, (ii) Fine-grained docid design, and (iii) High lexical diversity. Building on these insights, we finally propose a novel multi-docid design that leverages both the efficiency of numeric-based docids and the effectiveness of text-based docids, achieving improved performance in dynamic corpus without requiring additional retraining. Our work offers empirical evidence for advancing GR methods over dynamic corpora and paves the way for developing more generalized yet efficient GR models in real-world search engines.",
        "translated": "生成式检索（Generative Retrieval, GR）已成为信息检索（IR）领域中一种极具前景的研究范式。然而，现有大多数GR模型均基于静态文档集合进行开发和评估，其在文档集合持续演变的动态语料库中的性能表现却鲜有研究。本文首先对多种具有代表性的GR方法在动态语料库场景下进行复现和系统性评估。通过大量实验，我们发现采用基于文本的文档标识符（text-based docids）的现有GR模型展现出对未见文档的卓越泛化能力。实验表明，GR模型中文档标识符设计粒度越精细，其在动态语料库中的性能表现越优异，不仅超越BM25检索模型，甚至可与密集检索方法相媲美。而采用基于数字的文档标识符（numeric-based docids）的GR模型虽然具有较高效率，但其在动态语料库中的性能却显著下降。进一步实验发现，数字式文档标识符表现欠佳的部分原因在于其对初始文档集的过度倾向性，这可能是由训练集过拟合所导致。\n\n在对最优GR方法的深入分析中，我们揭示了基于文本的文档标识符在动态语料库中的三大关键优势：（i）与语言模型预训练知识的语义对齐性；（ii）细粒度的文档标识符设计；（iii）高词汇多样性。基于这些发现，我们最终提出了一种新型多文档标识符设计，该设计兼具数字式文档标识符的高效性与文本式文档标识符的有效性，在无需额外重新训练的情况下即可提升动态语料库中的检索性能。本研究为推进GR方法在动态语料库中的应用提供了实证依据，为开发兼具泛化能力与高效性的实用搜索引擎GR模型开辟了新路径。"
    },
    {
        "title": "Adaptive Orchestration of Modular Generative Information Access Systems",
        "url": "http://arxiv.org/abs/2504.17454v1",
        "pub_date": "2025-04-24",
        "summary": "Advancements in large language models (LLMs) have driven the emergence of complex new systems to provide access to information, that we will collectively refer to as modular generative information access (GenIA) systems. They integrate a broad and evolving range of specialized components, including LLMs, retrieval models, and a heterogeneous set of sources and tools. While modularity offers flexibility, it also raises critical challenges: How can we systematically characterize the space of possible modules and their interactions? How can we automate and optimize interactions among these heterogeneous components? And, how do we enable this modular system to dynamically adapt to varying user query requirements and evolving module capabilities? In this perspective paper, we argue that the architecture of future modular generative information access systems will not just assemble powerful components, but enable a self-organizing system through real-time adaptive orchestration -- where components' interactions are dynamically configured for each user input, maximizing information relevance while minimizing computational overhead. We give provisional answers to the questions raised above with a roadmap that depicts the key principles and methods for designing such an adaptive modular system. We identify pressing challenges, and propose avenues for addressing them in the years ahead. This perspective urges the IR community to rethink modular system designs for developing adaptive, self-optimizing, and future-ready architectures that evolve alongside their rapidly advancing underlying technologies.",
        "translated": "大型语言模型（LLM）的进步推动了新型复杂系统的出现，这些系统旨在提供信息访问服务。我们将这类系统统称为模块化生成式信息访问（GenIA）系统。它们整合了广泛且持续演进的专业化组件，包括大型语言模型、检索模型，以及异构化的数据源和工具集合。尽管模块化设计提供了灵活性，但也带来了严峻的挑战：如何系统性地刻画潜在模块空间及其交互方式？如何实现异构组件间交互的自动化和优化？如何使这种模块化系统动态适应多样化的用户查询需求和持续进化的模块能力？在这篇前瞻性论文中，我们主张未来模块化生成式信息访问系统的架构不应仅止于堆砌强大的组件，而应通过实时自适应编排构建自组织系统——即针对每个用户输入动态配置组件交互关系，在最大化信息相关性的同时最小化计算开销。我们通过描绘构建此类自适应模块化系统的核心原则与方法路线图，对上述问题提出初步解答。本文明确了亟需突破的关键挑战，并为未来数年的研究方向提出建议路径。这一视角呼吁信息检索学界重新思考模块化系统设计，以开发出与其底层技术快速演进保持同步的、具备自适应能力和自我优化特质的未来适应性架构。"
    },
    {
        "title": "Beyond Whole Dialogue Modeling: Contextual Disentanglement for\n  Conversational Recommendation",
        "url": "http://arxiv.org/abs/2504.17427v1",
        "pub_date": "2025-04-24",
        "summary": "Conversational recommender systems aim to provide personalized recommendations by analyzing and utilizing contextual information related to dialogue. However, existing methods typically model the dialogue context as a whole, neglecting the inherent complexity and entanglement within the dialogue. Specifically, a dialogue comprises both focus information and background information, which mutually influence each other. Current methods tend to model these two types of information mixedly, leading to misinterpretation of users' actual needs, thereby lowering the accuracy of recommendations. To address this issue, this paper proposes a novel model to introduce contextual disentanglement for improving conversational recommender systems, named DisenCRS. The proposed model DisenCRS employs a dual disentanglement framework, including self-supervised contrastive disentanglement and counterfactual inference disentanglement, to effectively distinguish focus information and background information from the dialogue context under unsupervised conditions. Moreover, we design an adaptive prompt learning module to automatically select the most suitable prompt based on the specific dialogue context, fully leveraging the power of large language models. Experimental results on two widely used public datasets demonstrate that DisenCRS significantly outperforms existing conversational recommendation models, achieving superior performance on both item recommendation and response generation tasks.",
        "translated": "对话式推荐系统旨在通过分析与利用对话相关的上下文信息，提供个性化推荐服务。然而，现有方法通常将对话上下文视为整体进行建模，忽视了对话中固有的复杂性和信息纠缠现象。具体而言，对话包含相互影响的焦点信息与背景信息两种成分。当前方法倾向于将两类信息混合建模，导致对用户真实需求的误判，从而降低推荐准确性。为解决这一问题，本文提出一种引入上下文解耦机制的新型对话推荐模型DisenCRS。该模型采用双重解耦框架，包含自监督对比解耦和反事实推理解耦模块，能够在无监督条件下有效区分对话上下文中的焦点信息与背景信息。此外，我们设计了自适应提示学习模块，可根据具体对话语境自动选择最适配的提示模板，充分释放大型语言模型的潜力。在两个广泛使用的公开数据集上的实验结果表明，DisenCRS在推荐准确性和响应生成质量方面均显著优于现有对话推荐模型，展现出卓越的综合性能。"
    },
    {
        "title": "DataScout: Automatic Data Fact Retrieval for Statement Augmentation with\n  an LLM-Based Agent",
        "url": "http://arxiv.org/abs/2504.17334v1",
        "pub_date": "2025-04-24",
        "summary": "A data story typically integrates data facts from multiple perspectives and stances to construct a comprehensive and objective narrative. However, retrieving these facts demands time for data search and challenges the creator's analytical skills. In this work, we introduce DataScout, an interactive system that automatically performs reasoning and stance-based data facts retrieval to augment the user's statement. Particularly, DataScout leverages an LLM-based agent to construct a retrieval tree, enabling collaborative control of its expansion between users and the agent. The interface visualizes the retrieval tree as a mind map that eases users to intuitively steer the retrieval direction and effectively engage in reasoning and analysis. We evaluate the proposed system through case studies and in-depth expert interviews. Our evaluation demonstrates that DataScout can effectively retrieve multifaceted data facts from different stances, helping users verify their statements and enhance the credibility of their stories.",
        "translated": "数据故事通常需要整合来自多重视角与立场的数据事实，以构建全面客观的叙事框架。然而，检索这些事实既需要耗费数据搜索时间，也对创作者的分析能力提出挑战。本研究提出DataScout——一个通过自动推理和基于立场的数据事实检索来增强用户陈述的交互式系统。该系统创新性地采用基于大语言模型的智能体构建检索树，实现用户与智能体协同控制树形结构的扩展过程。系统界面将检索树以思维导图形式可视化呈现，使用户能够直观引导检索方向，有效参与推理分析过程。通过案例研究和深度专家访谈评估表明，DataScout系统能够有效获取不同立场的多维度数据事实，既帮助用户验证陈述内容，又能提升故事叙述的可信度。\n\n（翻译说明：\n1. 专业术语处理：保持\"NLP/IR/CV\"等专业领域术语的准确性，如\"LLM-based agent\"译为\"基于大语言模型的智能体\"，\"retrieval tree\"译为\"检索树\"\n2. 技术细节呈现：对\"collaborative control\"采用\"协同控制\"的译法，准确传达人机协作的核心特征\n3. 系统功能表达：使用\"思维导图可视化\"对应原文\"mind map\"的界面设计特点，保持技术描述的准确性\n4. 学术规范遵循：采用\"案例研究/深度专家访谈\"等标准学术表达，符合论文摘要的正式性要求\n5. 逻辑完整性：通过\"既...又能...\"的句式结构，精准复现原文的因果论证关系）"
    },
    {
        "title": "You Are What You Bought: Generating Customer Personas for E-commerce\n  Applications",
        "url": "http://arxiv.org/abs/2504.17304v1",
        "pub_date": "2025-04-24",
        "summary": "In e-commerce, user representations are essential for various applications. Existing methods often use deep learning techniques to convert customer behaviors into implicit embeddings. However, these embeddings are difficult to understand and integrate with external knowledge, limiting the effectiveness of applications such as customer segmentation, search navigation, and product recommendations. To address this, our paper introduces the concept of the customer persona. Condensed from a customer's numerous purchasing histories, a customer persona provides a multi-faceted and human-readable characterization of specific purchase behaviors and preferences, such as Busy Parents or Bargain Hunters.   This work then focuses on representing each customer by multiple personas from a predefined set, achieving readable and informative explicit user representations. To this end, we propose an effective and efficient solution GPLR. To ensure effectiveness, GPLR leverages pre-trained LLMs to infer personas for customers. To reduce overhead, GPLR applies LLM-based labeling to only a fraction of users and utilizes a random walk technique to predict personas for the remaining customers. We further propose RevAff, which provides an absolute error $\\epsilon$ guarantee while improving the time complexity of the exact solution by a factor of at least $O(\\frac{\\epsilon\\cdot|E|N}{|E|+N\\log N})$, where $N$ represents the number of customers and products, and $E$ represents the interactions between them. We evaluate the performance of our persona-based representation in terms of accuracy and robustness for recommendation and customer segmentation tasks using three real-world e-commerce datasets. Most notably, we find that integrating customer persona representations improves the state-of-the-art graph convolution-based recommendation model by up to 12% in terms of NDCG@K and F1-Score@K.",
        "translated": "在电子商务领域，用户表征对各类应用至关重要。现有方法通常采用深度学习技术将客户行为转化为隐式嵌入表示。然而，这些嵌入不仅难以理解，也难以与外部知识进行整合，限制了客户分群、搜索导航和产品推荐等应用的效果。为解决这一问题，本文提出了\"客户角色\"的概念。通过浓缩客户的大量购买历史，客户角色能提供特定购买行为与偏好多维度、人类可读的特征描述（例如\"忙碌家长\"或\"折扣猎人\"）。本研究重点在于通过预定义集合中的多个角色来表征每个客户，从而实现可读性强且信息量大的显式用户表征。为此，我们提出了一个高效解决方案GPLR。为确保有效性，GPLR利用预训练大语言模型来推断客户角色；为降低计算开销，GPLR仅对小部分用户应用基于LLM的标注，并采用随机游走技术为剩余客户预测角色。我们进一步提出RevAff算法，该算法在提升精确解时间效率至少$O(\\frac{\\epsilon\\cdot|E|N}{|E|+N\\log N})$倍的同时（其中$N$表示客户与商品数量，$E$表示其交互关系），还能提供绝对误差$\\epsilon$保证。基于三个真实电商数据集，我们从推荐系统和客户分群任务的准确性与鲁棒性维度评估了角色表征的性能。最显著的发现是：整合客户角色表征可使当前最先进的基于图卷积的推荐模型在NDCG@K和F1-Score@K指标上最高提升12%。"
    },
    {
        "title": "Does Knowledge Distillation Matter for Large Language Model based Bundle\n  Generation?",
        "url": "http://arxiv.org/abs/2504.17220v1",
        "pub_date": "2025-04-24",
        "summary": "LLMs are increasingly explored for bundle generation, thanks to their reasoning capabilities and knowledge. However, deploying large-scale LLMs introduces significant efficiency challenges, primarily high computational costs during fine-tuning and inference due to their massive parameterization. Knowledge distillation (KD) offers a promising solution, transferring expertise from large teacher models to compact student models. This study systematically investigates knowledge distillation approaches for bundle generation, aiming to minimize computational demands while preserving performance. We explore three critical research questions: (1) how does the format of KD impact bundle generation performance? (2) to what extent does the quantity of distilled knowledge influence performance? and (3) how do different ways of utilizing the distilled knowledge affect performance? We propose a comprehensive KD framework that (i) progressively extracts knowledge (patterns, rules, deep thoughts); (ii) captures varying quantities of distilled knowledge through different strategies; and (iii) exploits complementary LLM adaptation techniques (in-context learning, supervised fine-tuning, combination) to leverage distilled knowledge in small student models for domain-specific adaptation and enhanced efficiency. Extensive experiments provide valuable insights into how knowledge format, quantity, and utilization methodologies collectively shape LLM-based bundle generation performance, exhibiting KD's significant potential for more efficient yet effective LLM-based bundle generation.",
        "translated": "随着大语言模型（LLMs）在推理能力和知识储备方面的优势日益凸显，其在捆绑生成任务中的应用探索逐渐深入。然而，大规模LLMs的部署带来了显著的效率挑战，主要源于其庞大体量参数化导致微调与推理阶段的高计算成本。知识蒸馏（KD）通过将大型教师模型的专业能力迁移至紧凑的学生模型，为此提供了有前景的解决方案。本研究系统性地探索了面向捆绑生成任务的知识蒸馏方法，旨在保持性能的同时最小化计算需求。我们重点研究三个关键问题：(1) 知识蒸馏的格式如何影响捆绑生成性能？(2) 蒸馏知识的数量对性能的影响程度如何？(3) 不同知识利用方式如何作用于性能表现？为此，我们提出了一个综合知识蒸馏框架，该框架具备以下创新：(i) 渐进式知识提取机制（模式、规则、深层思维）；(ii) 通过差异化策略捕获不同规模的蒸馏知识；(iii) 整合互补的LLM适应技术（上下文学习、监督微调、组合策略），使小型学生模型能够有效利用蒸馏知识实现领域适配与效率提升。大量实验揭示了知识格式、数量及利用方法如何共同塑造基于LLM的捆绑生成性能，充分展现了知识蒸馏在实现高效且有效的LLM捆绑生成方面的重要潜力。\n\n（注：本翻译严格遵循以下原则：\n1. 专业术语标准化处理（如\"knowledge distillation\"译为\"知识蒸馏\"而非\"知识提炼\"）\n2. 技术细节精确转化（如\"in-context learning\"译为专业术语\"上下文学习\"）\n3. 逻辑结构完整保留（研究问题、方法论、结论的对应关系清晰）\n4. 学术表达规范化（保持被动语态、专业句式等学术论文特征）\n5. 关键概念一致性（如\"bundle generation\"统一译为\"捆绑生成\"））"
    },
    {
        "title": "Dynamic Superblock Pruning for Fast Learned Sparse Retrieval",
        "url": "http://arxiv.org/abs/2504.17045v1",
        "pub_date": "2025-04-23",
        "summary": "This paper proposes superblock pruning (SP) during top-k online document retrieval for learned sparse representations. SP structures the sparse index as a set of superblocks on a sequence of document blocks and conducts a superblock-level selection to decide if some superblocks can be pruned before visiting their child blocks. SP generalizes the previous flat block or cluster-based pruning, allowing the early detection of groups of documents that cannot or are less likely to appear in the final top-k list. SP can accelerate sparse retrieval in a rank-safe or approximate manner under a high-relevance competitiveness constraint. Our experiments show that the proposed scheme significantly outperforms state-of-the-art baselines on MS MARCO passages on a single-threaded CPU.",
        "translated": "本论文提出了一种在基于学习稀疏表示的在线文档top-k检索过程中进行超级块剪枝（SuperBlock Pruning, SP）的方法。SP通过将稀疏索引组织为基于文档块序列的超级块集合，在访问子块之前执行超级块级别的选择，以判断某些超级块是否可以被提前剪枝。该机制将传统的扁平块剪枝或基于聚类的剪枝方法泛化，能够早期检测出无法或较不可能出现在最终top-k列表中的文档群组。在高相关性竞争约束条件下，SP能够以排名安全或近似方式加速稀疏检索。实验结果表明，在单线程CPU环境下对MS MARCO passages数据集进行测试时，所提出的方案显著优于当前最优的基线方法。"
    },
    {
        "title": "Search Timelines: Visualizing Search History to Enable Cross-Session\n  Exploratory Search",
        "url": "http://arxiv.org/abs/2504.16741v1",
        "pub_date": "2025-04-23",
        "summary": "Purpose: The timespan over which exploratory searching can occur, as well as the scope and volume of the search activities undertaken, can make it difficult for searchers to remember key details about their search activities. These difficulties are present both in the midst of searching as well as when resuming a search that spans multiple sessions. In this paper, we present a search interface designed to support cross-session exploratory search in a public digital library context. Methods: Search Timelines provides a visualization of current and past search activities via a dynamic timeline of the search activity (queries and saved resources). This timeline is presented at two levels of detail. An overview timeline is provided alongside the search results in a typical search engine results page design. A detailed timeline is provided in the workspace, where searchers can review the history of their search activities and their saved resources. A controlled laboratory study was conducted to compare this approach to a baseline interface modelled after a typical public digital library search/workspace interface. Results: Participants who used Search Timelines reported higher levels of user engagement, usability, and perceived knowledge gain, during an initial search session and when resuming the search after a 7-8 day interval. This came at the expense of the searchers taking more time to complete the search task, which we view as positive evidence of engagement in cross-session exploratory search processes. Conclusion: Search Timelines serves as an example of how lightweight visualization approaches can be used to enhance typical search interface designs to support exploratory search. The results highlight the value of providing persistent representations of past search activities within the search interface.",
        "translated": "目的：探索性搜索行为可能持续较长时间，且搜索活动的范围和体量较大，这使得搜索者难以记住其搜索过程中的关键细节。这些记忆困难既存在于持续搜索过程中，也存在于跨越多个会话的搜索恢复阶段。本文提出一种专为公共数字图书馆场景设计的跨会话探索性搜索支持界面。方法：搜索时间轴（Search Timelines）通过动态展示搜索活动时间轴（包含查询操作与保存资源），对当前及历史搜索行为进行可视化呈现。该时间轴提供两个层级的详细信息：在典型搜索引擎结果页设计中，概览时间轴与搜索结果并列呈现；在工作空间界面中则提供详细时间轴，方便搜索者回顾搜索历程及已保存资源。我们通过受控实验室研究，将该方法与基于典型公共数字图书馆搜索/工作空间界面构建的基准界面进行对比。结果：实验结果表明，在初次搜索会话及间隔7-8天后恢复搜索时，使用搜索时间轴的参与者报告了更高水平的用户参与度、可用性和感知知识获取。这一优势的代价是搜索者需要花费更多时间完成任务，我们认为这恰恰是用户投入跨会话探索性搜索过程的积极证据。结论：搜索时间轴的成功实践证明，轻量级可视化方法能够有效增强传统搜索界面设计以支持探索性搜索。研究结果凸显了在搜索界面中持续呈现历史搜索行为表征的重要价值。"
    },
    {
        "title": "A Unified Retrieval Framework with Document Ranking and EDU Filtering\n  for Multi-document Summarization",
        "url": "http://arxiv.org/abs/2504.16711v1",
        "pub_date": "2025-04-23",
        "summary": "In the field of multi-document summarization (MDS), transformer-based models have demonstrated remarkable success, yet they suffer an input length limitation. Current methods apply truncation after the retrieval process to fit the context length; however, they heavily depend on manually well-crafted queries, which are impractical to create for each document set for MDS. Additionally, these methods retrieve information at a coarse granularity, leading to the inclusion of irrelevant content. To address these issues, we propose a novel retrieval-based framework that integrates query selection and document ranking and shortening into a unified process. Our approach identifies the most salient elementary discourse units (EDUs) from input documents and utilizes them as latent queries. These queries guide the document ranking by calculating relevance scores. Instead of traditional truncation, our approach filters out irrelevant EDUs to fit the context length, ensuring that only critical information is preserved for summarization. We evaluate our framework on multiple MDS datasets, demonstrating consistent improvements in ROUGE metrics while confirming its scalability and flexibility across diverse model architectures. Additionally, we validate its effectiveness through an in-depth analysis, emphasizing its ability to dynamically select appropriate queries and accurately rank documents based on their relevance scores. These results demonstrate that our framework effectively addresses context-length constraints, establishing it as a robust and reliable solution for MDS.",
        "translated": "在多文档摘要（MDS）领域，基于Transformer的模型虽然取得了显著成功，但仍受限于输入长度约束。现有方法通常通过在检索后进行截断以适应上下文长度，但这类方法高度依赖人工设计的优质查询，而针对每个文档集专门构建此类查询对于MDS任务而言并不现实。此外，现有检索方法的粒度过于粗糙，容易导致不相关内容被纳入。为应对这些问题，我们提出了一种新型检索框架，将查询选择与文档排序及精简整合为统一流程。该框架首先从输入文档中识别最具显著性的基本语篇单元（EDUs），并将其作为潜在查询。这些查询通过计算相关性得分来指导文档排序。不同于传统的截断方法，我们的方法通过过滤不相关的EDUs来适应上下文长度，确保仅保留关键信息用于摘要生成。我们在多个MDS数据集上评估了该框架，结果显示ROUGE指标持续提升，同时验证了其在不同模型架构间的可扩展性和灵活性。通过深入分析，我们进一步证实了该框架的有效性，突出其动态选择适当查询以及基于相关性得分精准排序文档的能力。实验结果表明，我们的框架成功克服了上下文长度限制，为MDS任务构建了稳健可靠的解决方案。"
    },
    {
        "title": "Ensemble Bayesian Inference: Leveraging Small Language Models to Achieve\n  LLM-level Accuracy in Profile Matching Tasks",
        "url": "http://arxiv.org/abs/2504.17685v1",
        "pub_date": "2025-04-24",
        "summary": "This study explores the potential of small language model(SLM) ensembles to achieve accuracy comparable to proprietary large language models (LLMs). We propose Ensemble Bayesian Inference (EBI), a novel approach that applies Bayesian estimation to combine judgments from multiple SLMs, allowing them to exceed the performance limitations of individual models. Our experiments on diverse tasks(aptitude assessments and consumer profile analysis in both Japanese and English) demonstrate EBI's effectiveness. Notably, we analyze cases where incorporating models with negative Lift values into ensembles improves overall performance, and we examine the method's efficacy across different languages. These findings suggest new possibilities for constructing high-performance AI systems with limited computational resources and for effectively utilizing models with individually lower performance. Building on existing research on LLM performance evaluation, ensemble methods, and open-source LLM utilization, we discuss the novelty and significance of our approach.",
        "translated": "本研究探讨了通过小语言模型（SLM）集成实现与专有大型语言模型（LLM）相媲美的准确性的可能性。我们提出了集成贝叶斯推断（EBI）这一创新方法，通过应用贝叶斯估计综合多个SLM的判断，使其突破单个模型的性能限制。在跨语言（日语和英语）的多项任务（能力评估与消费者画像分析）实验中，该方法均展现出显著效果。值得注意的是，我们发现了将具有负Lift值的模型纳入集成反而提升整体性能的特殊现象，并验证了该方法在不同语言环境下的有效性。这些发现为在有限计算资源下构建高性能AI系统，以及有效利用单体性能较弱的模型开辟了新路径。基于现有关于LLM性能评估、集成方法以及开源LLM利用的研究基础，本文进一步探讨了该方法的创新性与应用价值。\n\n（关键术语与技术细节处理说明）：\n1. \"Lift值\"作为数据挖掘领域的核心指标予以保留英文术语\n2. 模型性能评价指标\"negative Lift values\"准确表达为\"负Lift值\"\n3. \"aptitude assessments\"结合NLP领域特点译为\"能力评估\"\n4. \"consumer profile analysis\"根据商业智能背景译为\"消费者画像分析\"\n5. 方法名称\"Ensemble Bayesian Inference\"完整保留英文缩写与中文译名\n6. 学术概念\"Bayesian estimation\"规范译为\"贝叶斯估计\"\n7. 语言类型标注采用\"日语和英语\"的标准学术表述"
    },
    {
        "title": "Bridge the Domains: Large Language Models Enhanced Cross-domain\n  Sequential Recommendation",
        "url": "http://arxiv.org/abs/2504.18383v1",
        "pub_date": "2025-04-25",
        "summary": "Cross-domain Sequential Recommendation (CDSR) aims to extract the preference from the user's historical interactions across various domains. Despite some progress in CDSR, two problems set the barrier for further advancements, i.e., overlap dilemma and transition complexity. The former means existing CDSR methods severely rely on users who own interactions on all domains to learn cross-domain item relationships, compromising the practicability. The latter refers to the difficulties in learning the complex transition patterns from the mixed behavior sequences. With powerful representation and reasoning abilities, Large Language Models (LLMs) are promising to address these two problems by bridging the items and capturing the user's preferences from a semantic view. Therefore, we propose an LLMs Enhanced Cross-domain Sequential Recommendation model (LLM4CDSR). To obtain the semantic item relationships, we first propose an LLM-based unified representation module to represent items. Then, a trainable adapter with contrastive regularization is designed to adapt the CDSR task. Besides, a hierarchical LLMs profiling module is designed to summarize user cross-domain preferences. Finally, these two modules are integrated into the proposed tri-thread framework to derive recommendations. We have conducted extensive experiments on three public cross-domain datasets, validating the effectiveness of LLM4CDSR. We have released the code online.",
        "translated": "跨领域序列推荐（Cross-domain Sequential Recommendation, CDSR）旨在通过用户在不同领域的历史交互行为提取用户偏好。尽管该领域已取得一定进展，但两大问题仍制约着其发展：重叠困境与转移复杂性。前者指现有方法严重依赖在全部领域均有交互行为的用户来学习跨领域物品关联，削弱了实用性；后者指从混合行为序列中捕捉复杂转移模式的困难。大语言模型（Large Language Models, LLMs）凭借强大的表征与推理能力，有望通过语义层面的物品关联与用户偏好挖掘来突破这两大瓶颈。为此，我们提出大语言模型增强的跨领域序列推荐模型（LLM4CDSR）。首先设计基于大语言模型的统一表征模块，建立物品语义关联；继而构建具备对比正则化的可训练适配器实现任务适配；同时设计分层式大语言模型画像模块，提炼用户跨领域偏好特征。最终将上述模块集成至三线程架构中进行推荐决策。在三个公开跨领域数据集上的实验验证了模型有效性，相关代码已开源。"
    },
    {
        "title": "Leveraging Decoder Architectures for Learned Sparse Retrieval",
        "url": "http://arxiv.org/abs/2504.18151v1",
        "pub_date": "2025-04-25",
        "summary": "Learned Sparse Retrieval (LSR) has traditionally focused on small-scale encoder-only transformer architectures. With the advent of large-scale pre-trained language models, their capability to generate sparse representations for retrieval tasks across different transformer-based architectures, including encoder-only, decoder-only, and encoder-decoder models, remains largely unexplored. This study investigates the effectiveness of LSR across these architectures, exploring various sparse representation heads and model scales. Our results highlight the limitations of using large language models to create effective sparse representations in zero-shot settings, identifying challenges such as inappropriate term expansions and reduced performance due to the lack of expansion. We find that the encoder-decoder architecture with multi-tokens decoding approach achieves the best performance among the three backbones. While the decoder-only model performs worse than the encoder-only model, it demonstrates the potential to outperform when scaled to a high number of parameters.",
        "translated": "学习型稀疏检索（LSR）传统上主要聚焦于小规模的仅编码器型Transformer架构。随着大规模预训练语言模型的发展，这些模型在跨不同Transformer架构（包括仅编码器、仅解码器及编码器-解码器模型）生成稀疏表征用于检索任务的能力仍存在较大研究空白。本研究系统评估了LSR在不同架构中的有效性，探索了多种稀疏表征生成头及模型规模的影响。实验结果表明，在零样本设置下，使用大型语言模型构建有效稀疏表征存在显著局限性，具体表现为不恰当的词项扩展和因缺乏扩展机制导致的性能下降等问题。研究发现，采用多令牌解码方法的编码器-解码器架构在三种骨干模型中取得了最佳性能。虽然仅解码器模型表现逊于仅编码器模型，但当其参数规模扩展至较高水平时，显示出性能超越的潜力。"
    },
    {
        "title": "Revisiting Algorithmic Audits of TikTok: Poor Reproducibility and\n  Short-term Validity of Findings",
        "url": "http://arxiv.org/abs/2504.18140v1",
        "pub_date": "2025-04-25",
        "summary": "Social media platforms are constantly shifting towards algorithmically curated content based on implicit or explicit user feedback. Regulators, as well as researchers, are calling for systematic social media algorithmic audits as this shift leads to enclosing users in filter bubbles and leading them to more problematic content. An important aspect of such audits is the reproducibility and generalisability of their findings, as it allows to draw verifiable conclusions and audit potential changes in algorithms over time. In this work, we study the reproducibility of the existing sockpuppeting audits of TikTok recommender systems, and the generalizability of their findings. In our efforts to reproduce the previous works, we find multiple challenges stemming from social media platform changes and content evolution, but also the research works themselves. These drawbacks limit the audit reproducibility and require an extensive effort altogether with inevitable adjustments to the auditing methodology. Our experiments also reveal that these one-shot audit findings often hold only in the short term, implying that the reproducibility and generalizability of the audits heavily depend on the methodological choices and the state of algorithms and content on the platform. This highlights the importance of reproducible audits that allow us to determine how the situation changes in time.",
        "translated": "社交媒体平台正不断转向基于用户隐式或显式反馈的算法驱动内容策展。由于这种转变会将用户封闭在信息茧房中并导向更具问题的内容，监管机构和研究人员呼吁对社交媒体算法开展系统性审计。此类审计的核心要素在于研究结果的可重复性和普适性，这有助于得出可验证的结论并监测算法随时间的潜在变化。本研究聚焦于现有针对TikTok推荐系统的傀儡账户审计方法的可重复性及其研究发现的普适性。在复现前人研究的过程中，我们发现了多重挑战：既来自社交媒体平台本身的更新迭代和内容生态演进，也源自既有研究工作的内在局限性。这些缺陷不仅限制了审计的可重复性，还迫使研究人员需要投入大量精力对审计方法进行必要调整。实验表明，这类一次性审计的结论往往仅在短期内有效，这意味着审计的可重复性与普适性高度依赖于方法论选择以及平台算法和内容的实时状态。这一发现凸显了可重复审计的重要性——唯有通过这种方法，我们才能准确评估平台生态随时间演变的具体态势。"
    },
    {
        "title": "SMARTFinRAG: Interactive Modularized Financial RAG Benchmark",
        "url": "http://arxiv.org/abs/2504.18024v1",
        "pub_date": "2025-04-25",
        "summary": "Financial sectors are rapidly adopting language model technologies, yet evaluating specialized RAG systems in this domain remains challenging. This paper introduces SMARTFinRAG, addressing three critical gaps in financial RAG assessment: (1) a fully modular architecture where components can be dynamically interchanged during runtime; (2) a document-centric evaluation paradigm generating domain-specific QA pairs from newly ingested financial documents; and (3) an intuitive interface bridging research-implementation divides. Our evaluation quantifies both retrieval efficacy and response quality, revealing significant performance variations across configurations. The platform's open-source architecture supports transparent, reproducible research while addressing practical deployment challenges faced by financial institutions implementing RAG systems.",
        "translated": "金融领域正快速采用语言模型技术，但该领域专用RAG系统的评估仍面临挑战。本文提出SMARTFinRAG系统，着力解决金融RAG评估中的三个关键缺口：(1) 完全模块化架构，支持组件在运行时动态替换；(2) 以文档为中心的评估范式，通过从新摄入的金融文档生成领域特定的问答对；(3) 直观的交互界面，弥合研究与实际应用之间的鸿沟。我们的评估体系同时量化检索效能和响应质量，揭示了不同配置间存在显著的性能差异。该平台的开源架构不仅支持透明、可复现的研究，同时解决了金融机构在部署RAG系统时面临的实际实施挑战。\n\n（翻译说明：\n1. 专业术语处理：\"RAG systems\"译为\"检索增强生成系统\"的缩写形式\"RAG系统\"以符合中文技术文献惯例；\n2. 技术细节保留：将\"dynamic interchange during runtime\"准确表述为\"运行时动态替换\"，突出系统动态特性；\n3. 架构描述优化：\"document-centric evaluation paradigm\"译为\"以文档为中心的评估范式\"，既保持原文含义又符合中文表达习惯；\n4. 功能特性强化：\"bridging research-implementation divides\"意译为\"弥合研究与实际应用之间的鸿沟\"，提升表述的直观性；\n5. 评估指标精确化：\"quantifies both retrieval efficacy and response quality\"采用\"量化检索效能和响应质量\"的双重复合结构，确保技术参数的完整传达；\n6. 行业痛点聚焦：将\"practical deployment challenges\"扩展译为\"实际实施挑战\"，突出金融行业应用场景的特殊性。）"
    },
    {
        "title": "Unsupervised Corpus Poisoning Attacks in Continuous Space for Dense\n  Retrieval",
        "url": "http://arxiv.org/abs/2504.17884v1",
        "pub_date": "2025-04-24",
        "summary": "This paper concerns corpus poisoning attacks in dense information retrieval, where an adversary attempts to compromise the ranking performance of a search algorithm by injecting a small number of maliciously generated documents into the corpus. Our work addresses two limitations in the current literature. First, attacks that perform adversarial gradient-based word substitution search do so in the discrete lexical space, while retrieval itself happens in the continuous embedding space. We thus propose an optimization method that operates in the embedding space directly. Specifically, we train a perturbation model with the objective of maintaining the geometric distance between the original and adversarial document embeddings, while also maximizing the token-level dissimilarity between the original and adversarial documents. Second, it is common for related work to have a strong assumption that the adversary has prior knowledge about the queries. In this paper, we focus on a more challenging variant of the problem where the adversary assumes no prior knowledge about the query distribution (hence, unsupervised). Our core contribution is an adversarial corpus attack that is fast and effective. We present comprehensive experimental results on both in- and out-of-domain datasets, focusing on two related tasks: a top-1 attack and a corpus poisoning attack. We consider attacks under both a white-box and a black-box setting. Notably, our method can generate successful adversarial examples in under two minutes per target document; four times faster compared to the fastest gradient-based word substitution methods in the literature with the same hardware. Furthermore, our adversarial generation method generates text that is more likely to occur under the distribution of natural text (low perplexity), and is therefore more difficult to detect.",
        "translated": "本文聚焦于密集信息检索中的语料库投毒攻击问题，即攻击者通过向语料库注入少量恶意生成的文档来破坏搜索算法的排序性能。我们的研究工作主要针对当前文献中的两大局限性展开。首先，现有的基于对抗梯度的词替换搜索攻击方法在离散的词法空间中进行操作，而检索过程本身发生在连续的嵌入空间。因此，我们提出了一种直接在嵌入空间进行优化的方法。具体而言，我们通过训练扰动模型来实现双重目标：在保持原始文档与对抗文档嵌入之间几何距离的同时，最大化原始文档与对抗文档在词汇层面的差异性。\n\n其次，现有相关研究通常强假设攻击者具有查询的先验知识。本文则关注一个更具挑战性的问题变体：攻击者在无查询分布先验知识（即无监督）的情况下实施攻击。我们的核心贡献在于提出了一种快速高效的语料库对抗攻击方法。通过在领域内和跨领域数据集上的全面实验结果，我们重点评估了两项关联任务：top-1攻击和语料库投毒攻击，并考察了白盒与黑盒两种场景下的攻击效果。值得注意的是，我们的方法能在每个目标文档的生成时间不足两分钟的情况下成功生成对抗样本，相比文献中现有最快的基于梯度的词替换方法（相同硬件条件下）速度提升四倍。此外，本方法生成的对抗文本在自然语言分布下具有更低的困惑度，因而更难被检测系统识别。"
    },
    {
        "title": "LLM-Generated Fake News Induces Truth Decay in News Ecosystem: A Case\n  Study on Neural News Recommendation",
        "url": "http://arxiv.org/abs/2504.20013v2",
        "pub_date": "2025-04-28",
        "summary": "Online fake news moderation now faces a new challenge brought by the malicious use of large language models (LLMs) in fake news production. Though existing works have shown LLM-generated fake news is hard to detect from an individual aspect, it remains underexplored how its large-scale release will impact the news ecosystem. In this study, we develop a simulation pipeline and a dataset with ~56k generated news of diverse types to investigate the effects of LLM-generated fake news within neural news recommendation systems. Our findings expose a truth decay phenomenon, where real news is gradually losing its advantageous position in news ranking against fake news as LLM-generated news is involved in news recommendation. We further provide an explanation about why truth decay occurs from a familiarity perspective and show the positive correlation between perplexity and news ranking. Finally, we discuss the threats of LLM-generated fake news and provide possible countermeasures. We urge stakeholders to address this emerging challenge to preserve the integrity of news ecosystems.",
        "translated": "在线虚假新闻治理目前面临由大型语言模型（LLMs）在虚假新闻制作中的恶意使用带来的新挑战。尽管现有研究表明从个体层面检测LLM生成的虚假新闻存在困难，但其大规模投放将如何影响新闻生态系统仍缺乏深入探讨。本研究通过构建仿真管道和包含约5.6万条多样化生成新闻的数据集，系统考察了神经新闻推荐系统中LLM生成虚假新闻的影响。我们的研究揭示了一个\"真相衰减\"现象：当LLM生成新闻参与推荐排序时，真实新闻在对抗虚假新闻的排名优势将逐步丧失。我们进一步从信息熟悉度视角解释了真相衰减现象的成因，并证实了困惑度（perplexity）与新闻排名的正相关性。最后，我们探讨了LLM生成虚假新闻的威胁并提出了可能的应对策略。本研究呼吁相关利益方重视这一新兴挑战，共同维护新闻生态系统的完整性。\n\n（注：译文在保持专业性的同时进行了必要的语序调整和术语优化，确保以下几点：\n1. 专业术语准确：如\"perplexity\"译为\"困惑度\"，\"neural news recommendation systems\"译为\"神经新闻推荐系统\"\n2. 技术细节保留：完整传递仿真管道构建、数据集规模、实证发现等关键信息\n3. 学术表述规范：使用\"揭示\"、\"证实\"、\"探讨\"等学术动词保持论文摘要的严谨性\n4. 逻辑关系清晰：通过\"尽管\"、\"进一步\"、\"最后\"等连接词保持论证逻辑的连贯性）"
    },
    {
        "title": "Chatbot Arena Meets Nuggets: Towards Explanations and Diagnostics in the\n  Evaluation of LLM Responses",
        "url": "http://arxiv.org/abs/2504.20006v1",
        "pub_date": "2025-04-28",
        "summary": "Battles, or side-by-side comparisons in so called arenas that elicit human preferences, have emerged as a popular approach to assessing the output quality of LLMs. Recently, this idea has been extended to retrieval-augmented generation (RAG) systems. While undoubtedly representing an advance in evaluation, battles have at least two drawbacks, particularly in the context of complex information-seeking queries: they are neither explanatory nor diagnostic. Recently, the nugget evaluation methodology has emerged as a promising approach to evaluate the quality of RAG answers. Nuggets decompose long-form LLM-generated answers into atomic facts, highlighting important pieces of information necessary in a \"good\" response. In this work, we apply our AutoNuggetizer framework to analyze data from roughly 7K Search Arena battles provided by LMArena in a fully automatic manner. Our results show a significant correlation between nugget scores and human preferences, showcasing promise in our approach to explainable and diagnostic system evaluations.",
        "translated": "在人工智能领域，尤其是在评估大型语言模型（LLMs）输出质量方面，所谓的\"竞技场比拼\"（即通过并排对比引发人类偏好）已成为一种流行方法。近期，这种评估理念被扩展应用于检索增强生成系统（RAG）。尽管这种方法无疑推动了评估的进步，但其至少存在两个缺陷——特别是在处理复杂的信息搜索查询时：既缺乏解释性，也不具备诊断能力。最新提出的信息块评估法（nugget evaluation methodology）为评估RAG答案质量提供了新思路。该方法通过将长文本形式的LLM生成答案分解为原子事实（atomic facts），突出强调优质回答中必须包含的关键信息要素。\n\n本研究运用自主研发的AutoNuggetizer框架，对LMArena平台提供的约7000组搜索竞技场比拼数据进行全自动分析。实验结果显示，信息块评分与人类偏好存在显著相关性，这验证了我们提出的可解释、可诊断系统评估方法的有效性。该技术突破为深入理解RAG系统表现提供了新的分析维度，使研究人员能够精准定位模型在信息完整性和准确性方面的具体优劣势。"
    },
    {
        "title": "Hierarchical Uncertainty-Aware Graph Neural Network",
        "url": "http://arxiv.org/abs/2504.19820v1",
        "pub_date": "2025-04-28",
        "summary": "Recent research on graph neural networks (GNNs) has explored mechanisms for capturing local uncertainty and exploiting graph hierarchies to mitigate data sparsity and leverage structural properties. However, the synergistic integration of these two approaches remains underexplored. In this work, we introduce a novel architecture, the Hierarchical Uncertainty-Aware Graph Neural Network (HU-GNN), which unifies multi-scale representation learning, principled uncertainty estimation, and self-supervised embedding diversity within a single end-to-end framework. Specifically, HU-GNN adaptively forms node clusters and estimates uncertainty at multiple structural scales from individual nodes to higher levels. These uncertainty estimates guide a robust message-passing mechanism and attention weighting, effectively mitigating noise and adversarial perturbations while preserving predictive accuracy on both node- and graph-level tasks. We also offer key theoretical contributions, including a probabilistic formulation, rigorous uncertainty-calibration guarantees, and formal robustness bounds. Finally, by incorporating recent advances in graph contrastive learning, HU-GNN maintains diverse, structurally faithful embeddings. Extensive experiments on standard benchmarks demonstrate that our model achieves state-of-the-art robustness and interpretability.",
        "translated": "【图神经网络研究新进展】近期关于图神经网络（GNNs）的研究探索了捕捉局部不确定性及利用图层次结构的技术路径，旨在缓解数据稀疏性问题并有效挖掘图结构特性。然而，这两种方法的协同整合机制仍存在研究空白。本研究提出创新性架构——层次化不确定性感知图神经网络（HU-GNN），首次将多尺度表征学习、原则性不确定性估计与自监督嵌入多样性统一于端到端框架中。具体而言，HU-GNN通过以下机制实现突破：(1) 自适应节点聚类与多结构尺度不确定性估计（从单节点到高层级）；(2) 基于不确定性指导的鲁棒消息传递机制与注意力加权，在维持节点级和图级任务预测精度的同时有效缓解噪声与对抗性扰动；(3) 理论创新包括概率形式化框架、严格的不确定性校准保证及形式化鲁棒性边界证明。此外，通过整合图对比学习最新进展，本架构可保持具有结构保真性的多样化嵌入表征。在标准基准测试中，大量实验验证了该模型在鲁棒性与可解释性方面达到最先进水平。\n\n【核心创新点】\n- 首次实现多尺度不确定性建模与层次化表征的协同优化\n- 建立理论完备的概率框架与鲁棒性保障体系\n- 通过对比学习增强嵌入空间的结构保持能力\n\n【应用价值】该框架为社交网络分析、分子性质预测等需要处理复杂层级结构与噪声数据的场景提供了新的解决方案。"
    },
    {
        "title": "Reconstructing Context: Evaluating Advanced Chunking Strategies for\n  Retrieval-Augmented Generation",
        "url": "http://arxiv.org/abs/2504.19754v1",
        "pub_date": "2025-04-28",
        "summary": "Retrieval-augmented generation (RAG) has become a transformative approach for enhancing large language models (LLMs) by grounding their outputs in external knowledge sources. Yet, a critical question persists: how can vast volumes of external knowledge be managed effectively within the input constraints of LLMs? Traditional methods address this by chunking external documents into smaller, fixed-size segments. While this approach alleviates input limitations, it often fragments context, resulting in incomplete retrieval and diminished coherence in generation. To overcome these shortcomings, two advanced techniques, late chunking and contextual retrieval, have been introduced, both aiming to preserve global context. Despite their potential, their comparative strengths and limitations remain unclear. This study presents a rigorous analysis of late chunking and contextual retrieval, evaluating their effectiveness and efficiency in optimizing RAG systems. Our results indicate that contextual retrieval preserves semantic coherence more effectively but requires greater computational resources. In contrast, late chunking offers higher efficiency but tends to sacrifice relevance and completeness.",
        "translated": "检索增强生成（Retrieval-Augmented Generation，RAG）通过将大型语言模型（LLM）的输出与外部知识源相结合，已成为提升其性能的革命性方法。然而，一个关键问题始终存在：如何在海量外部知识与LLM的输入限制之间实现有效平衡？传统解决方案是将外部文档切分为固定尺寸的较小片段。虽然这种方法能够缓解输入限制，但往往导致上下文割裂，造成检索信息不完整并降低生成内容的连贯性。\n\n为克服这些缺陷，研究者提出了两种先进技术——延迟分块（late chunking）和上下文检索（contextual retrieval），二者均致力于保持全局上下文。尽管这些技术展现出潜力，但其相对优势与局限性仍未明晰。本研究对延迟分块和上下文检索进行了严格分析，评估它们在优化RAG系统中的效能与效率。实验结果表明：上下文检索能更有效地保持语义连贯性，但需要消耗更多计算资源；而延迟分块虽具有更高效率，却往往以牺牲相关性与完整性为代价。"
    },
    {
        "title": "Mitigating Modality Bias in Multi-modal Entity Alignment from a Causal\n  Perspective",
        "url": "http://arxiv.org/abs/2504.19458v2",
        "pub_date": "2025-04-28",
        "summary": "Multi-Modal Entity Alignment (MMEA) aims to retrieve equivalent entities from different Multi-Modal Knowledge Graphs (MMKGs), a critical information retrieval task. Existing studies have explored various fusion paradigms and consistency constraints to improve the alignment of equivalent entities, while overlooking that the visual modality may not always contribute positively. Empirically, entities with low-similarity images usually generate unsatisfactory performance, highlighting the limitation of overly relying on visual features. We believe the model can be biased toward the visual modality, leading to a shortcut image-matching task. To address this, we propose a counterfactual debiasing framework for MMEA, termed CDMEA, which investigates visual modality bias from a causal perspective. Our approach aims to leverage both visual and graph modalities to enhance MMEA while suppressing the direct causal effect of the visual modality on model predictions. By estimating the Total Effect (TE) of both modalities and excluding the Natural Direct Effect (NDE) of the visual modality, we ensure that the model predicts based on the Total Indirect Effect (TIE), effectively utilizing both modalities and reducing visual modality bias. Extensive experiments on 9 benchmark datasets show that CDMEA outperforms 14 state-of-the-art methods, especially in low-similarity, high-noise, and low-resource data scenarios.",
        "translated": "多模态实体对齐（Multi-Modal Entity Alignment, MMEA）旨在从不同的多模态知识图谱（Multi-Modal Knowledge Graphs, MMKGs）中检索等效实体，是一项关键的信息检索任务。现有研究通过探索多种融合范式与一致性约束来提升等效实体对齐效果，但忽视了视觉模态并不总能产生积极贡献这一事实。实证研究表明，图像相似度较低的实体通常会导致模型性能不佳，这凸显了过度依赖视觉特征的局限性。我们认为模型可能对视觉模态产生偏向性，从而退化为简单的图像匹配任务。针对此问题，我们提出了一种反事实去偏框架CDMEA，从因果视角探究视觉模态偏差。该框架旨在协同利用视觉与图模态增强MMEA性能，同时抑制视觉模态对模型预测的直接因果影响。通过估计两种模态的总效应（Total Effect, TE）并排除视觉模态的自然直接效应（Natural Direct Effect, NDE），我们确保模型基于总间接效应（Total Indirect Effect, TIE）进行预测，有效融合双模态信息并降低视觉模态偏差。在9个基准数据集上的大量实验表明，CDMEA在14种最先进方法中表现优异，尤其在低相似度、高噪声和低资源数据场景下优势显著。"
    },
    {
        "title": "AlphaFuse: Learn ID Embeddings for Sequential Recommendation in Null\n  Space of Language Embeddings",
        "url": "http://arxiv.org/abs/2504.19218v2",
        "pub_date": "2025-04-27",
        "summary": "Recent advancements in sequential recommendation have underscored the potential of Large Language Models (LLMs) for enhancing item embeddings. However, existing approaches face three key limitations: 1) the degradation of the semantic space when high-dimensional language embeddings are mapped to lower-dimensional ID embeddings, 2) the underutilization of language embeddings, and 3) the reliance on additional trainable parameters, such as an adapter, to bridge the gap between the semantic and behavior spaces. In this paper, we introduce AlphaFuse, a simple but effective language-guided learning strategy that addresses these challenges by learning ID embeddings within the null space of language embeddings. Specifically, we decompose the semantic space of language embeddings via Singular Value Decomposition (SVD), distinguishing it into a semantic-rich row space and a semantic-sparse null space. Collaborative signals are then injected into the null space, while preserving the rich semantics of the row space. AlphaFuse prevents degradation of the semantic space, integrates the retained language embeddings into the final item embeddings, and eliminates the need for auxiliary trainable modules, enabling seamless adaptation to any sequential recommendation framework. We validate the effectiveness and flexibility of AlphaFuse through extensive experiments on three benchmark datasets, including cold-start user and long-tail settings, showcasing significant improvements in both discriminative and diffusion-based generative sequential recommenders. Our codes and datasets are available at https://github.com/Hugo-Chinn/AlphaFuse.",
        "translated": "顺序推荐领域的最新进展揭示了大语言模型（Large Language Models, LLMs）在增强项目嵌入方面的潜力。然而，现有方法面临三个关键限制：1）当高维语言嵌入映射到低维ID嵌入时导致的语义空间退化；2）语言嵌入的利用不足；3）依赖额外可训练参数（如适配器）来弥合语义空间与行为空间之间的鸿沟。本文提出AlphaFuse——一种简单但有效的语言引导学习策略，通过将ID嵌入学习置于语言嵌入的零空间内来解决上述挑战。具体而言，我们通过奇异值分解（Singular Value Decomposition, SVD）对语言嵌入的语义空间进行解耦，将其区分为语义丰富的行空间和语义稀疏的零空间。随后将协同信号注入零空间，同时保留行空间的丰富语义。AlphaFuse不仅防止了语义空间退化，还将保留的语言嵌入整合到最终的项目嵌入中，且无需辅助可训练模块，能够无缝适配任何顺序推荐框架。通过在三个基准数据集（包括冷启动用户和长尾场景设置）上的大量实验，我们验证了AlphaFuse在判别式和基于扩散的生成式顺序推荐器中均能带来显著提升的有效性与灵活性。代码及数据集已开源：https://github.com/Hugo-Chinn/AlphaFuse。"
    },
    {
        "title": "Relative Contrastive Learning for Sequential Recommendation with\n  Similarity-based Positive Pair Selection",
        "url": "http://arxiv.org/abs/2504.19178v1",
        "pub_date": "2025-04-27",
        "summary": "Contrastive Learning (CL) enhances the training of sequential recommendation (SR) models through informative self-supervision signals. Existing methods often rely on data augmentation strategies to create positive samples and promote representation invariance. Some strategies such as item reordering and item substitution may inadvertently alter user intent. Supervised Contrastive Learning (SCL) based methods find an alternative to augmentation-based CL methods by selecting same-target sequences (interaction sequences with the same target item) to form positive samples. However, SCL-based methods suffer from the scarcity of same-target sequences and consequently lack enough signals for contrastive learning. In this work, we propose to use similar sequences (with different target items) as additional positive samples and introduce a Relative Contrastive Learning (RCL) framework for sequential recommendation. RCL comprises a dual-tiered positive sample selection module and a relative contrastive learning module. The former module selects same-target sequences as strong positive samples and selects similar sequences as weak positive samples. The latter module employs a weighted relative contrastive loss, ensuring that each sequence is represented closer to its strong positive samples than its weak positive samples. We apply RCL on two mainstream deep learning-based SR models, and our empirical results reveal that RCL can achieve 4.88% improvement averagely than the state-of-the-art SR methods on five public datasets and one private dataset.",
        "translated": "对比学习通过提供信息丰富的自监督信号，有效提升了序列推荐模型的训练效果。现有方法通常依赖数据增强策略生成正样本以促进表示不变性，但诸如商品重排序和商品替换等策略可能无意中改变用户原始意图。基于监督对比学习的方法通过选择具有相同目标商品的交互序列（同目标序列）构建正样本，为基于增强的对比学习方法提供了替代方案。然而这类方法受限于同目标序列的稀缺性，难以获得充足的对比学习信号。本研究提出使用具有不同目标商品的相似序列作为额外正样本，构建了面向序列推荐的相对对比学习框架。该框架包含双层级正样本选择模块和相对对比学习模块：前者筛选同目标序列作为强正样本，选取相似序列作为弱正样本；后者采用加权相对对比损失函数，确保每个序列在表示空间中更接近其强正样本而非弱正样本。我们将该框架应用于两个主流深度学习序列推荐模型，实验结果表明在五个公共数据集和一个私有数据集上，相对对比学习方法相较现有最优序列推荐模型平均取得了4.88%的性能提升。"
    },
    {
        "title": "LLM-Evaluation Tropes: Perspectives on the Validity of LLM-Evaluations",
        "url": "http://arxiv.org/abs/2504.19076v1",
        "pub_date": "2025-04-27",
        "summary": "Large Language Models (LLMs) are increasingly used to evaluate information retrieval (IR) systems, generating relevance judgments traditionally made by human assessors. Recent empirical studies suggest that LLM-based evaluations often align with human judgments, leading some to suggest that human judges may no longer be necessary, while others highlight concerns about judgment reliability, validity, and long-term impact. As IR systems begin incorporating LLM-generated signals, evaluation outcomes risk becoming self-reinforcing, potentially leading to misleading conclusions.   This paper examines scenarios where LLM-evaluators may falsely indicate success, particularly when LLM-based judgments influence both system development and evaluation. We highlight key risks, including bias reinforcement, reproducibility challenges, and inconsistencies in assessment methodologies. To address these concerns, we propose tests to quantify adverse effects, guardrails, and a collaborative framework for constructing reusable test collections that integrate LLM judgments responsibly. By providing perspectives from academia and industry, this work aims to establish best practices for the principled use of LLMs in IR evaluation.",
        "translated": "大型语言模型（LLMs）正被越来越多地用于评估信息检索（IR）系统，其生成的关联性判断传统上由人类评估者完成。近期实证研究表明，基于LLM的评估结果常与人类判断结果一致，这导致部分研究者认为可能不再需要人工评估者，但另一些学者则对其判断的可靠性、有效性及长期影响提出了担忧。当IR系统开始整合LLM生成的信号时，评估结果可能陷入自我强化的循环，最终导致误导性结论。本文重点探讨LLM评估器可能错误指示成功的场景，尤其是在LLM生成的判断同时影响系统开发和评估过程的情况下。我们重点揭示了若干关键风险，包括偏见强化、可复现性挑战以及评估方法的不一致性。针对这些问题，我们提出了量化负面影响的测试方法、防护机制，以及构建可复用测试集的协作框架，以负责任的方式整合LLM的判断结果。通过整合学术界和工业界的观点，本研究旨在为IR评估中LLM的原则性应用建立最佳实践指南。"
    },
    {
        "title": "Feature Fusion Revisited: Multimodal CTR Prediction for MMCTR Challenge",
        "url": "http://arxiv.org/abs/2504.18961v1",
        "pub_date": "2025-04-26",
        "summary": "With the rapid advancement of Multimodal Large Language Models (MLLMs), an increasing number of researchers are exploring their application in recommendation systems. However, the high latency associated with large models presents a significant challenge for such use cases. The EReL@MIR workshop provided a valuable opportunity to experiment with various approaches aimed at improving the efficiency of multimodal representation learning for information retrieval tasks. As part of the competition's requirements, participants were mandated to submit a technical report detailing their methodologies and findings. Our team was honored to receive the award for Task 2 - Winner (Multimodal CTR Prediction). In this technical report, we present our methods and key findings. Additionally, we propose several directions for future work, particularly focusing on how to effectively integrate recommendation signals into multimodal representations. The codebase for our implementation is publicly available at: https://github.com/Lattice-zjj/MMCTR_Code, and the trained model weights can be accessed at: https://huggingface.co/FireFlyCourageous/MMCTR_DIN_MicroLens_1M_x1.",
        "translated": "随着多模态大语言模型（MLLMs）的快速发展，越来越多的研究者开始探索其在推荐系统中的应用。然而，大型模型伴随的高延迟特性为此类应用场景带来了重大挑战。EReL@MIR研讨会为尝试多种提升信息检索任务中多模态表示学习效率的方法提供了宝贵机会。根据竞赛要求，参赛者必须提交详细阐述方法及发现的技术报告。我们团队荣幸获得了任务二（多模态CTR预测）的优胜奖项。本技术报告将系统阐述我们的方法论与核心发现，同时针对未来研究方向提出若干建议，尤其聚焦于如何有效将推荐信号整合到多模态表示中。项目代码库已开源至：https://github.com/Lattice-zjj/MMCTR_Code，训练完成的模型权重可通过以下地址获取：https://huggingface.co/FireFlyCourageous/MMCTR_DIN_MicroLens_1M_x1。\n\n（翻译说明：  \n1. 专业术语处理：对MLLMs、CTR等专业缩写保留英文原词并附加中文解释，确保技术准确性  \n2. 技术细节呈现：对\"recommendation signals\"等概念采用\"推荐信号\"的译法，符合领域内惯用表达  \n3. 逻辑关系重构：将原文复合句合理拆分为符合中文表达习惯的短句，如将\"participants were mandated...\"独立成句  \n4. 学术规范遵循：对奖项名称\"Task 2 - Winner\"采用竞赛领域标准译法\"任务二 - 优胜者\"  \n5. 技术资源标注：完整保留代码库与模型权重链接的原始格式，确保可访问性）"
    },
    {
        "title": "Generative Product Recommendations for Implicit Superlative Queries",
        "url": "http://arxiv.org/abs/2504.18748v1",
        "pub_date": "2025-04-26",
        "summary": "In Recommender Systems, users often seek the best products through indirect, vague, or under-specified queries, such as \"best shoes for trail running\". Such queries, also referred to as implicit superlative queries, pose a significant challenge for standard retrieval and ranking systems as they lack an explicit mention of attributes and require identifying and reasoning over complex factors. We investigate how Large Language Models (LLMs) can generate implicit attributes for ranking as well as reason over them to improve product recommendations for such queries. As a first step, we propose a novel four-point schema for annotating the best product candidates for superlative queries called SUPERB, paired with LLM-based product annotations. We then empirically evaluate several existing retrieval and ranking approaches on our new dataset, providing insights and discussing their integration into real-world e-commerce production systems.",
        "translated": "在推荐系统中，用户经常通过间接、模糊或未明确指定的查询来寻找最佳产品，例如\"最适合越野跑的鞋子\"。这类被称为隐式最高级查询的请求，由于缺乏明确的属性说明且需要识别和推理复杂因素，给标准检索和排序系统带来了重大挑战。我们研究了大型语言模型（LLMs）如何为排序生成隐式属性，并对其进行推理以改进针对此类查询的产品推荐。首先，我们提出了一种名为SUPERB（面向最高级查询的最佳产品标注）的新型四点标注模式，配合基于LLM的产品标注方法。随后，我们在新构建的数据集上对多种现有检索与排序方法进行了实证评估，为实际电子商务生产系统的集成提供了深刻见解和实践讨论。"
    },
    {
        "title": "MINT: Multi-Vector Search Index Tuning",
        "url": "http://arxiv.org/abs/2504.20018v1",
        "pub_date": "2025-04-28",
        "summary": "Vector search plays a crucial role in many real-world applications. In addition to single-vector search, multi-vector search becomes important for multi-modal and multi-feature scenarios today. In a multi-vector database, each row is an item, each column represents a feature of items, and each cell is a high-dimensional vector. In multi-vector databases, the choice of indexes can have a significant impact on performance. Although index tuning for relational databases has been extensively studied, index tuning for multi-vector search remains unclear and challenging. In this paper, we define multi-vector search index tuning and propose a framework to solve it. Specifically, given a multi-vector search workload, we develop algorithms to find indexes that minimize latency and meet storage and recall constraints. Compared to the baseline, our latency achieves 2.1X to 8.3X speedup.",
        "translated": "向量搜索在众多现实应用中发挥着关键作用。除单向量搜索外，多向量搜索在当前多模态和多特征场景中日益重要。在多向量数据库中，每行代表一个数据项，每列表示数据项的特征，而每个单元格则存储高维向量。在多向量数据库中，索引选择对系统性能具有显著影响。虽然关系数据库的索引调优已得到广泛研究，但多向量搜索的索引优化问题仍不明确且充满挑战。本文明确定义了多向量搜索索引调优问题，并提出系统性解决方案框架。具体而言，针对给定的多向量搜索工作负载，我们开发了能够自动寻找在满足存储约束和召回率要求下最小化查询延迟的索引优化算法。实验表明，相较于基准方法，我们的方案实现了2.1倍到8.3倍的延迟优化。"
    },
    {
        "title": "LLM-Generated Fake News Induces Truth Decay in News Ecosystem: A Case\n  Study on Neural News Recommendation",
        "url": "http://arxiv.org/abs/2504.20013v2",
        "pub_date": "2025-04-28",
        "summary": "Online fake news moderation now faces a new challenge brought by the malicious use of large language models (LLMs) in fake news production. Though existing works have shown LLM-generated fake news is hard to detect from an individual aspect, it remains underexplored how its large-scale release will impact the news ecosystem. In this study, we develop a simulation pipeline and a dataset with ~56k generated news of diverse types to investigate the effects of LLM-generated fake news within neural news recommendation systems. Our findings expose a truth decay phenomenon, where real news is gradually losing its advantageous position in news ranking against fake news as LLM-generated news is involved in news recommendation. We further provide an explanation about why truth decay occurs from a familiarity perspective and show the positive correlation between perplexity and news ranking. Finally, we discuss the threats of LLM-generated fake news and provide possible countermeasures. We urge stakeholders to address this emerging challenge to preserve the integrity of news ecosystems.",
        "translated": "在线虚假新闻治理当前面临一项新挑战:大型语言模型(LLMs)被恶意应用于虚假新闻生产。尽管已有研究表明从个体层面检测LLM生成的虚假新闻存在困难，但其大规模传播将对新闻生态系统产生何种影响仍缺乏深入探究。本研究通过构建模拟管道和包含约5.6万条多样化生成新闻的数据集，深入分析了神经新闻推荐系统中LLM生成虚假新闻的影响机制。研究发现揭示了\"真相衰减\"现象:当LLM生成的新闻参与推荐排序时，真实新闻在排名中的优势地位会逐步丧失。我们进一步从熟悉度视角阐释了该现象的产生机制，并证实了困惑度与新闻排名之间的正相关性。最后，本文探讨了LLM生成虚假新闻的潜在威胁，提出了可能的应对策略。我们呼吁相关利益方重视这一新兴挑战，共同维护新闻生态系统的完整性。\n\n关键术语处理说明:\n1. \"truth decay\"译为\"真相衰减现象\"，既保持术语准确性又符合中文表达习惯\n2. \"perplexity\"译为\"困惑度\"，采用自然语言处理领域的标准译法\n3. \"neural news recommendation systems\"译为\"神经新闻推荐系统\"，准确反映其基于神经网络的技术特性\n4. \"familiarity perspective\"译为\"熟悉度视角\"，既保留原意又符合中文学术表达规范\n5. 技术指标\"~56k\"译为\"约5.6万条\"，遵循中文数字表达规范同时保持数据精确性\n\n本翻译严格遵循学术翻译规范，在保证专业术语准确性的同时，注重逻辑连贯性和可读性，完整保留了原文的技术细节和研究发现。"
    },
    {
        "title": "Efficient Domain-adaptive Continual Pretraining for the Process Industry\n  in the German Language",
        "url": "http://arxiv.org/abs/2504.19856v1",
        "pub_date": "2025-04-28",
        "summary": "Domain-adaptive continual pretraining (DAPT) is a state-of-the-art technique that further trains a language model (LM) on its pretraining task, e.g., language masking. Although popular, it requires a significant corpus of domain-related data, which is difficult to obtain for specific domains in languages other than English, such as the process industry in the German language. This paper introduces an efficient approach called ICL-augmented pretraining or ICL-APT that leverages in-context learning (ICL) and k-nearest neighbors (kNN) to augment target data with domain-related and in-domain texts, significantly reducing GPU time while maintaining strong model performance. Our results show that this approach performs better than traditional DAPT by 3.5 of the average IR metrics (e.g., mAP, MRR, and nDCG) and requires almost 4 times less computing time, providing a cost-effective solution for industries with limited computational capacity. The findings highlight the broader applicability of this framework to other low-resource industries, making NLP-based solutions more accessible and feasible in production environments.",
        "translated": "领域自适应持续预训练（Domain-adaptive continual pretraining, DAPT）是一种前沿技术，通过在预训练任务（如语言掩码任务）上对语言模型（LM）进行持续训练以提升其性能。尽管该技术应用广泛，但其需要大量领域相关数据作为支撑，这对于英语以外的特定语言领域（如德语流程工业领域）而言往往难以获取。本文提出了一种高效方法——基于上下文学习增强的预训练（ICL-augmented pretraining, ICL-APT），该方法通过整合上下文学习（ICL）和k近邻算法（kNN），利用领域相关文本和域内文本对目标数据进行增强，在保持模型优异性能的同时显著减少GPU计算时间。实验结果表明，该方法相较于传统DAPT在平均信息检索指标（如mAP、MRR和nDCG）上提升3.5个百分点，且所需计算时间减少近四倍，为计算资源受限的工业领域提供了高性价比的解决方案。研究结论表明，该框架可广泛适用于其他资源匮乏的行业，使得基于自然语言处理的解决方案在生产环境中更具可行性和推广价值。"
    },
    {
        "title": "Reconstructing Context: Evaluating Advanced Chunking Strategies for\n  Retrieval-Augmented Generation",
        "url": "http://arxiv.org/abs/2504.19754v1",
        "pub_date": "2025-04-28",
        "summary": "Retrieval-augmented generation (RAG) has become a transformative approach for enhancing large language models (LLMs) by grounding their outputs in external knowledge sources. Yet, a critical question persists: how can vast volumes of external knowledge be managed effectively within the input constraints of LLMs? Traditional methods address this by chunking external documents into smaller, fixed-size segments. While this approach alleviates input limitations, it often fragments context, resulting in incomplete retrieval and diminished coherence in generation. To overcome these shortcomings, two advanced techniques, late chunking and contextual retrieval, have been introduced, both aiming to preserve global context. Despite their potential, their comparative strengths and limitations remain unclear. This study presents a rigorous analysis of late chunking and contextual retrieval, evaluating their effectiveness and efficiency in optimizing RAG systems. Our results indicate that contextual retrieval preserves semantic coherence more effectively but requires greater computational resources. In contrast, late chunking offers higher efficiency but tends to sacrifice relevance and completeness.",
        "translated": "检索增强生成（Retrieval-Augmented Generation，RAG）通过将大语言模型（LLMs）的输出建立在外部知识源的基础上，已成为增强其性能的变革性方法。然而，一个关键问题始终存在：如何在海量外部知识与LLMs的输入限制之间实现有效平衡？传统解决方案将外部文档切分为固定尺寸的小片段，这种方法虽然缓解了输入限制，但往往导致上下文语境割裂，引发检索不完整和生成连贯性下降的问题。\n\n为克服这些缺陷，学界提出了两种旨在保持全局语境的高级技术——延迟分块（late chunking）和上下文检索（contextual retrieval）。尽管两者都展现出潜力，但其相对优势与局限性尚未明晰。本研究对这两种技术展开严谨分析，评估它们在优化RAG系统时的效能与效率。实验结果表明：上下文检索能更有效地保持语义连贯性，但需要更高的计算资源；相较之下，延迟分块虽具有更高效率，却倾向于以相关性和完整性为代价。"
    }
]